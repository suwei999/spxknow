#!/usr/bin/env python3
"""
CUDAå…¼å®¹æ€§æ£€æŸ¥è„šæœ¬
ç”¨äºè¯Šæ–­ PyTorch CUDA ä¸ GPU çš„å…¼å®¹æ€§é—®é¢˜
"""

import sys
import subprocess

def check_nvidia_smi():
    """æ£€æŸ¥ nvidia-smi æ˜¯å¦å¯ç”¨"""
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("=" * 60)
            print("âœ… NVIDIA GPU ä¿¡æ¯:")
            print("=" * 60)
            print(result.stdout)
            return True
        else:
            print("âŒ nvidia-smi æ‰§è¡Œå¤±è´¥")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smi æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²å®‰è£… NVIDIA é©±åŠ¨")
        return False
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ GPU æ—¶å‡ºé”™: {e}")
        return False

def check_pytorch():
    """æ£€æŸ¥ PyTorch å’Œ CUDA ç‰ˆæœ¬"""
    try:
        import torch
        print("=" * 60)
        print("âœ… PyTorch ä¿¡æ¯:")
        print("=" * 60)
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                print(f"\nGPU {i}:")
                print(f"  åç§°: {torch.cuda.get_device_name(i)}")
                print(f"  è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_capability(i)}")
                print(f"  æ€»å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨")
            
        return True
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ PyTorch æ—¶å‡ºé”™: {e}")
        return False

def test_cuda_kernel():
    """æµ‹è¯• CUDA kernel æ˜¯å¦å¯ç”¨"""
    try:
        import torch
        if not torch.cuda.is_available():
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ kernel æµ‹è¯•")
            return False
            
        print("=" * 60)
        print("ğŸ§ª æµ‹è¯• CUDA Kernel å…¼å®¹æ€§:")
        print("=" * 60)
        
        try:
            # æµ‹è¯•ç®€å•çš„ CUDA æ“ä½œ
            test_tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()
            result = test_tensor + 1.0
            print(f"âœ… CUDA kernel æµ‹è¯•é€šè¿‡")
            print(f"   æµ‹è¯•ç»“æœ: {result.cpu().numpy()}")
            del test_tensor, result
            torch.cuda.empty_cache()
            return True
        except Exception as e:
            error_msg = str(e).lower()
            print(f"âŒ CUDA kernel æµ‹è¯•å¤±è´¥:")
            print(f"   é”™è¯¯: {e}")
            
            if "no kernel image" in error_msg:
                print("\n" + "=" * 60)
                print("ğŸ”§ é—®é¢˜è¯Šæ–­:")
                print("=" * 60)
                print("PyTorch ç¼–è¯‘æ—¶æ”¯æŒçš„ CUDA æ¶æ„ä¸å½“å‰ GPU ä¸åŒ¹é…")
                print("\nè§£å†³æ–¹æ¡ˆ:")
                print("1. æ£€æŸ¥ GPU è®¡ç®—èƒ½åŠ›:")
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        cap = torch.cuda.get_device_capability(i)
                        print(f"   GPU {i}: {cap[0]}.{cap[1]} (sm_{cap[0]}{cap[1]})")
                
                print("\n2. å®‰è£…åŒ¹é…çš„ PyTorch ç‰ˆæœ¬:")
                print("   è®¿é—® https://pytorch.org/get-started/locally/")
                print("   é€‰æ‹©å¯¹åº”çš„ CUDA ç‰ˆæœ¬å’Œ GPU æ¶æ„")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ RTX 5090 (sm_100)
                try:
                    import torch
                    if torch.cuda.is_available():
                        cap = torch.cuda.get_device_capability(0)
                        if cap[0] >= 10:
                            print("\n   âš ï¸  æ£€æµ‹åˆ° Blackwell æ¶æ„ (RTX 5090)ï¼Œéœ€è¦æœ€æ–°ç‰ˆæœ¬:")
                            print("   pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124")
                            print("   æˆ–")
                            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124")
                        else:
                            print("\n   å¯¹äº CUDA 12.4 (æ¨èï¼Œæ”¯æŒæœ€æ–° GPU):")
                            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124")
                            print("\n   å¯¹äº CUDA 12.1:")
                            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
                            print("\n   å¯¹äº CUDA 11.8:")
                            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
                except:
                    print("\n   å¯¹äº CUDA 12.4 (æ¨è):")
                    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124")
                    print("\n   å¯¹äº CUDA 12.1:")
                    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
                
            return False
            
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…ï¼Œæ— æ³•æµ‹è¯•")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯• CUDA kernel æ—¶å‡ºé”™: {e}")
        return False

def check_pytorch_arch():
    """æ£€æŸ¥ PyTorch æ”¯æŒçš„æ¶æ„"""
    try:
        import torch
        if not torch.cuda.is_available():
            return
            
        print("=" * 60)
        print("ğŸ“‹ PyTorch ç¼–è¯‘ä¿¡æ¯:")
        print("=" * 60)
        
        # å°è¯•è·å–ç¼–è¯‘ä¿¡æ¯
        try:
            # PyTorch 2.0+ æ”¯æŒ
            if hasattr(torch.version, 'cuda'):
                print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        except:
            pass
            
        # æ£€æŸ¥æ”¯æŒçš„æ¶æ„ï¼ˆé€šè¿‡å°è¯•ä¸åŒæ¶æ„çš„ kernelï¼‰
        print("\næ”¯æŒçš„æ¶æ„æ£€æµ‹:")
        try:
            # è¿™ä¸ªæ–¹æ³•ä¸ç›´æ¥å¯ç”¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡é”™è¯¯ä¿¡æ¯æ¨æ–­
            print("(éœ€è¦é€šè¿‡å®é™…è¿è¡Œæ¥æ£€æµ‹)")
        except:
            pass
            
    except ImportError:
        pass

def main():
    print("=" * 60)
    print("CUDA å…¼å®¹æ€§è¯Šæ–­å·¥å…·")
    print("=" * 60)
    print()
    
    # 1. æ£€æŸ¥ GPU
    has_gpu = check_nvidia_smi()
    print()
    
    # 2. æ£€æŸ¥ PyTorch
    has_pytorch = check_pytorch()
    print()
    
    # 3. æµ‹è¯• CUDA kernel
    if has_pytorch:
        kernel_ok = test_cuda_kernel()
        print()
        
        # 4. æ£€æŸ¥æ¶æ„ä¿¡æ¯
        check_pytorch_arch()
        print()
        
        if not kernel_ok:
            print("=" * 60)
            print("ğŸ’¡ å»ºè®®:")
            print("=" * 60)
            print("1. å¦‚æœ GPU è®¡ç®—èƒ½åŠ›è¾ƒä½ï¼ˆå¦‚ sm_60ï¼‰ï¼Œå¯èƒ½éœ€è¦å®‰è£…æ”¯æŒæ›´å¤šæ¶æ„çš„ PyTorch")
            print("2. æˆ–è€…ä½¿ç”¨ CPU ç‰ˆæœ¬ï¼ˆæ€§èƒ½è¾ƒæ…¢ä½†ç¨³å®šï¼‰")
            print("3. æ£€æŸ¥ CUDA é©±åŠ¨ç‰ˆæœ¬æ˜¯å¦ä¸ PyTorch è¦æ±‚çš„ç‰ˆæœ¬åŒ¹é…")
            print()
            sys.exit(1)
    else:
        print("âš ï¸ è¯·å…ˆå®‰è£… PyTorch")
        sys.exit(1)
    
    print("=" * 60)
    print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ŒCUDA å¯ç”¨")
    print("=" * 60)

if __name__ == "__main__":
    main()

