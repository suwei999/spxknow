"""
Image Vectorization Service
æ ¹æ®æ–‡æ¡£å¤„ç†æµç¨‹è®¾è®¡å®ç°CLIP/ResNet/ViTå›¾ç‰‡å‘é‡åŒ–åŠŸèƒ½
"""

import os
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from PIL import Image
import torch
import open_clip
import cv2
from app.core.logging import logger
from app.core.exceptions import CustomException, ErrorCode
from app.utils.download_progress import (
    log_download_start, 
    log_download_success, 
    log_download_error,
    setup_hf_download_progress
)

# å½»åº•ç§»é™¤å¯¹ torchvision çš„å¯¼å…¥ï¼Œé¿å…å› ç¯å¢ƒä¸å…¼å®¹å¯¼è‡´åº”ç”¨å¯åŠ¨å¤±è´¥
# å¦‚æœåç»­éœ€è¦ ResNet/ViTï¼Œå¯åœ¨å…·å¤‡å…¼å®¹ç¯å¢ƒæ—¶å†æŒ‰éœ€å¼•å…¥

class ImageVectorizationService:
    """å›¾ç‰‡å‘é‡åŒ–æœåŠ¡ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models = {}
        self.transforms = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–è§†è§‰æ¨¡å‹ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è§†è§‰æ¨¡å‹")
            from app.config.settings import settings as _settings

            if _settings.HF_HOME:
                hf_home = _settings.HF_HOME
                logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®çš„ HF_HOME: {hf_home}")
            elif _settings.CLIP_CACHE_DIR:
                hf_home = os.path.dirname(_settings.CLIP_CACHE_DIR)
                logger.info(
                    "ğŸ“ æœªé…ç½® HF_HOMEï¼Œå°†æ ¹æ® CLIP_CACHE_DIR æ¨å¯¼ HF_HOMEã€‚"
                    f" HF_HOME={hf_home}"
                )
            else:
                hf_home = os.path.expanduser("~/.cache/huggingface")
                logger.info(f"ğŸ“ æœªé…ç½® HF_HOMEï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤: {hf_home}")
            os.environ["HF_HOME"] = hf_home
            logger.info(f"ğŸ“ å·²è®¾ç½® HF_HOME: {hf_home}")

            os.makedirs(_settings.CLIP_MODELS_DIR, exist_ok=True)
            os.makedirs(_settings.CLIP_CACHE_DIR, exist_ok=True)
            logger.info(f"ğŸ“ CLIPæ¨¡å‹ç›®å½•: {_settings.CLIP_MODELS_DIR}")
            logger.info(f"ğŸ“ CLIPç¼“å­˜ç›®å½•: {_settings.CLIP_CACHE_DIR}")
            os.environ["OPENCLIP_CACHE"] = _settings.CLIP_CACHE_DIR

            model_name = getattr(_settings, "CLIP_MODEL_NAME", "ViT-B-32")
            preload_path = _settings.CLIP_PRETRAINED_PATH
            cache_dir = _settings.CLIP_CACHE_DIR

            candidate_paths = []
            if preload_path:
                candidate_paths.append(preload_path)
            candidate_paths.append(cache_dir)
            candidate_paths.append(hf_home)
            default_hf_cache = os.path.expanduser("~/.cache/huggingface")
            if not _settings.HF_HOME and default_hf_cache not in candidate_paths:
                candidate_paths.append(default_hf_cache)

            def _probe_model(directory: str) -> Optional[str]:
                if not directory or not os.path.exists(directory):
                    return None
                for root, _, files in os.walk(directory):
                    for file in files:
                        lower = file.lower()
                        if lower.endswith((".pt", ".pth", ".safetensors", ".bin")) and (
                            "clip" in lower or "laion" in lower or "vit-b-32" in lower or "openclip" in lower
                        ):
                            full = os.path.join(root, file)
                            try:
                                if os.path.getsize(full) > 10 * 1024 * 1024:
                                    return full
                            except OSError:
                                continue
                return None

            model_local_path: Optional[str] = None
            for path in candidate_paths:
                found = _probe_model(path)
                if found:
                    model_local_path = found
                    break

            using_cache = model_local_path is not None
            if using_cache:
                logger.info(f"ğŸ”§ ä½¿ç”¨æœ¬åœ°CLIPæ¨¡å‹: {model_local_path}")
                os.environ.update({
                    "HF_HUB_OFFLINE": "1",
                    "TRANSFORMERS_OFFLINE": "1",
                    "HF_DATASETS_OFFLINE": "1",
                    "HF_HUB_DISABLE_PROGRESS_BARS": "1",
                })
                pretrained_arg = model_local_path
            else:
                logger.info("ğŸŒ æœªæ£€æµ‹åˆ°æœ¬åœ°CLIPæ¨¡å‹ï¼Œå…è®¸è”ç½‘ä¸‹è½½")
                for key in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]:
                    os.environ.pop(key, None)
                os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "0"
                pretrained_arg = "laion2b_s34b_b79k"

            try:
                from huggingface_hub import offline_mode
                offline_mode(using_cache)
            except (ImportError, AttributeError):
                pass

            try:
                clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                    model_name,
                    pretrained=pretrained_arg,
                    cache_dir=None if using_cache else _settings.CLIP_CACHE_DIR,
                    device=self.device,
                )
                logger.info("âœ… CLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            except Exception as err:
                raise CustomException(
                    code=ErrorCode.VECTOR_GENERATION_FAILED,
                    message=(
                        "CLIPæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦å¯è®¿é—® huggingface.co æˆ–æ‰‹åŠ¨å°†æ¨¡å‹ä¸‹è½½è‡³ "
                        f"{_settings.CLIP_PRETRAINED_PATH}ã€‚é”™è¯¯è¯¦æƒ…: {err}"
                    ),
                ) from err

            clip_model.eval()
            clip_model.to(self.device)
            self.models["clip"] = clip_model
            self.transforms["clip"] = clip_preprocess
            logger.info("âœ… CLIPæ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡å¹¶å¯ä½¿ç”¨")
        except CustomException:
            raise
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è§†è§‰æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"åˆå§‹åŒ–è§†è§‰æ¨¡å‹å¤±è´¥: {str(e)}"
            )
    
    def generate_clip_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨CLIPç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹CLIPå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'clip')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨CLIPæå–ç‰¹å¾
            with torch.no_grad():
                image_features = self.models['clip'].encode_image(image)
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                embedding = image_features.cpu().numpy().flatten().tolist()
            
            logger.info(f"CLIPå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"CLIPå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"CLIPå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_clip_text_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆ512ç»´ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬ï¼‰
            
        Returns:
            512ç»´å‘é‡åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹CLIPæ–‡æœ¬å‘é‡åŒ–: {text[:50]}...")
            
            # æ£€æŸ¥CLIPæ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if 'clip' not in self.models:
                raise CustomException(
                    code=ErrorCode.VECTOR_GENERATION_FAILED,
                    message="CLIPæ¨¡å‹æœªåˆå§‹åŒ–"
                )
            
            clip_model = self.models['clip']
            
            # å¯¹æ–‡æœ¬è¿›è¡Œtokenize
            # open_clip.tokenize è¿”å›çš„æ˜¯ torch.Tensorï¼Œéœ€è¦ç§»åŠ¨åˆ°device
            text_tokens = open_clip.tokenize([text]).to(self.device)
            
            # ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨
            with torch.no_grad():
                text_features = clip_model.encode_text(text_tokens)
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡ï¼ˆä¸å›¾åƒå‘é‡å¤„ç†æ–¹å¼ä¸€è‡´ï¼‰
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                embedding = text_features.cpu().numpy().flatten().tolist()
            
            if not embedding or len(embedding) != 512:
                logger.error(f"CLIPæ–‡æœ¬å‘é‡ç”Ÿæˆå¤±è´¥æˆ–ç»´åº¦ä¸æ­£ç¡®: {len(embedding) if embedding else 0}")
                raise CustomException(
                    code=ErrorCode.VECTOR_GENERATION_FAILED,
                    message=f"CLIPæ–‡æœ¬å‘é‡ç”Ÿæˆå¤±è´¥æˆ–ç»´åº¦ä¸æ­£ç¡®: æœŸæœ›512ç»´ï¼Œå®é™…{len(embedding) if embedding else 0}ç»´"
                )
            
            logger.info(f"CLIPæ–‡æœ¬å‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except CustomException:
            raise
        except Exception as e:
            logger.error(f"CLIPæ–‡æœ¬å‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"CLIPæ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_resnet_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨ResNetç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹ResNetå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'resnet')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨ResNetæå–ç‰¹å¾
            with torch.no_grad():
                features = self.models['resnet'](image)
                # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–åçš„ç‰¹å¾ä½œä¸ºåµŒå…¥å‘é‡
                embedding = features.cpu().numpy().flatten().tolist()
            
            logger.info(f"ResNetå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ResNetå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"ResNetå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_vit_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨ViTç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹ViTå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'vit')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨ViTæå–ç‰¹å¾
            with torch.no_grad():
                features = self.models['vit'](image)
                # ä½¿ç”¨åˆ†ç±»å¤´å‰çš„ç‰¹å¾ä½œä¸ºåµŒå…¥å‘é‡
                embedding = features.cpu().numpy().flatten().tolist()
            
            logger.info(f"ViTå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ViTå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"ViTå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_multi_model_embedding(self, image_path: str, models: List[str] = None) -> Dict[str, List[float]]:
        """ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹å¤šæ¨¡å‹å‘é‡åŒ–: {image_path}")
            
            if models is None:
                models = ['clip', 'resnet', 'vit']
            
            embeddings = {}
            
            for model_name in models:
                try:
                    if model_name == 'clip':
                        embeddings['clip'] = self.generate_clip_embedding(image_path)
                    elif model_name == 'resnet':
                        embeddings['resnet'] = self.generate_resnet_embedding(image_path)
                    elif model_name == 'vit':
                        embeddings['vit'] = self.generate_vit_embedding(image_path)
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {model_name} å‘é‡åŒ–å¤±è´¥: {e}")
                    embeddings[model_name] = []
            
            logger.info(f"å¤šæ¨¡å‹å‘é‡åŒ–å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡å‹å‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"å¤šæ¨¡å‹å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_hybrid_embedding(self, image_path: str) -> List[float]:
        """ç”Ÿæˆæ··åˆåµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æ··åˆå‘é‡åŒ–: {image_path}")
            
            # è·å–å¤šä¸ªæ¨¡å‹çš„åµŒå…¥å‘é‡
            embeddings = self.generate_multi_model_embedding(image_path)
            
            # èåˆç­–ç•¥ï¼šåŠ æƒå¹³å‡
            weights = {
                'clip': 0.5,    # CLIPæƒé‡æœ€é«˜ï¼Œå› ä¸ºæ”¯æŒå›¾æ–‡è”åˆ
                'resnet': 0.3,  # ResNetæƒé‡ä¸­ç­‰
                'vit': 0.2      # ViTæƒé‡è¾ƒä½
            }
            
            # è®¡ç®—åŠ æƒå¹³å‡
            hybrid_embedding = []
            for i in range(512):  # ä½¿ç”¨512ç»´ä½œä¸ºæ ‡å‡†ç»´åº¦
                weighted_sum = 0.0
                total_weight = 0.0
                
                for model_name, embedding in embeddings.items():
                    if embedding and len(embedding) > i:
                        weight = weights.get(model_name, 0.0)
                        weighted_sum += embedding[i] * weight
                        total_weight += weight
                
                if total_weight > 0:
                    hybrid_embedding.append(weighted_sum / total_weight)
                else:
                    hybrid_embedding.append(0.0)
            
            logger.info(f"æ··åˆå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(hybrid_embedding)}")
            return hybrid_embedding
            
        except Exception as e:
            logger.error(f"æ··åˆå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"æ··åˆå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def _preprocess_image(self, image_path: str, model_type: str) -> Optional[torch.Tensor]:
        """å›¾ç‰‡é¢„å¤„ç† - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.debug(f"å¼€å§‹å›¾ç‰‡é¢„å¤„ç†: {image_path}, æ¨¡å‹: {model_type}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„å¤„ç†æ–¹æ³•
            if model_type == 'clip':
                # CLIPé¢„å¤„ç†
                transform = self.transforms['clip']
                processed_image = transform(image).unsqueeze(0).to(self.device)
            elif model_type in ['resnet', 'vit']:
                # ResNet/ViTé¢„å¤„ç†
                transform = self.transforms[model_type]
                processed_image = transform(image).unsqueeze(0).to(self.device)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return None
            
            logger.debug(f"å›¾ç‰‡é¢„å¤„ç†å®Œæˆ: {image_path}")
            return processed_image
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡é¢„å¤„ç†é”™è¯¯: {e}", exc_info=True)
            return None
    
    def extract_image_features(self, image_path: str) -> Dict[str, Any]:
        """æå–å›¾ç‰‡ç‰¹å¾ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æå–å›¾ç‰‡ç‰¹å¾: {image_path}")
            
            # ä½¿ç”¨OpenCVæå–ä¼ ç»Ÿç‰¹å¾
            image = cv2.imread(image_path)
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"æ— æ³•è¯»å–å›¾ç‰‡: {image_path}"
                )
            
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # æå–SIFTç‰¹å¾
            sift = cv2.SIFT_create()
            keypoints, descriptors = sift.detectAndCompute(gray, None)
            
            # æå–ORBç‰¹å¾
            orb = cv2.ORB_create()
            orb_keypoints, orb_descriptors = orb.detectAndCompute(gray, None)
            
            # è·å–å›¾ç‰‡åŸºæœ¬ä¿¡æ¯
            height, width = image.shape[:2]
            
            features = {
                'sift_keypoints': len(keypoints) if keypoints is not None else 0,
                'sift_descriptors': descriptors.tolist() if descriptors is not None else [],
                'orb_keypoints': len(orb_keypoints) if orb_keypoints is not None else 0,
                'orb_descriptors': orb_descriptors.tolist() if orb_descriptors is not None else [],
                'image_size': {'width': width, 'height': height},
                'aspect_ratio': width / height if height > 0 else 1.0
            }
            
            logger.info(f"å›¾ç‰‡ç‰¹å¾æå–å®Œæˆ: {image_path}")
            return features
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç‰¹å¾æå–é”™è¯¯: {e}", exc_info=True)
            return {}
    
    def calculate_image_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """è®¡ç®—å›¾ç‰‡ç›¸ä¼¼åº¦ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.debug("å¼€å§‹è®¡ç®—å›¾ç‰‡ç›¸ä¼¼åº¦")
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            if len(vec1) != len(vec2):
                logger.warning(f"å‘é‡ç»´åº¦ä¸ä¸€è‡´: {len(vec1)} vs {len(vec2)}")
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                logger.warning("å‘é‡ä¸ºé›¶å‘é‡")
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            logger.debug(f"å›¾ç‰‡ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ: {similarity}")
            return similarity
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}", exc_info=True)
            return 0.0
    
    def batch_process_images(self, image_paths: List[str], model_type: str = 'clip') -> List[List[float]]:
        """æ‰¹é‡å¤„ç†å›¾ç‰‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†å›¾ç‰‡ï¼Œæ•°é‡: {len(image_paths)}, æ¨¡å‹: {model_type}")
            
            embeddings = []
            
            for i, image_path in enumerate(image_paths):
                try:
                    logger.debug(f"å¤„ç†å›¾ç‰‡ {i+1}/{len(image_paths)}: {image_path}")
                    
                    if model_type == 'clip':
                        embedding = self.generate_clip_embedding(image_path)
                    elif model_type == 'resnet':
                        embedding = self.generate_resnet_embedding(image_path)
                    elif model_type == 'vit':
                        embedding = self.generate_vit_embedding(image_path)
                    elif model_type == 'hybrid':
                        embedding = self.generate_hybrid_embedding(image_path)
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                        embedding = []
                    
                    embeddings.append(embedding)
                    
                except Exception as e:
                    logger.error(f"å›¾ç‰‡ {image_path} å¤„ç†å¤±è´¥: {e}")
                    embeddings.append([])
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(embeddings)} ä¸ªå›¾ç‰‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†é”™è¯¯: {e}", exc_info=True)
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info("è·å–è§†è§‰æ¨¡å‹ä¿¡æ¯")
            
            model_info = {
                'available_models': list(self.models.keys()),
                'device': str(self.device),
                'clip_info': {
                    'model_name': 'ViT-B/32',
                    'embedding_dim': 512,
                    'supports_text_image': True
                },
                'resnet_info': {
                    'model_name': 'ResNet-50',
                    'embedding_dim': 1000,
                    'supports_text_image': False
                },
                'vit_info': {
                    'model_name': 'ViT-B/16',
                    'embedding_dim': 1000,
                    'supports_text_image': False
                },
                'hybrid_info': {
                    'model_name': 'Hybrid (CLIP+ResNet+ViT)',
                    'embedding_dim': 512,
                    'supports_text_image': True
                }
            }
            
            logger.info("è§†è§‰æ¨¡å‹ä¿¡æ¯è·å–å®Œæˆ")
            return model_info
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯é”™è¯¯: {e}", exc_info=True)
            return {'error': str(e)}
    
    def cleanup_models(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        try:
            logger.info("å¼€å§‹æ¸…ç†æ¨¡å‹èµ„æº")
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¸…ç†æ¨¡å‹
            self.models.clear()
            self.transforms.clear()
            
            logger.info("æ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹èµ„æºæ¸…ç†é”™è¯¯: {e}", exc_info=True)
