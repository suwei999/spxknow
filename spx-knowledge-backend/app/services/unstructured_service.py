"""
Unstructured Service
æ ¹æ®æ–‡æ¡£å¤„ç†æµç¨‹è®¾è®¡å®ç°Unstructuredæ–‡æ¡£è§£æåŠŸèƒ½
"""

from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy.orm import Session
from fastapi import UploadFile
import os
import tempfile
import json
import zipfile
import re
import shutil
from unstructured.partition.auto import partition
from unstructured.staging.base import elements_to_json
from app.core.logging import logger
from app.config.settings import settings
from app.services.office_converter import convert_docx_to_pdf
from app.core.exceptions import CustomException, ErrorCode
from app.utils.download_progress import (
    log_download_start, 
    log_download_success, 
    log_download_error
)
import xml.etree.ElementTree as ET
import io

# é¢„ç¼–è¯‘å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
_RE_WHITESPACE = re.compile(r'\s+')
_RE_TABS = re.compile(r"[\t\f\v]+")
_RE_MULTI_SPACES = re.compile(r"[ ]{2,}")
_RE_CHINESE = re.compile(r'[\u4e00-\u9fff]')
_RE_CHARS_ONLY = re.compile(r'[a-zA-Z\u4e00-\u9fff\d]')
_RE_PUNCTUATION = re.compile(r'[^\w\s\u4e00-\u9fff]')
_RE_RANDOM_CHARS = re.compile(r'[^a-zA-Z\u4e00-\u9fff\s]{3,}')

# é¢„ç¼–è¯‘é€šç”¨å†…å®¹ä¿æŠ¤æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
_RE_SECTION_SINGLE = re.compile(r'^\d+\.')  # å•æ•°å­—åŠ ç‚¹
_RE_SECTION_MULTI = re.compile(r'\d+\.\d+(?:\.\d+)*')  # å¤šçº§ç¼–å·
_RE_SECTION_CHINESE = re.compile(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]')  # ä¸­æ–‡ç¼–å·
_RE_SECTION_ROMAN = re.compile(r'[IVX]+\.\d+')  # ç½—é©¬æ•°å­—
_RE_SECTION_CHAPTER = re.compile(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]')  # ç« èŠ‚æ–‡å­—
_RE_SECTION_PART = re.compile(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[éƒ¨åˆ†ç¯‡]')  # éƒ¨åˆ†/ç¯‡
_RE_APPENDIX_EN = re.compile(r'Appendix\s+[A-Z\d]', re.IGNORECASE)  # Appendix A/1
_RE_APPENDIX_CN = re.compile(r'Appendix\s+[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]', re.IGNORECASE)  # Appendix ä¸€
_RE_CHINESE_5_CHARS = re.compile(r'[\u4e00-\u9fa5]{5,}')  # 5ä¸ªä»¥ä¸Šè¿ç»­æ±‰å­—
_RE_ENGLISH_WORD = re.compile(r'\b[a-zA-Z]{3,}\b')  # 3ä¸ªä»¥ä¸Šå­—æ¯çš„å•è¯

# å¸¸é‡å®šä¹‰
DEFAULT_PAGE_HEIGHT = 842  # A4é«˜åº¦ï¼ˆç‚¹ï¼‰
MIN_TEXT_LENGTH_FOR_DUPLICATE = 10
MIN_TEXT_LENGTH_FOR_NOISE = 5
MIN_TEXT_LENGTH_FOR_NOISE_PDF = 3
MAX_PREVIOUS_TEXTS = 50
MAX_DUPLICATE_CHECK = 10

class UnstructuredService:
    """Unstructuredæ–‡æ¡£è§£ææœåŠ¡ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
    
    def __init__(self, db: Session):
        self.db = db
        
        # é…ç½® Hugging Face æ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œæœ¬åœ°æ²¡æœ‰åˆ™è”ç½‘ä¸‹è½½ï¼‰
        try:
            hf_home = settings.HF_HOME or settings.UNSTRUCTURED_MODELS_DIR
            # âœ… å…³é”®ï¼šå¼ºåˆ¶è®¾ç½®æ‰€æœ‰ HF ç›¸å…³çš„ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿åªä½¿ç”¨é…ç½®çš„ç›®å½•
            os.environ['HF_HOME'] = hf_home  # å¼ºåˆ¶è¦†ç›–ï¼Œä¸ä½¿ç”¨ setdefault
            transformers_cache = os.path.join(hf_home, 'transformers_cache')
            datasets_cache = os.path.join(hf_home, 'datasets_cache')
            
            # âœ… å…³é”®ï¼šè®¾ç½® HF_HUB_CACHEï¼Œå¼ºåˆ¶ huggingface_hub ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•
            # HF_HUB_CACHE æ˜¯ huggingface_hub åº“çš„ä¸»è¦ç¼“å­˜è·¯å¾„
            hf_hub_cache = os.path.join(hf_home, 'hub')
            os.environ['TRANSFORMERS_CACHE'] = transformers_cache  # å¼ºåˆ¶è¦†ç›–
            os.environ['HF_DATASETS_CACHE'] = datasets_cache  # å¼ºåˆ¶è¦†ç›–
            os.environ['HF_HUB_CACHE'] = hf_hub_cache  # âœ… æ–°å¢ï¼šå¼ºåˆ¶ huggingface_hub ä½¿ç”¨é…ç½®çš„ç¼“å­˜
            
            # ç¡®ä¿æ‰€æœ‰æ¨¡å‹ç›®å½•è‡ªåŠ¨åˆ›å»º
            os.makedirs(hf_home, exist_ok=True)
            os.makedirs(transformers_cache, exist_ok=True)
            os.makedirs(datasets_cache, exist_ok=True)
            os.makedirs(hf_hub_cache, exist_ok=True)  # âœ… æ–°å¢ï¼šåˆ›å»º HF Hub ç¼“å­˜ç›®å½•
            
            logger.info(f"âœ… å·²å¼ºåˆ¶è®¾ç½® Hugging Face ç¼“å­˜ç›®å½•ï¼ˆä¸å†ä½¿ç”¨é»˜è®¤ ~/.cache/huggingfaceï¼‰:")
            logger.info(f"   HF_HOME={hf_home}")
            logger.info(f"   HF_HUB_CACHE={hf_hub_cache}")
            logger.info(f"   TRANSFORMERS_CACHE={transformers_cache}")
            
            # YOLOX æ¨¡å‹è·¯å¾„é…ç½®ï¼ˆunstructured åº“ä½¿ç”¨çš„å¸ƒå±€æ£€æµ‹æ¨¡å‹ï¼‰
            # æ¨¡å‹è·¯å¾„ï¼šmodels/unstructured/yolo_x_layout/yolox_10.05.onnx
            yolo_model_dir = os.path.join(settings.UNSTRUCTURED_MODELS_DIR, 'yolo_x_layout')
            yolo_model_path = os.path.join(yolo_model_dir, 'yolox_10.05.onnx')
            
            # ç¡®ä¿ YOLOX æ¨¡å‹ç›®å½•è‡ªåŠ¨åˆ›å»º
            os.makedirs(yolo_model_dir, exist_ok=True)
            
            # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒ…æ‹¬æŒ‡å®šè·¯å¾„å’ŒHFç¼“å­˜ç›®å½•ï¼‰
            # unstructured åº“ä¸‹è½½çš„æ¨¡å‹å¯èƒ½ç¼“å­˜åœ¨ HF ç¼“å­˜ç›®å½•ä¸­
            model_found = False
            model_location = None
            
            def _check_model_in_directory(directory, desc=""):
                """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾ YOLOX æ¨¡å‹"""
                if not os.path.exists(directory):
                    return None
                try:
                    for root, dirs, files in os.walk(directory):
                        for file in files:
                            if file.endswith('.onnx') and ('yolo' in file.lower() or 'layout' in file.lower()):
                                cached_model_path = os.path.join(root, file)
                                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆYOLOXæ¨¡å‹é€šå¸¸>1MBï¼‰
                                try:
                                    file_size = os.path.getsize(cached_model_path)
                                    if file_size > 1024 * 1024:  # è‡³å°‘1MB
                                        return cached_model_path
                                except OSError:
                                    continue
                except Exception:
                    pass
                return None
            
            # âœ… æ¨¡å‹æ£€æŸ¥é¡ºåºï¼ˆä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼‰ï¼š
            # 1. é¦–å…ˆæ£€æŸ¥æŒ‡å®šè·¯å¾„ï¼ˆsettings.py é…ç½®çš„æœ¬åœ°è·¯å¾„ï¼‰
            # 2. ç„¶åæ£€æŸ¥é…ç½®çš„ç¼“å­˜ç›®å½•ï¼ˆsettings.py é…ç½®çš„ç¼“å­˜ç›®å½•ï¼‰
            # 3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œå‡†å¤‡è”ç½‘ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
            
            # 1. ä¼˜å…ˆæ£€æŸ¥æŒ‡å®šè·¯å¾„ï¼ˆæœ€ä¼˜å…ˆï¼‰
            if os.path.exists(yolo_model_path):
                model_found = True
                model_location = yolo_model_path
                # âœ… å…³é”®ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ unstructured åº“ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸è®¿é—®ç½‘ç»œ
                os.environ['UNSTRUCTURED_LAYOUT_MODEL'] = yolo_model_path
                logger.info(f"âœ… æ­¥éª¤1: åœ¨æŒ‡å®šè·¯å¾„æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨: {yolo_model_path}")
            else:
                # 2. æ£€æŸ¥é…ç½®çš„ Hugging Face ç¼“å­˜ç›®å½•ï¼ˆç¬¬äºŒæ­¥ï¼‰
                # æ³¨æ„ï¼šåªæ£€æŸ¥ settings.py é…ç½®çš„ç¼“å­˜ç›®å½•ï¼Œä¸æ£€æŸ¥é»˜è®¤çš„ ~/.cache/huggingface
                cached_model = _check_model_in_directory(transformers_cache, "é…ç½®çš„ç¼“å­˜ç›®å½•")
                if cached_model:
                    model_found = True
                    model_location = cached_model
                    # âœ… å…³é”®ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ unstructured åº“ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹
                    os.environ['UNSTRUCTURED_LAYOUT_MODEL'] = cached_model
                    logger.info(f"âœ… æ­¥éª¤2: åœ¨é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­å‘ç°æ¨¡å‹: {cached_model}")
                    logger.info(f"âœ… å·²è®¾ç½® UNSTRUCTURED_LAYOUT_MODEL ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œä¸ä¼šè®¿é—®ç½‘ç»œ")
                else:
                    # 3. æ¨¡å‹ä¸å­˜åœ¨äºé…ç½®çš„ä½ç½®ï¼Œå‡†å¤‡ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
                    logger.info(f"âš ï¸ æ­¥éª¤3: æ¨¡å‹æœªåœ¨é…ç½®çš„ä½ç½®æ‰¾åˆ°ï¼Œå°†è”ç½‘ä¸‹è½½åˆ°: {transformers_cache}")
                    logger.info(f"   é…ç½®çš„ç¼“å­˜ç›®å½•: {transformers_cache}")
                    logger.info(f"   é…ç½®çš„ HF_HOME: {hf_home}")
                    # æ¨¡å‹å°†åœ¨è°ƒç”¨ partition æ—¶ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
            
            # âœ… å…³é”®ï¼šå¦‚æœæ‰¾åˆ°äº†æ¨¡å‹ï¼Œè®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
            if model_found and model_location:
                # è®¾ç½® HF_HUB_OFFLINE=1ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œä¸è®¿é—®ç½‘ç»œ
                os.environ.setdefault('HF_HUB_OFFLINE', '1')
                logger.debug(f"âœ… å·²è®¾ç½® HF_HUB_OFFLINE=1ï¼Œç¦ç”¨ Hugging Face ç½‘ç»œè¯·æ±‚")
            
            if not model_found:
                # è®¾ç½® Hugging Face ä¸‹è½½è¿›åº¦æ˜¾ç¤ºï¼ˆæ— è®ºæ˜¯å¦å…è®¸ä¸‹è½½éƒ½è®¾ç½®ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ï¼‰
                try:
                    os.environ.setdefault('HF_HUB_DISABLE_PROGRESS_BARS', '0')
                    from huggingface_hub.utils import disable_progress_bars
                    disable_progress_bars(False)
                except Exception:
                    pass
                
                if settings.UNSTRUCTURED_AUTO_DOWNLOAD_MODEL:
                    logger.info(f"âš ï¸ æœ¬åœ° YOLOX æ¨¡å‹ä¸å­˜åœ¨: {yolo_model_path}")
                    logger.info("ğŸ“¥ å·²å¯ç”¨è‡ªåŠ¨ä¸‹è½½ï¼Œé¦–æ¬¡ä½¿ç”¨ hi_res ç­–ç•¥æ—¶å°†è‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ¨¡å‹")
                    logger.info(f"ğŸ’¾ ä¸‹è½½åçš„æ¨¡å‹å°†ä¿å­˜åˆ°ç¼“å­˜ç›®å½•: {transformers_cache}")
                    # æ³¨æ„ï¼šå®é™…ä¸‹è½½ä¼šåœ¨ partition è°ƒç”¨æ—¶ï¼ˆä½¿ç”¨ hi_res ç­–ç•¥ï¼‰æ‰å‘ç”Ÿ
                    # æ¨¡å‹ä¸‹è½½åä¼šç¼“å­˜åœ¨ HF_HOME ç›®å½•ä¸‹
                else:
                    logger.warning(f"âš ï¸ æœ¬åœ° YOLOX æ¨¡å‹ä¸å­˜åœ¨: {yolo_model_path}")
                    logger.warning("âŒ UNSTRUCTURED_AUTO_DOWNLOAD_MODEL=Falseï¼Œç¦æ­¢è‡ªåŠ¨ä¸‹è½½")
                    logger.warning("ğŸ’¡ è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æˆ–è®¾ç½® UNSTRUCTURED_AUTO_DOWNLOAD_MODEL=True")
            
            logger.info(f"ğŸ“ Hugging Face æ¨¡å‹ç›®å½•: {hf_home}")
            logger.info(f"ğŸ“ Transformers ç¼“å­˜ç›®å½•: {transformers_cache}")
        except Exception as e:
            logger.error(f"âŒ é…ç½® Hugging Face æ¨¡å‹è·¯å¾„å¤±è´¥: {e}", exc_info=True)
        
        # æ³¨å…¥ Poppler åˆ° PATHï¼ˆWindows å¸¸è§é—®é¢˜ï¼‰
        try:
            if settings.POPPLER_PATH:
                poppler_bin = settings.POPPLER_PATH
                if os.path.isdir(poppler_bin) and poppler_bin not in os.environ.get('PATH', ''):
                    os.environ['PATH'] = poppler_bin + os.pathsep + os.environ.get('PATH', '')
                    os.environ.setdefault('POPPLER_PATH', poppler_bin)
                    logger.info(f"å·²æ³¨å…¥ POPPLER_PATH åˆ°ç¯å¢ƒ: {poppler_bin}")
            # æ³¨å…¥ Tesseractï¼ˆå¯é€‰ï¼‰
            if getattr(settings, 'TESSERACT_PATH', None):
                tess_bin = settings.TESSERACT_PATH
                if os.path.isdir(tess_bin) and tess_bin not in os.environ.get('PATH', ''):
                    os.environ['PATH'] = tess_bin + os.pathsep + os.environ.get('PATH', '')
                    logger.info(f"å·²æ³¨å…¥ TESSERACT_PATH åˆ°ç¯å¢ƒ: {tess_bin}")
            if getattr(settings, 'TESSDATA_PREFIX', None):
                os.environ.setdefault('TESSDATA_PREFIX', settings.TESSDATA_PREFIX)
                logger.info(f"è®¾ç½® TESSDATA_PREFIX: {settings.TESSDATA_PREFIX}")
        except Exception as e:
            logger.warning(f"æ³¨å…¥ POPPLER_PATH å¤±è´¥: {e}")
        # æ ¹æ®è®¾è®¡æ–‡æ¡£çš„è§£æç­–ç•¥é…ç½® - ä»settingsè¯»å–
        self.parsing_strategies = {
            'pdf': {
                'strategy': settings.UNSTRUCTURED_PDF_STRATEGY,
                'ocr_languages': settings.UNSTRUCTURED_PDF_OCR_LANGUAGES,
                'extract_images_in_pdf': settings.UNSTRUCTURED_PDF_EXTRACT_IMAGES,
                'extract_image_block_types': settings.UNSTRUCTURED_PDF_IMAGE_TYPES,
                # é«˜ä¿çœŸè§£æå¢å¼ºï¼šå¯ç”¨è¡¨æ ¼ç»“æ„ã€å…ƒæ•°æ®ã€ä¸­æ–‡è¯†åˆ«
                'infer_table_structure': True,
                'include_metadata': True,
                'languages': getattr(settings, 'UNSTRUCTURED_LANGUAGES', ['zh', 'en'])
            },
            'docx': {
                'strategy': settings.UNSTRUCTURED_DOCX_STRATEGY,
                'extract_images_in_pdf': settings.UNSTRUCTURED_DOCX_EXTRACT_IMAGES,
                # å¯¹ Office æ–‡æ¡£åŒæ ·å¯ç”¨ç»“æ„ä¸å…ƒæ•°æ®æå–ï¼ˆæœªè¢«æ”¯æŒçš„å‚æ•°å°†è¢«å¿½ç•¥ï¼‰
                'infer_table_structure': True,
                'include_metadata': True,
                'languages': getattr(settings, 'UNSTRUCTURED_LANGUAGES', ['zh', 'en'])
            },
            'pptx': {
                'strategy': settings.UNSTRUCTURED_PPTX_STRATEGY,
                'extract_images_in_pdf': settings.UNSTRUCTURED_PPTX_EXTRACT_IMAGES
            },
            'html': {
                'strategy': settings.UNSTRUCTURED_HTML_STRATEGY,
                'extract_images_in_pdf': settings.UNSTRUCTURED_HTML_EXTRACT_IMAGES
            },
            'txt': {
                'strategy': 'fast',
                'encoding': settings.UNSTRUCTURED_TXT_ENCODING
            }
        }
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡/æ¡†æ¶èƒ½åŠ›
        if settings.UNSTRUCTURED_AUTO_DEVICE:
            try:
                import torch  # type: ignore
                torch_version = getattr(torch, '__version__', 'unknown')
                use_cuda = torch.cuda.is_available()
                os.environ.setdefault('UNSTRUCTURED_DEVICE', 'cuda' if use_cuda else 'cpu')
                logger.info(f"Unstructured è®¾å¤‡: {'CUDA' if use_cuda else 'CPU'}, torch={torch_version}")
                # è‹¥é…ç½®äº† hi_res ä¸” torch ç‰ˆæœ¬è¿‡ä½ï¼Œè‡ªåŠ¨é™çº§
                from packaging import version
                if torch_version != 'unknown' and version.parse(torch_version) < version.parse('2.1.0'):
                    if self.parsing_strategies['pdf'].get('strategy') == 'hi_res':
                        self.parsing_strategies['pdf']['strategy'] = 'fast'
                        logger.warning("æ£€æµ‹åˆ° torch<2.1ï¼ŒPDF hi_res è‡ªåŠ¨é™çº§ä¸º fast")
            except Exception as _e:
                logger.warning(f"æœªæ£€æµ‹åˆ° torch æˆ–è‡ªåŠ¨è®¾å¤‡é…ç½®å¤±è´¥: {_e}. å°†ä½¿ç”¨ fast/CPU æµç¨‹ã€‚")
    
    def _select_parsing_strategy(self, file_type: str) -> str:
        """é€‰æ‹©è§£æç­–ç•¥ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        file_type = file_type.lower() if file_type else "unknown"
        
        # æ ¹æ®è®¾è®¡æ–‡æ¡£çš„è§£æç­–ç•¥é€‰æ‹©
        strategy_map = {
            'pdf': 'hi_res',  # é«˜åˆ†è¾¨ç‡è§£æã€OCRã€å›¾ç‰‡æå–
            'docx': 'fast',    # å¿«é€Ÿè§£æã€å›¾ç‰‡æå–
            'pptx': 'fast',    # å¿«é€Ÿè§£æã€å›¾ç‰‡æå–
            'html': 'fast',    # å¿«é€Ÿè§£æã€å›¾ç‰‡æå–
            'txt': 'fast',     # å¿«é€Ÿè§£æ
            'unknown': 'auto'  # è‡ªåŠ¨é€‰æ‹©
        }
        
        return strategy_map.get(file_type, 'auto')

    def _validate_docx_integrity(self, file_path: str) -> None:
        """DOCX è§£æå‰çš„å®Œæ•´æ€§æ ¡éªŒï¼šå¿…é¡»æ˜¯åˆæ³• OOXML zipï¼Œä¸”åŒ…å«æ ¸å¿ƒæ¡ç›®ã€‚
        ä¸é€šè¿‡ç›´æ¥æŠ›ä¸šåŠ¡å¼‚å¸¸ï¼ˆä¸é™çº§ï¼‰ã€‚"""
        try:
            with zipfile.ZipFile(file_path, 'r') as zf:
                names = set(zf.namelist())
        except Exception as e:
            raise CustomException(
                code=ErrorCode.DOCUMENT_PARSING_FAILED,
                message=f"DOCX æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ ZIPï¼ˆå¯èƒ½å·²æŸåï¼‰: {e}"
            )
        required = {"[Content_Types].xml", "_rels/.rels", "word/document.xml"}
        missing = [n for n in required if n not in names]
        if missing:
            raise CustomException(
                code=ErrorCode.DOCUMENT_PARSING_FAILED,
                message=f"DOCX ç»“æ„ç¼ºå°‘æ ¸å¿ƒæ¡ç›®: {', '.join(missing)}ï¼ˆæ–‡ä»¶å¯èƒ½ç”±éæ ‡å‡†å·¥å…·å¯¼å‡ºæˆ–å·²æŸåï¼‰"
            )
        # æ·±åº¦æ ¡éªŒï¼šrels ä¸­ä¸åº”å¼•ç”¨ NULLï¼Œä¸”å¼•ç”¨çš„éƒ¨ä»¶å¿…é¡»å­˜åœ¨
        try:
            with zipfile.ZipFile(file_path, 'r') as zf:
                # 1) ç¦æ­¢ä»»ä½•æ¡ç›®åä¸º 'NULL'
                upper_names = {name.upper() for name in zf.namelist()}
                if 'NULL' in upper_names:
                    raise CustomException(
                        code=ErrorCode.DOCUMENT_PARSING_FAILED,
                        message="DOCX åŒ…å«éæ³•éƒ¨ä»¶å 'NULL'ï¼Œæ–‡ä»¶å·²æŸåæˆ–å¯¼å‡ºä¸è§„èŒƒ"
                    )
                # 2) æ‰«ææ‰€æœ‰ .relsï¼Œæ£€æŸ¥ Target
                rels_files = [n for n in zf.namelist() if n.endswith('.rels')]
                for rel_path in rels_files:
                    with zf.open(rel_path) as fp:
                        tree = ET.parse(fp)
                        root = tree.getroot()
                        # å…³ç³»å‘½åç©ºé—´å¸¸è§ä¸º http://schemas.openxmlformats.org/package/2006/relationships
                        for rel in root.findall('.//{*}Relationship'):
                            target = rel.get('Target') or ''
                            if target.strip().upper() == 'NULL':
                                raise CustomException(
                                    code=ErrorCode.DOCUMENT_PARSING_FAILED,
                                    message=f"DOCX å…³ç³»æ–‡ä»¶ {rel_path} å¼•ç”¨äº†éæ³• Target='NULL'ï¼ˆæ–‡ä»¶æŸå/å¯¼å‡ºä¸è§„èŒƒï¼‰"
                                )
                            # ä»…æ ¡éªŒç›¸å¯¹è·¯å¾„ç›®æ ‡æ˜¯å¦å­˜åœ¨
                            if not (target.startswith('http://') or target.startswith('https://')):
                                # å½’ä¸€åŒ–æˆ zip å†…éƒ¨è·¯å¾„
                                base_dir = rel_path.rsplit('/', 1)[0] if '/' in rel_path else ''
                                normalized = f"{base_dir}/{target}" if base_dir else target
                                normalized = normalized.replace('\\', '/').lstrip('./')
                                if normalized and normalized not in zf.namelist():
                                    # æŸäº›ç›®æ ‡å¯èƒ½æ˜¯ä¸Šçº§ç›®å½•ï¼Œå¦‚ ../word/media/image1.pngï¼Œåšä¸€æ¬¡ç®€å•å½’ä¸€åŒ–
                                    while normalized.startswith('../'):
                                        normalized = normalized[3:]
                                    if normalized not in zf.namelist():
                                        raise CustomException(
                                            code=ErrorCode.DOCUMENT_PARSING_FAILED,
                                            message=f"DOCX å¼•ç”¨äº†ç¼ºå¤±çš„éƒ¨ä»¶: {target}ï¼ˆæ¥æº {rel_path}ï¼‰"
                                        )
        except CustomException:
            raise
        except Exception as e:
            # æ ¡éªŒè¿‡ç¨‹å¼‚å¸¸ï¼ŒæŒ‰è§£æå¤±è´¥å¤„ç†ï¼Œç»™å‡ºæ˜ç¡®æç¤º
            raise CustomException(
                code=ErrorCode.DOCUMENT_PARSING_FAILED,
                message=f"DOCX ç»“æ„æ ¡éªŒå¤±è´¥: {e}"
            )
    
    def _attempt_docx_repair(self, file_path: str) -> Optional[str]:
        """å°è¯•è‡ªåŠ¨ä¿®å¤å¸¸è§ DOCX å…³ç³»é”™è¯¯ï¼ˆofficeDocument å…³ç³»æŒ‡å‘ NULL æˆ–ç¼ºå¤±ï¼‰ã€‚
        ä¿®å¤æˆåŠŸè¿”å›æ–° docx ä¸´æ—¶è·¯å¾„ï¼Œå¦åˆ™è¿”å› Noneã€‚"""
        try:
            with zipfile.ZipFile(file_path, 'r') as zf:
                names = set(zf.namelist())
                if "_rels/.rels" not in names:
                    return None
                candidates = [n for n in names if n.startswith('word/document') and n.endswith('.xml')]
                if not candidates:
                    return None
                target_part = 'word/document.xml' if 'word/document.xml' in names else sorted(candidates)[0]
                with zf.open('_rels/.rels') as fp:
                    tree = ET.parse(fp)
                root = tree.getroot()
                modified = False
                for rel in root.findall('.{*}Relationship'):
                    pass
                for rel in root.findall('.//{*}Relationship'):
                    r_type = rel.get('Type') or ''
                    if r_type.endswith('/officeDocument'):
                        cur = (rel.get('Target') or '').strip()
                        normalized = cur.replace('\\','/').lstrip('./')
                        if cur.upper() == 'NULL' or normalized not in names:
                            rel.set('Target', target_part)
                            modified = True
                if not modified:
                    return None
                fd, tmp_path = tempfile.mkstemp(suffix='.docx')
                os.close(fd)
                with zipfile.ZipFile(tmp_path, 'w', zipfile.ZIP_DEFLATED) as out:
                    for name in zf.namelist():
                        if name == '_rels/.rels':
                            buf = io.BytesIO()
                            tree.write(buf, encoding='utf-8', xml_declaration=True)
                            out.writestr(name, buf.getvalue())
                        else:
                            out.writestr(name, zf.read(name))
                logger.warning(f"DOCX å·²è‡ªåŠ¨ä¿®å¤ officeDocument å…³ç³» -> {target_part}ï¼Œä¿®å¤æ–‡ä»¶: {tmp_path}")
                return tmp_path
        except Exception as e:
            logger.error(f"å°è¯•ä¿®å¤ DOCX å¤±è´¥: {e}")
            return None

    def parse_document(self, file_path: str, strategy: Optional[str] = None) -> Dict[str, Any]:
        """
        è§£ææ–‡æ¡£ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°ï¼ˆUnstructured å¤±è´¥å°†ç›´æ¥æŠ›é”™ï¼Œä¸åšé™çº§ï¼‰
        
        æ³¨æ„ï¼š
        1. å¦‚æœDOCXä¸å®Œæ•´ä¼šè½¬æ¢ä¸ºPDFï¼Œç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶ä¼šåœ¨è§£æå®Œæˆåè‡ªåŠ¨æ¸…ç†
        2. è½¬æ¢åçš„PDFè·¯å¾„ä¼šè¿”å›åœ¨result['converted_pdf_path']ä¸­ï¼Œè°ƒç”¨è€…è´Ÿè´£ä¿å­˜åˆ°MinIO
        """
        # è·Ÿè¸ªéœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿®å¤çš„DOCXã€è½¬æ¢çš„PDFåŠå…¶ä¸´æ—¶ç›®å½•ï¼‰
        temp_files_to_cleanup = []  # ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        temp_dirs_to_cleanup = []   # ä¸´æ—¶ç›®å½•è·¯å¾„åˆ—è¡¨
        original_file_path = file_path  # ä¿å­˜åŸå§‹æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæœ€ç»ˆè¿”å›ç»“æœ
        
        try:
            logger.info(f"å¼€å§‹è§£ææ–‡æ¡£: {file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                raise CustomException(
                    code=ErrorCode.FILE_NOT_FOUND,
                    message=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                )
            
            # è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶ï¼‰
            file_size = os.path.getsize(file_path)
            file_extension = os.path.splitext(file_path)[1].lower()
            logger.info(f"æ–‡ä»¶å¤§å°: {file_size} bytes, æ‰©å±•å: {file_extension}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£æç­–ç•¥
            file_type = self._get_file_type(file_extension)
            strategy_config = self.parsing_strategies.get(file_type, {})
            
            # å¦‚æœæä¾›äº†strategyå‚æ•°ï¼Œä½¿ç”¨å®ƒ
            if strategy:
                strategy_config['strategy'] = strategy
            
            logger.info(f"ä½¿ç”¨è§£æç­–ç•¥: {file_type}, é…ç½®: {strategy_config}")
            
            # DOCXï¼šå°è¯•ä¿®å¤/æ¸…æ´—ï¼›è‹¥ä»å¤±è´¥å¯é€‰è½¬ PDF å†è§£æ
            path_for_parse = file_path
            if file_type == 'docx':
                repaired_docx_path = None
                if settings.ENABLE_DOCX_REPAIR:
                    logger.info("[DOCX] å°è¯•è‡ªåŠ¨ä¿®å¤ä¸»æ–‡æ¡£å…³ç³»ä¸æ— æ•ˆå¼•ç”¨â€¦")
                    repaired_docx_path = self._attempt_docx_repair(file_path)
                    if repaired_docx_path:
                        path_for_parse = repaired_docx_path
                        temp_files_to_cleanup.append(repaired_docx_path)  # æ ‡è®°ä¸ºéœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶
                        logger.info(f"[DOCX] ä¿®å¤æˆåŠŸï¼Œä½¿ç”¨ä¿®å¤åçš„ä¸´æ—¶æ–‡ä»¶: {path_for_parse}")
                    else:
                        logger.info("[DOCX] æœªè¿›è¡Œä¿®å¤æˆ–æ— éœ€ä¿®å¤ï¼Œç»§ç»­æ ¡éªŒã€‚")
                
                try:
                    logger.info(f"[DOCX] å¼€å§‹å®Œæ•´æ€§æ ¡éªŒ: {path_for_parse}")
                    self._validate_docx_integrity(path_for_parse)
                    logger.info("[DOCX] å®Œæ•´æ€§æ ¡éªŒé€šè¿‡ã€‚")
                except CustomException as e:
                    logger.error(f"[DOCX] å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: {e}")
                    if settings.ENABLE_OFFICE_TO_PDF:
                        logger.info("[DOCX] å¯ç”¨ LibreOffice å…œåº•ï¼šå¼€å§‹ DOCXâ†’PDF è½¬æ¢â€¦")
                        pdf_path = convert_docx_to_pdf(path_for_parse)
                        if not pdf_path:
                            logger.error("[DOCX] LibreOffice è½¬æ¢å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è§£æã€‚")
                            raise
                        
                        # PDFè½¬æ¢æˆåŠŸï¼Œè®°å½•PDFè·¯å¾„ï¼ˆä½†ä¸ç«‹å³æ¸…ç†ï¼Œç”±è°ƒç”¨è€…è´Ÿè´£ï¼‰
                        file_type = 'pdf'
                        path_for_parse = pdf_path
                        # é‡è¦ï¼šè½¬æ¢åçš„PDFä¸ç«‹å³æ¸…ç†ï¼Œéœ€è¦åœ¨ä¿å­˜åˆ°MinIOåå†æ¸…ç†
                        # PDFæ–‡ä»¶ä¼šè¿”å›ç»™è°ƒç”¨è€…ï¼Œç”±è°ƒç”¨è€…å†³å®šä½•æ—¶æ¸…ç†
                        pdf_dir = os.path.dirname(pdf_path)
                        # PDFæ–‡ä»¶æœ¬èº«ä¸åŠ å…¥æ¸…ç†åˆ—è¡¨ï¼ˆç”±è°ƒç”¨è€…è´Ÿè´£ï¼‰
                        # ä½†PDFç›®å½•éœ€è¦è·Ÿè¸ªï¼Œä»¥ä¾¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿèƒ½æ¸…ç†ï¼ˆé€šè¿‡è°ƒç”¨è€…çš„finallyå—ï¼‰
                        # æ³¨æ„ï¼šæ­£å¸¸æµç¨‹ä¸­ï¼Œç›®å½•ä¼šåœ¨document_tasksä¸­æ¸…ç†
                        
                        strategy_config = self.parsing_strategies.get(file_type, {})
                        logger.info(f"[DOCX] è½¬æ¢ä¸º PDF æˆåŠŸï¼Œæ”¹ç”¨ PDF ç®¡çº¿è§£æ: {path_for_parse}")
                        logger.info(f"[DOCXâ†’PDF] PDFæ–‡ä»¶éœ€è¦ç”±è°ƒç”¨è€…ä¿å­˜åˆ°MinIOï¼Œæš‚ä¸è‡ªåŠ¨æ¸…ç†")
                    else:
                        raise
            
            # ä»…ä½¿ç”¨ Unstructuredï¼›ä»»ä½•å¼‚å¸¸å°†ç›´æ¥æŠ›å‡º
            logger.info(f"è°ƒç”¨ Unstructured.partitionï¼Œæ–‡ä»¶={path_for_parse}ï¼Œé…ç½®={strategy_config}")
            
            # âš ï¸ é‡è¦ï¼šåœ¨è°ƒç”¨ partition ä¹‹å‰ï¼Œæ£€æŸ¥æ‰€æœ‰å¯èƒ½éœ€è¦çš„æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
            # unstructured åº“å¯èƒ½åŠ è½½å¤šä¸ªæ¨¡å‹ï¼š
            # 1. YOLOX å¸ƒå±€æ£€æµ‹æ¨¡å‹ï¼ˆhi_res ç­–ç•¥ï¼‰
            # 2. ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹ï¼ˆinfer_table_structure=Trueï¼‰
            
            # âœ… å…³é”®ï¼šç¡®ä¿ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•ï¼Œä¸ä½¿ç”¨é»˜è®¤ä½ç½®
            hf_home = settings.HF_HOME or settings.UNSTRUCTURED_MODELS_DIR
            transformers_cache = os.path.join(hf_home, 'transformers_cache')
            hf_hub_cache = os.path.join(hf_home, 'hub')
            
            # âœ… å…³é”®ï¼šå†æ¬¡å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ partition è°ƒç”¨æ—¶ä½¿ç”¨é…ç½®çš„ç›®å½•
            os.environ['HF_HOME'] = hf_home
            os.environ['HF_HUB_CACHE'] = hf_hub_cache
            os.environ['TRANSFORMERS_CACHE'] = transformers_cache
            logger.debug(f"è°ƒç”¨ partition å‰ç¡®è®¤ç¼“å­˜ç›®å½•: HF_HUB_CACHE={hf_hub_cache}")
            
            def _check_model_in_directory(directory, file_pattern=None, min_size_mb=1):
                """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶"""
                if not os.path.exists(directory):
                    return False
                try:
                    for root, dirs, files in os.walk(directory):
                        for file in files:
                            file_path = os.path.join(root, file)
                            # æ£€æŸ¥æ–‡ä»¶å¤§å°
                            try:
                                file_size = os.path.getsize(file_path)
                                if file_size < min_size_mb * 1024 * 1024:
                                    continue
                                
                                # å¦‚æœæŒ‡å®šäº†æ¨¡å¼ï¼Œæ£€æŸ¥æ–‡ä»¶å
                                if file_pattern:
                                    if file_pattern(file.lower()):
                                        return True
                                else:
                                    # é»˜è®¤æ£€æŸ¥å¸¸è§æ¨¡å‹æ–‡ä»¶æ‰©å±•å
                                    if any(file.lower().endswith(ext) for ext in ['.onnx', '.pt', '.pth', '.safetensors', '.bin']):
                                        return True
                            except OSError:
                                continue
                except Exception:
                    pass
                return False
            
            # âœ… æ¨¡å‹æ£€æŸ¥é¡ºåºï¼ˆä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼‰ï¼š
            # 1. é¦–å…ˆæ£€æŸ¥æŒ‡å®šè·¯å¾„ï¼ˆsettings.py é…ç½®çš„æœ¬åœ°è·¯å¾„ï¼‰
            # 2. ç„¶åæ£€æŸ¥é…ç½®çš„ç¼“å­˜ç›®å½•ï¼ˆsettings.py é…ç½®çš„ç¼“å­˜ç›®å½•ï¼‰
            # 3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œå‡†å¤‡è”ç½‘ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
            
            # 1. æ£€æŸ¥ YOLOX æ¨¡å‹ï¼ˆä»…åœ¨ hi_res ç­–ç•¥æ—¶ï¼‰
            yolo_model_path = os.path.join(
                settings.UNSTRUCTURED_MODELS_DIR, 
                'yolo_x_layout', 
                'yolox_10.05.onnx'
            )
            
            yolo_model_exists = False
            yolo_model_path_found = None
            
            # æ­¥éª¤1ï¼šä¼˜å…ˆæ£€æŸ¥æŒ‡å®šè·¯å¾„ï¼ˆæœ€ä¼˜å…ˆï¼‰
            if os.path.exists(yolo_model_path):
                yolo_model_exists = True
                yolo_model_path_found = yolo_model_path
                # âœ… å…³é”®ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                os.environ['UNSTRUCTURED_LAYOUT_MODEL'] = yolo_model_path
                logger.info(f"âœ… æ­¥éª¤1: åœ¨æŒ‡å®šè·¯å¾„æ‰¾åˆ°æœ¬åœ° YOLOX æ¨¡å‹: {yolo_model_path}")
            elif strategy_config.get('strategy') == 'hi_res':
                # æ­¥éª¤2ï¼šæ£€æŸ¥é…ç½®çš„ç¼“å­˜ç›®å½•
                def _find_yolo_model_in_cache(cache_dir):
                    """åœ¨ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾ YOLOX æ¨¡å‹"""
                    if not os.path.exists(cache_dir):
                        return None
                    try:
                        for root, dirs, files in os.walk(cache_dir):
                            for file in files:
                                if file.endswith('.onnx') and ('yolo' in file.lower() or 'layout' in file.lower()):
                                    model_path = os.path.join(root, file)
                                    try:
                                        if os.path.getsize(model_path) > 1024 * 1024:  # >1MB
                                            return model_path
                                    except OSError:
                                        continue
                    except Exception:
                        pass
                    return None
                
                # âœ… åªæ£€æŸ¥é…ç½®çš„ç¼“å­˜ç›®å½•ï¼ˆsettings.py é…ç½®çš„ï¼‰
                cached_model = _find_yolo_model_in_cache(transformers_cache)
                if cached_model:
                    yolo_model_exists = True
                    yolo_model_path_found = cached_model
                    # âœ… å…³é”®ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹
                    os.environ['UNSTRUCTURED_LAYOUT_MODEL'] = cached_model
                    logger.info(f"âœ… æ­¥éª¤2: åœ¨é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­å‘ç° YOLOX æ¨¡å‹: {cached_model}")
                    logger.info(f"âœ… å·²è®¾ç½® UNSTRUCTURED_LAYOUT_MODEL ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œä¸ä¼šè®¿é—®ç½‘ç»œ")
                else:
                    # æ­¥éª¤3ï¼šæ¨¡å‹ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
                    logger.info(f"âš ï¸ æ­¥éª¤3: æ¨¡å‹æœªåœ¨é…ç½®çš„ä½ç½®æ‰¾åˆ°:")
                    logger.info(f"   æŒ‡å®šè·¯å¾„: {yolo_model_path}")
                    logger.info(f"   é…ç½®ç¼“å­˜ç›®å½•: {transformers_cache}")
                    logger.info(f"   å‡†å¤‡è”ç½‘ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•")
            
            # âœ… å…³é”®ï¼šå¦‚æœæ‰¾åˆ°äº†æ¨¡å‹ï¼Œè®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
            if yolo_model_exists and yolo_model_path_found:
                # âš ï¸ å¼ºåˆ¶è®¾ç½®å¤šä¸ªç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ä¸è®¿é—®ç½‘ç»œ
                os.environ['HF_HUB_OFFLINE'] = '1'  # å¼ºåˆ¶è¦†ç›–ï¼Œä¸ä½¿ç”¨ setdefault
                os.environ['TRANSFORMERS_OFFLINE'] = '1'  # transformers åº“çš„ç¦»çº¿æ¨¡å¼
                os.environ['HF_DATASETS_OFFLINE'] = '1'  # datasets åº“çš„ç¦»çº¿æ¨¡å¼
                
                # âœ… å…³é”®ï¼šç¡®è®¤ UNSTRUCTURED_LAYOUT_MODEL å·²è®¾ç½®
                if 'UNSTRUCTURED_LAYOUT_MODEL' not in os.environ:
                    os.environ['UNSTRUCTURED_LAYOUT_MODEL'] = yolo_model_path_found
                
                logger.info(f"âœ… å·²å¼ºåˆ¶è®¾ç½®ç¦»çº¿æ¨¡å¼: HF_HUB_OFFLINE=1, UNSTRUCTURED_LAYOUT_MODEL={yolo_model_path_found}")
                logger.debug(f"ç¯å¢ƒå˜é‡éªŒè¯: HF_HUB_OFFLINE={os.environ.get('HF_HUB_OFFLINE')}, "
                           f"UNSTRUCTURED_LAYOUT_MODEL={os.environ.get('UNSTRUCTURED_LAYOUT_MODEL')}")
            
            # 2. æ£€æŸ¥ ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨äº†è¡¨æ ¼ç»“æ„æ£€æµ‹ï¼‰
            resnet_model_exists = False
            resnet_model_location = None
            if strategy_config.get('infer_table_structure'):
                # ResNet18 æ¨¡å‹ç”± timm åº“åŠ è½½ï¼Œä½¿ç”¨ Hugging Face Hub çš„ç¼“å­˜æœºåˆ¶
                # timm ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ï¼štimm/resnet18.a1_in1k
                # ç¼“å­˜è·¯å¾„æ ¼å¼ï¼š~/.cache/huggingface/hub/models--timm--resnet18.a1_in1k/ æˆ–
                #               transformers_cache/hub/models--timm--resnet18.a1_in1k/
                
                def _check_resnet_model_specific_path(cache_base_dir):
                    """æ£€æŸ¥ ResNet18 æ¨¡å‹çš„ç‰¹å®šç¼“å­˜è·¯å¾„"""
                    if not os.path.exists(cache_base_dir):
                        return None
                    try:
                        # Hugging Face Hub çš„ç¼“å­˜è·¯å¾„æ ¼å¼
                        hub_cache = os.path.join(cache_base_dir, 'hub')
                        if os.path.exists(hub_cache):
                            # æŸ¥æ‰¾ timm/resnet18.a1_in1k æ¨¡å‹çš„ç¼“å­˜ç›®å½•
                            # è·¯å¾„æ ¼å¼ï¼šmodels--timm--resnet18.a1_in1k (ç©ºæ ¼å’Œæ–œæ è¢«æ›¿æ¢ä¸º --)
                            model_dirs = [
                                'models--timm--resnet18.a1_in1k',
                                'models--timm--resnet18',
                                'models--resnet18',
                            ]
                            for model_dir_name in model_dirs:
                                model_dir = os.path.join(hub_cache, model_dir_name)
                                if os.path.exists(model_dir):
                                    # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
                                    for root, dirs, files in os.walk(model_dir):
                                        for file in files:
                                            if (file.endswith('.safetensors') or 
                                                file.endswith('.bin') or 
                                                file.endswith('.pt') or 
                                                file.endswith('.pth')):
                                                file_path = os.path.join(root, file)
                                                try:
                                                    if os.path.getsize(file_path) > 10 * 1024 * 1024:  # >10MB
                                                        return file_path
                                                except OSError:
                                                    continue
                        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šçš„ hub ç›®å½•ï¼Œä½¿ç”¨é€šç”¨çš„é€’å½’æŸ¥æ‰¾
                        for root, dirs, files in os.walk(cache_base_dir):
                            for file in files:
                                file_lower = file.lower()
                                # ç²¾ç¡®åŒ¹é… resnet18 ç›¸å…³æ–‡ä»¶
                                if (('resnet18' in file_lower or 
                                     ('timm' in file_lower and 'resnet' in file_lower)) and
                                    (file.endswith('.safetensors') or 
                                     file.endswith('.bin') or 
                                     file.endswith('.pt') or 
                                     file.endswith('.pth'))):
                                    file_path = os.path.join(root, file)
                                    try:
                                        if os.path.getsize(file_path) > 10 * 1024 * 1024:  # >10MB
                                            return file_path
                                    except OSError:
                                        continue
                    except Exception:
                        pass
                    return None
                
                # æ£€æŸ¥æŒ‡å®šç¼“å­˜ç›®å½•
                cached_model = _check_resnet_model_specific_path(transformers_cache)
                if cached_model:
                    resnet_model_exists = True
                    resnet_model_location = cached_model
                    logger.info(f"âœ… åœ¨æŒ‡å®šç¼“å­˜ç›®å½•ä¸­å‘ç° ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹: {cached_model}")
                    logger.info(f"ğŸ’¡ timm åº“å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼ˆæ—¥å¿—ä¸­å¯èƒ½ä»æ˜¾ç¤º 'Loading from Hugging Face hub'ï¼Œä½†å®é™…æ˜¯ä»ç¼“å­˜åŠ è½½ï¼‰")
                # âš ï¸ ä¸å†æ£€æŸ¥é»˜è®¤ç¼“å­˜ç›®å½• ~/.cache/huggingface
                # æ‰€æœ‰æ¨¡å‹éƒ½åº”è¯¥åœ¨é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­ï¼ˆHF_HOME æˆ– UNSTRUCTURED_MODELS_DIRï¼‰
                logger.debug(f"ResNet18 æ¨¡å‹æœªåœ¨é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­æ‰¾åˆ°ï¼Œå°†ä»…åœ¨é…ç½®ç›®å½•ä¸­æŸ¥æ‰¾æˆ–ä¸‹è½½")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸‹è½½ YOLOX æ¨¡å‹
            need_yolo_download = (
                strategy_config.get('strategy') == 'hi_res' and 
                not yolo_model_exists and 
                settings.UNSTRUCTURED_AUTO_DOWNLOAD_MODEL
            )
            
            if need_yolo_download:
                # è®°å½•ä¸‹è½½å¼€å§‹ï¼ˆä»…åœ¨ç¡®å®éœ€è¦ä½¿ç”¨ hi_res ä¸”æœ¬åœ°/ç¼“å­˜ä¸­éƒ½ä¸å­˜åœ¨æ—¶ï¼‰
                log_download_start(
                    model_name="YOLOX å¸ƒå±€æ£€æµ‹æ¨¡å‹",
                    source="Hugging Face",
                    estimated_size="10-50 MB"
                )
                logger.info(f"ğŸ’¾ æ¨¡å‹å°†ä¸‹è½½åˆ°ç¼“å­˜ç›®å½•: {transformers_cache}")
            
            # ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹çš„ä¸‹è½½åˆ¤æ–­
            need_resnet_download = (
                strategy_config.get('infer_table_structure') and 
                not resnet_model_exists and 
                settings.UNSTRUCTURED_AUTO_DOWNLOAD_MODEL
            )
            
            if need_resnet_download:
                # è®°å½•ä¸‹è½½å¼€å§‹ï¼ˆä»…åœ¨ç¡®å®éœ€è¦è¡¨æ ¼ç»“æ„æ£€æµ‹ä¸”ç¼“å­˜ä¸­ä¸å­˜åœ¨æ—¶ï¼‰
                log_download_start(
                    model_name="ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹",
                    source="Hugging Face",
                    estimated_size="50-100 MB"
                )
                logger.info(f"ğŸ’¾ æ¨¡å‹å°†ä¸‹è½½åˆ°ç¼“å­˜ç›®å½•: {transformers_cache}")
                logger.info(f"ğŸ“ æ³¨æ„ï¼štimm åº“åŠ è½½æ—¶æ—¥å¿—å¯èƒ½æ˜¾ç¤º 'Loading from Hugging Face hub'ï¼Œè¿™æ˜¯æ­£å¸¸çš„æ—¥å¿—è¾“å‡º")
            elif strategy_config.get('infer_table_structure') and resnet_model_exists:
                # æ˜ç¡®å‘ŠçŸ¥ç¼“å­˜å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç¼“å­˜
                logger.info(f"âœ… ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹å·²åœ¨ç¼“å­˜ä¸­: {resnet_model_location}")
                logger.info(f"ğŸ’¡ å°†ç›´æ¥ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½ï¼ˆtimm åº“çš„æ—¥å¿—å¯èƒ½ä»æ˜¾ç¤º 'Loading from Hugging Face hub'ï¼Œä½†å®é™…æ˜¯ä»ç¼“å­˜åŠ è½½ï¼‰")
            
            try:
                # âœ… å…³é”®ï¼šåœ¨è°ƒç”¨ partition ä¹‹å‰ï¼Œæœ€åä¸€æ¬¡å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡
                # æ ¹æ®æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œè®¾ç½®ä¸åŒçš„ç­–ç•¥ï¼š
                # 1. å¦‚æœæ¨¡å‹å­˜åœ¨ï¼šè®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                # 2. å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼šç¡®ä¿ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
                
                if yolo_model_exists and yolo_model_path_found:
                    # âœ… ç­–ç•¥1ï¼šæ¨¡å‹å·²å­˜åœ¨ï¼Œè®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                    os.environ['HF_HUB_OFFLINE'] = '1'
                    os.environ['TRANSFORMERS_OFFLINE'] = '1'
                    os.environ['HF_DATASETS_OFFLINE'] = '1'
                    os.environ['UNSTRUCTURED_LAYOUT_MODEL'] = yolo_model_path_found
                    
                    logger.info(f"âœ… æ¨¡å‹å·²å­˜åœ¨ï¼Œè®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹:")
                    logger.info(f"   UNSTRUCTURED_LAYOUT_MODEL={yolo_model_path_found}")
                    logger.info(f"   HF_HUB_OFFLINE=1ï¼ˆç¦ç”¨ç½‘ç»œè¯·æ±‚ï¼‰")
                else:
                    # âœ… ç­–ç•¥2ï¼šæ¨¡å‹ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½ï¼Œç¡®ä¿ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
                    # ç§»é™¤ç¦»çº¿æ¨¡å¼ï¼Œå…è®¸ä¸‹è½½
                    if 'HF_HUB_OFFLINE' in os.environ:
                        del os.environ['HF_HUB_OFFLINE']
                    if 'TRANSFORMERS_OFFLINE' in os.environ:
                        del os.environ['TRANSFORMERS_OFFLINE']
                    if 'HF_DATASETS_OFFLINE' in os.environ:
                        del os.environ['HF_DATASETS_OFFLINE']
                    
                    # âœ… å…³é”®ï¼šç¡®ä¿ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•
                    # huggingface_hub ä¼šä½¿ç”¨ HF_HUB_CACHE ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç›®å½•
                    os.environ['HF_HUB_CACHE'] = hf_hub_cache
                    os.environ['HF_HOME'] = hf_home
                    os.environ['TRANSFORMERS_CACHE'] = transformers_cache
                    
                    logger.info(f"ğŸ“¥ æ¨¡å‹ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•:")
                    logger.info(f"   HF_HUB_CACHE={hf_hub_cache}")
                    logger.info(f"   HF_HOME={hf_home}")
                    logger.info(f"   TRANSFORMERS_CACHE={transformers_cache}")
                
                # âš ï¸ å°è¯• monkey patch huggingface_hubï¼Œç¡®ä¿ä¸‹è½½åˆ°é…ç½®ç›®å½•ï¼ˆå¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼‰
                if not yolo_model_exists or not yolo_model_path_found:
                    try:
                        import huggingface_hub.file_download
                        original_hf_hub_download = getattr(huggingface_hub.file_download, 'hf_hub_download', None)
                        
                        if original_hf_hub_download:
                            def patched_hf_hub_download(*args, **kwargs):
                                """ç¡®ä¿æ¨¡å‹ä¸‹è½½åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•"""
                                # âœ… å¼ºåˆ¶ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•
                                # æ³¨æ„ï¼šä¸è¦åœ¨ kwargs ä¸­é‡å¤è®¾ç½® cache_dirï¼Œé¿å… "multiple values" é”™è¯¯
                                if 'cache_dir' not in kwargs:
                                    kwargs['cache_dir'] = hf_hub_cache
                                else:
                                    # å¦‚æœå·²ç»æŒ‡å®šäº† cache_dirï¼Œä½¿ç”¨é…ç½®çš„ç›®å½•è¦†ç›–
                                    kwargs['cache_dir'] = hf_hub_cache
                                
                                repo_id = args[0] if len(args) > 0 else kwargs.get('repo_id', 'unknown')
                                filename = args[1] if len(args) > 1 else kwargs.get('filename', 'unknown')
                                
                                logger.debug(f"ä¸‹è½½æ¨¡å‹ {repo_id}/{filename} åˆ°é…ç½®ç›®å½•: {hf_hub_cache}")
                                try:
                                    return original_hf_hub_download(*args, **kwargs)
                                except Exception as e:
                                    logger.warning(f"ä»é…ç½®ç›®å½•ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸå§‹å‡½æ•°: {e}")
                                    # ç§»é™¤ cache_dir å‚æ•°ï¼Œè®©åŸå§‹å‡½æ•°ä½¿ç”¨é»˜è®¤è¡Œä¸º
                                    kwargs.pop('cache_dir', None)
                                    return original_hf_hub_download(*args, **kwargs)
                            
                            # âœ… å¯ç”¨ monkey patchï¼Œç¡®ä¿ä¸‹è½½åˆ°é…ç½®ç›®å½•
                            huggingface_hub.file_download.hf_hub_download = patched_hf_hub_download
                            logger.debug(f"âœ… å·²å¯ç”¨ huggingface_hub monkey patchï¼Œç¡®ä¿ä¸‹è½½åˆ°é…ç½®ç›®å½•")
                    except Exception as e:
                        logger.debug(f"å‡†å¤‡ monkey patch huggingface_hub æ—¶å‡ºé”™ï¼ˆä¸å½±å“ä¸‹è½½ï¼‰: {e}")
                
                elements = partition(filename=path_for_parse, **strategy_config)
                logger.info(f"Unstructuredè§£æå®Œæˆï¼Œæå–åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                
                # å¦‚æœç¡®å®è¿›è¡Œäº† YOLOX ä¸‹è½½ï¼Œè®°å½•æˆåŠŸ
                if need_yolo_download:
                    log_download_success(
                        model_name="YOLOX å¸ƒå±€æ£€æµ‹æ¨¡å‹",
                        save_path=transformers_cache
                    )
                
                # å¦‚æœç¡®å®è¿›è¡Œäº† ResNet18 ä¸‹è½½ï¼Œè®°å½•æˆåŠŸ
                if need_resnet_download:
                    log_download_success(
                        model_name="ResNet18 è¡¨æ ¼ç»“æ„æ£€æµ‹æ¨¡å‹",
                        save_path=transformers_cache
                    )
                    
            except Exception as parse_error:
                # åˆ¤æ–­æ˜¯å¦æ˜¯æ¨¡å‹ä¸‹è½½ç›¸å…³çš„é”™è¯¯
                error_str = str(parse_error).lower()
                is_download_error = any(keyword in error_str for keyword in [
                    'download', 'huggingface', 'hub', 'network', 'connection', 
                    'timeout', 'unpack', 'http', 'https', 'ssl', 'certificate',
                    'yolo', 'layout', 'model', 'resnet', 'timm', 'table'
                ])
                
                if is_download_error:
                    log_download_error(
                        model_name="YOLOX å¸ƒå±€æ£€æµ‹æ¨¡å‹",
                        error=parse_error,
                        download_url="https://huggingface.co/unstructuredio/yolo_x_layout/resolve/main/yolox_10.05.onnx",
                        local_path=yolo_model_path,
                        readme_path="models/unstructured/README.md"
                    )
                    
                    raise CustomException(
                        code=ErrorCode.DOCUMENT_PARSING_FAILED,
                        message=f"YOLOXæ¨¡å‹ä¸‹è½½å¤±è´¥: {str(parse_error)}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° {yolo_model_path}ã€‚è¯¦è§ models/unstructured/README.md"
                    )
                else:
                    # å…¶ä»–è§£æé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise
            
            # ç¡®å®šå®é™…è§£æçš„æ–‡ä»¶ç±»å‹ï¼ˆå¯èƒ½æ˜¯è½¬æ¢åçš„PDFï¼‰
            # å¦‚æœWordæ–‡æ¡£è¢«è½¬æ¢ä¸ºPDFï¼Œåº”è¯¥æŒ‰PDFå¤„ç†ï¼ˆåº”ç”¨PDFç‰¹æœ‰çš„OCRå™ªå£°è¿‡æ»¤ï¼‰
            actual_file_type = os.path.splitext(path_for_parse)[1].lower()
            is_actually_pdf = actual_file_type == '.pdf'
            
            # å¤„ç†è§£æç»“æœï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶è·¯å¾„å’Œå¤§å°ï¼Œä½†ä¼ é€’å®é™…æ–‡ä»¶ç±»å‹ä¿¡æ¯ï¼‰
            result = self._process_parsed_elements(elements, original_file_path, file_size, is_pdf=is_actually_pdf)
            
            # å¦‚æœè½¬æ¢äº†PDFï¼Œè®°å½•PDFè·¯å¾„ï¼ˆè°ƒç”¨è€…éœ€è¦ä¿å­˜åˆ°MinIOï¼‰
            # æ³¨æ„ï¼šè½¬æ¢åçš„PDFä¸åœ¨finallyä¸­æ¸…ç†ï¼Œéœ€è¦è°ƒç”¨è€…ä¿å­˜åå†æ¸…ç†
            converted_pdf_path_in_result = None
            if path_for_parse != original_file_path and actual_file_type == '.pdf':
                converted_pdf_path_in_result = path_for_parse
                result['converted_pdf_path'] = path_for_parse  # ä¸´æ—¶PDFè·¯å¾„
                result['is_converted_pdf'] = True  # æ ‡è®°æ˜¯è½¬æ¢åçš„PDF
                logger.info(f"[DOCXâ†’PDF] å·²è½¬æ¢PDFï¼Œä¸´æ—¶è·¯å¾„: {path_for_parse}ï¼Œéœ€è¦ä¿å­˜åˆ°MinIOåå†æ¸…ç†")
                logger.info(f"[DOCXâ†’PDF] å·²åº”ç”¨PDFç‰¹æœ‰çš„OCRå™ªå£°è¿‡æ»¤ç­–ç•¥")
                
                # ä»æ¸…ç†åˆ—è¡¨ä¸­ç§»é™¤PDFæ–‡ä»¶ï¼ˆè°ƒç”¨è€…è´Ÿè´£æ¸…ç†ï¼‰
                if path_for_parse in temp_files_to_cleanup:
                    temp_files_to_cleanup.remove(path_for_parse)
                    logger.debug(f"[DOCXâ†’PDF] å·²å°†PDFä»è‡ªåŠ¨æ¸…ç†åˆ—è¡¨ä¸­ç§»é™¤: {path_for_parse}")
            
            logger.info(f"æ–‡æ¡£è§£æå®Œæˆï¼Œæå–åˆ° {len(result.get('text_content', ''))} å­—ç¬¦")
            return result
            
        except CustomException:
            raise
        except Exception as e:
            logger.error(f"æ–‡æ¡£è§£æé”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.DOCUMENT_PARSING_FAILED,
                message=f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}"
            )
        finally:
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼ˆæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥éƒ½è¦æ¸…ç†ï¼‰
            self._cleanup_temp_files(temp_files_to_cleanup, temp_dirs_to_cleanup)
    
    def extract_images(self, file_path: str) -> List[Dict[str, Any]]:
        """æå–å›¾ç‰‡ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æå–å›¾ç‰‡: {file_path}")
            
            # ä½¿ç”¨Unstructuredæå–å›¾ç‰‡
            elements = partition(
                filename=file_path,
                extract_images_in_pdf=True,
                extract_image_block_types=['Image', 'Table']
            )
            
            images = []
            for i, element in enumerate(elements):
                if element.category == 'Image':
                    image_info = {
                        'image_id': f"img_{i+1:03d}",
                        'element_id': element.element_id,
                        'image_type': 'image',
                        'page_number': getattr(element, 'metadata', {}).get('page_number', 1),
                        'coordinates': self._extract_coordinates(element),
                        'description': getattr(element, 'text', ''),
                        'ocr_text': getattr(element, 'text', ''),
                        'element_type': element.category
                    }
                    images.append(image_info)
                elif element.category == 'Table':
                    # è¡¨æ ¼è½¬å›¾ç‰‡
                    table_info = {
                        'image_id': f"table_{i+1:03d}",
                        'element_id': element.element_id,
                        'image_type': 'table',
                        'page_number': getattr(element, 'metadata', {}).get('page_number', 1),
                        'coordinates': self._extract_coordinates(element),
                        'description': 'è¡¨æ ¼å†…å®¹',
                        'ocr_text': getattr(element, 'text', ''),
                        'element_type': element.category,
                        'table_data': self._extract_table_data(element)
                    }
                    images.append(table_info)
            
            logger.info(f"å›¾ç‰‡æå–å®Œæˆï¼Œå…±æå–åˆ° {len(images)} å¼ å›¾ç‰‡")
            return images
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡æå–é”™è¯¯: {e}", exc_info=True)
            return []
    
    def extract_tables(self, file_path: str) -> List[Dict[str, Any]]:
        """æå–è¡¨æ ¼ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æå–è¡¨æ ¼: {file_path}")
            
            # ä½¿ç”¨Unstructuredæå–è¡¨æ ¼
            elements = partition(
                filename=file_path,
                extract_image_block_types=['Table']
            )
            
            tables = []
            for i, element in enumerate(elements):
                if element.category == 'Table':
                    table_info = {
                        'table_id': f"table_{i+1:03d}",
                        'element_id': element.element_id,
                        'page_number': getattr(element, 'metadata', {}).get('page_number', 1),
                        'coordinates': self._extract_coordinates(element),
                        'table_data': self._extract_table_data(element),
                        'table_text': getattr(element, 'text', ''),
                        'element_type': element.category
                    }
                    tables.append(table_info)
            
            logger.info(f"è¡¨æ ¼æå–å®Œæˆï¼Œå…±æå–åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
            return tables
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æå–é”™è¯¯: {e}", exc_info=True)
            return []
    
    def chunk_text(self, text: str, document_type: str = "auto", chunk_size: int = None, chunk_overlap: int = None, 
                   text_element_index_map: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
        """
        æ–‡æœ¬åˆ†å— - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°æ™ºèƒ½åˆ†å—ç­–ç•¥
        è¿”å›æ ¼å¼ï¼šList[Dict] åŒ…å« {'content': str, 'element_index_start': int, 'element_index_end': int}
        """
        try:
            # æ ¹æ®è®¾è®¡æ–‡æ¡£çš„åˆ†å—ç­–ç•¥é…ç½®
            strategies = {
                "semantic": {"chunk_size": 1000, "chunk_overlap": 200, "min_size": 100},
                "structure": {"chunk_size": 1500, "chunk_overlap": 150, "min_size": 200},
                "fixed": {"chunk_size": 512, "chunk_overlap": 50, "min_size": 100}
            }
            
            # é€‰æ‹©ç­–ç•¥
            strategy = strategies.get(document_type, strategies["semantic"])
            
            # ä½¿ç”¨æä¾›çš„å‚æ•°æˆ–é»˜è®¤ç­–ç•¥ï¼Œå¹¶ä¸æ¨¡å‹ä¸Šé™å¯¹é½ï¼Œé¿å…åç»­å‘é‡åŒ–æˆªæ–­
            from app.config.settings import settings as _settings
            model_max = int(getattr(_settings, 'TEXT_EMBED_MAX_CHARS', 1024))
            proposed = chunk_size or strategy["chunk_size"]
            final_chunk_size = min(proposed, model_max)
            final_chunk_overlap = chunk_overlap or strategy["chunk_overlap"]
            min_size = strategy["min_size"]
            
            logger.info(f"å¼€å§‹æ–‡æœ¬åˆ†å—ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}, ç­–ç•¥: {document_type}, åˆ†å—å¤§å°: {final_chunk_size}, é‡å : {final_chunk_overlap}")
            
            # æ ¹æ®è®¾è®¡æ–‡æ¡£çš„æ™ºèƒ½åˆ†å—ç­–ç•¥
            chunks = []
            current_chunk = ""
            current_chunk_start_pos = 0  # å½“å‰åˆ†å—åœ¨ text_content ä¸­çš„èµ·å§‹ä½ç½®
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = text.split('\n\n')
            
            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue
                
                # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šç°æœ‰åˆ†å—è¶…è¿‡å¤§å°é™åˆ¶
                if len(current_chunk) + len(paragraph) > final_chunk_size:
                    if current_chunk:
                        # è®¡ç®—å½“å‰åˆ†å—è¦†ç›–çš„ element_index èŒƒå›´
                        chunk_end_pos = current_chunk_start_pos + len(current_chunk)
                        element_index_start, element_index_end = self._get_element_index_range(
                            current_chunk_start_pos, chunk_end_pos, text_element_index_map
                        )
                        chunks.append({
                            'content': current_chunk.strip(),
                            'element_index_start': element_index_start,
                            'element_index_end': element_index_end
                        })
                        
                        # æ·»åŠ é‡å éƒ¨åˆ†
                        if final_chunk_overlap > 0 and len(chunks) > 0:
                            # è®¡ç®—é‡å éƒ¨åˆ†çš„èµ·å§‹ä½ç½®
                            overlap_start_pos = chunk_end_pos - final_chunk_overlap
                            overlap_element_start, _ = self._get_element_index_range(
                                overlap_start_pos, chunk_end_pos, text_element_index_map
                            )
                            current_chunk = chunks[-1]['content'][-final_chunk_overlap:] + "\n\n" + paragraph
                            current_chunk_start_pos = overlap_start_pos  # é‡å éƒ¨åˆ†çš„èµ·å§‹ä½ç½®
                        else:
                            current_chunk = paragraph
                            current_chunk_start_pos = chunk_end_pos + 2  # +2 for \n\n
                    else:
                        # æ®µè½æœ¬èº«å¤ªé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                        sub_chunks = self._split_long_paragraph(paragraph, final_chunk_size)
                        # å¤„ç†å‰é¢çš„å®Œæ•´åˆ†å—
                        for sub_chunk in sub_chunks[:-1]:
                            chunk_end_pos = current_chunk_start_pos + len(sub_chunk)
                            element_index_start, element_index_end = self._get_element_index_range(
                                current_chunk_start_pos, chunk_end_pos, text_element_index_map
                            )
                            chunks.append({
                                'content': sub_chunk.strip(),
                                'element_index_start': element_index_start,
                                'element_index_end': element_index_end
                            })
                            current_chunk_start_pos = chunk_end_pos + 2  # +2 for \n\n
                        # ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„åˆ†å—
                        current_chunk = sub_chunks[-1]
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
            
            # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
            if current_chunk:
                chunk_end_pos = current_chunk_start_pos + len(current_chunk)
                element_index_start, element_index_end = self._get_element_index_range(
                    current_chunk_start_pos, chunk_end_pos, text_element_index_map
                )
                chunks.append({
                    'content': current_chunk.strip(),
                    'element_index_start': element_index_start,
                    'element_index_end': element_index_end
                })
            
            # è¿‡æ»¤ç©ºåˆ†å—å’Œå°äºæœ€å°å°ºå¯¸çš„åˆ†å—
            chunks = [chunk for chunk in chunks if chunk.get('content', '').strip() and len(chunk.get('content', '')) >= min_size]
            
            logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
            
            # å…¼å®¹æ—§æ¥å£ï¼šå¦‚æœæ²¡æœ‰æä¾› text_element_index_mapï¼Œè¿”å›çº¯å­—ç¬¦ä¸²åˆ—è¡¨
            if text_element_index_map is None:
                return [chunk['content'] if isinstance(chunk, dict) else chunk for chunk in chunks]
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†å—é”™è¯¯: {e}", exc_info=True)
            # é”™è¯¯æ—¶è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºå•ä¸ªåˆ†å—
            if text_element_index_map:
                element_index_start = text_element_index_map[0]['element_index'] if text_element_index_map else 0
                element_index_end = text_element_index_map[-1]['element_index'] if text_element_index_map else 0
                return [{'content': text, 'element_index_start': element_index_start, 'element_index_end': element_index_end}]
            return [text]
    
    def _get_element_index_range(self, start_pos: int, end_pos: int, 
                                 text_element_index_map: Optional[List[Dict[str, Any]]]) -> tuple:
        """
        æ ¹æ®æ–‡æœ¬ä½ç½®èŒƒå›´ï¼Œè®¡ç®—å¯¹åº”çš„ element_index èŒƒå›´
        è¿”å›: (element_index_start, element_index_end)
        """
        if not text_element_index_map:
            return (None, None)
        
        element_indices = []
        for map_item in text_element_index_map:
            map_start = map_item.get('start_pos', 0)
            map_end = map_item.get('end_pos', 0)
            # å¦‚æœæ–‡æœ¬æ®µè½çš„èŒƒå›´ä¸åˆ†å—èŒƒå›´æœ‰é‡å 
            if not (map_end < start_pos or map_start > end_pos):
                element_indices.append(map_item.get('element_index'))
        
        if not element_indices:
            return (None, None)
        
        return (min(element_indices), max(element_indices))
    
    def _is_table_of_contents(self, element_text: str, element_category: str, coordinates: Dict[str, Any], page_number: int) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯ç›®å½•
        
        ç›®å½•ç‰¹å¾ï¼š
        1. åŒ…å«ç« èŠ‚ç¼–å·æ¨¡å¼ï¼ˆå¦‚ 1.1, 1.2.1, 2.3.4 ç­‰ï¼‰
        2. åŒ…å«é¡µç æ•°å­—ï¼ˆé€šå¸¸åœ¨è¡Œå°¾ï¼‰
        3. å¯èƒ½åŒ…å«"ç›®å½•"ã€"Table of Contents"ç­‰å…³é”®è¯
        4. å¯èƒ½åŒ…å«è¿æ¥ç‚¹ï¼ˆ... æˆ–å¤šä¸ªç©ºæ ¼åˆ†éš”ï¼‰
        5. è¡¨æ ¼ç±»å‹ä½†å†…å®¹æ˜¯ç›®å½•ç»“æ„
        """
        if not element_text or len(element_text.strip()) < 5:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®å½•å…³é”®è¯
        toc_keywords = [
            r'ç›®å½•', r'ç›®\s*å½•', r'Table\s+of\s+Contents', r'CONTENTS',
            r'åˆ†å—å†…å®¹', r'ç« èŠ‚ç›®å½•', r'å†…å®¹ç›®å½•'  # ç§»é™¤å¤ªå®½æ³›çš„"å†…å®¹"ã€"ç« èŠ‚"
        ]
        has_toc_keyword = any(re.search(keyword, element_text, re.IGNORECASE) for keyword in toc_keywords)
        
        # æ£€æŸ¥ç« èŠ‚ç¼–å·æ¨¡å¼ï¼ˆå¦‚ 1.1, 1.2.1, 2.3.4.5ï¼‰
        # æ”¯æŒå¤šç§æ ¼å¼ï¼š1.1ã€1.1.1ã€I.1ã€ä¸€ã€1.2.3.4 ç­‰
        section_patterns = [
            r'\d+\.\d+(?:\.\d+)*',  # 1.1, 1.2.3, 1.2.3.4
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]',  # ä¸€ã€äºŒã€ä¸‰ã€
            r'[IVX]+\.\d+',  # I.1, II.2.3
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
        ]
        has_section_number = any(re.search(pattern, element_text) for pattern in section_patterns)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¡µç ï¼ˆæ•°å­—åœ¨è¡Œå°¾ï¼Œå¯èƒ½è¢«ç‚¹åˆ†éš”ï¼‰
        # é¡µç é€šå¸¸åœ¨è¡Œæœ«ï¼Œå¯èƒ½å‰åæœ‰ç©ºæ ¼æˆ–ç‚¹
        page_number_patterns = [
            r'\d+\s*$',  # è¡Œå°¾æ•°å­—
            r'\.\.\.+\s*\d+',  # ... 123
            r'\s+\d+\s*$',  # å¤šä¸ªç©ºæ ¼åè·Ÿæ•°å­—åœ¨è¡Œå°¾
        ]
        has_page_number = any(re.search(pattern, element_text, re.MULTILINE) for pattern in page_number_patterns)
        
        # æ£€æŸ¥è¿æ¥ç‚¹æ¨¡å¼ï¼ˆç›®å½•å¸¸ç”¨ ... æˆ–ç©ºæ ¼åˆ†éš”æ ‡é¢˜å’Œé¡µç ï¼‰
        has_leader_dots = '...' in element_text or 'â€¦' in element_text
        
        # å¦‚æœæ˜¯Tableç±»å‹ï¼Œä½†å†…å®¹ç¬¦åˆç›®å½•ç‰¹å¾ï¼Œå¾ˆå¯èƒ½æ˜¯è¯¯è¯†åˆ«
        if element_category == 'Table':
            # è¡¨æ ¼è¢«è¯†åˆ«ä¸ºç›®å½•çš„æ¡ä»¶æ›´ä¸¥æ ¼
            if has_toc_keyword and (has_section_number or has_page_number):
                return True
            if has_section_number and has_page_number and has_leader_dots:
                return True
            # æ£€æŸ¥è¡¨æ ¼å•å…ƒæ ¼æ˜¯å¦å‘ˆç°ç›®å½•ç»“æ„ï¼ˆå¤šæ•°è¡ŒåŒ…å«ç« èŠ‚å·+é¡µç ï¼‰
            lines = element_text.split('\n')
            section_lines = 0
            lines_to_check = lines[:10]  # åªæ£€æŸ¥å‰10è¡Œ
            if len(lines_to_check) > 0:
                for line in lines_to_check:
                    if any(re.search(pattern, line) for pattern in section_patterns):
                        if any(re.search(pattern, line) for pattern in page_number_patterns):
                            section_lines += 1
                if section_lines / len(lines_to_check) > 0.5:  # è¶…è¿‡50%çš„è¡Œç¬¦åˆç›®å½•æ¨¡å¼
                    return True
        
        # å¯¹äºæ–‡æœ¬ç±»å‹ï¼Œæ¡ä»¶ç¨å¾®å®½æ¾
        if element_category not in ['Table', 'Image']:
            if has_toc_keyword:
                return True
            if has_section_number and has_page_number:
                return True
            if has_section_number and has_leader_dots:
                return True
        
        return False
    
    def _is_header_or_footer(self, element_text: str, element_category: str, coordinates: Dict[str, Any], page_number: int, page_height: float = 0) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯é¡µçœ‰æˆ–é¡µè„š
        
        é¡µçœ‰é¡µè„šç‰¹å¾ï¼š
        1. ä½ç½®åœ¨é¡µé¢é¡¶éƒ¨æˆ–åº•éƒ¨ï¼ˆæ ¹æ®åæ ‡åˆ¤æ–­ï¼‰- âš ï¸ ä¼˜å…ˆä½¿ç”¨åæ ‡åˆ¤æ–­
        2. é€šå¸¸åŒ…å«é¡µç ã€æ–‡æ¡£æ ‡é¢˜ã€æ—¥æœŸç­‰
        3. å¯èƒ½æ¯é¡µé‡å¤å‡ºç°
        4. æ–‡æœ¬é€šå¸¸è¾ƒçŸ­
        5. å¯èƒ½åŒ…å«é¡µçœ‰é¡µè„šå…³é”®è¯ï¼ˆä½œä¸ºè¡¥å……åˆ¤æ–­ï¼‰
        
        åˆ¤æ–­ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ï¼‰ï¼š
        1. ä¼˜å…ˆä½¿ç”¨åæ ‡ä½ç½®åˆ¤æ–­ï¼ˆæœ€å‡†ç¡®ï¼‰
        2. å¦‚æœåæ ‡ä¿¡æ¯ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆå…³é”®è¯ã€é•¿åº¦ç­‰ï¼‰
        """
        if not element_text:
            return False
        
        text_stripped = element_text.strip()
        
        # âš ï¸ ç­–ç•¥1ï¼šä¼˜å…ˆä½¿ç”¨åæ ‡ä½ç½®åˆ¤æ–­ï¼ˆæœ€å‡†ç¡®å¯é ï¼‰
        if coordinates and page_height > 0:
            y_pos = coordinates.get('y', 0)
            element_height = coordinates.get('height', 0)
            element_bottom = y_pos + element_height
            
            # âœ… ä¼˜åŒ–ï¼šæ›´ä¸¥æ ¼çš„é¡µçœ‰é¡µè„šåŒºåŸŸåˆ¤æ–­
            # é¡µçœ‰ï¼šåœ¨é¡µé¢é¡¶éƒ¨15%ä»¥å†…ï¼ˆæ›´ä¸¥æ ¼ï¼Œé¿å…è¯¯åˆ¤æ­£æ–‡æ ‡é¢˜ï¼‰
            # é¡µè„šï¼šåœ¨é¡µé¢åº•éƒ¨15%ä»¥å†…ï¼ˆæ›´ä¸¥æ ¼ï¼Œé¿å…è¯¯åˆ¤æ­£æ–‡ï¼‰
            # åŒæ—¶è€ƒè™‘å…ƒç´ æœ¬èº«çš„é«˜åº¦ï¼Œå¦‚æœå…ƒç´ å¾ˆé«˜ï¼ˆå é¡µé¢è¶…è¿‡50%ï¼‰ï¼Œä¸åº”è¢«åˆ¤æ–­ä¸ºé¡µçœ‰é¡µè„š
            element_height_ratio = element_height / page_height if page_height > 0 else 0
            
            # åªæœ‰å½“å…ƒç´ é«˜åº¦ä¸è¶…è¿‡é¡µé¢çš„50%æ—¶ï¼Œæ‰è¿›è¡Œé¡µçœ‰é¡µè„šåˆ¤æ–­
            if element_height_ratio <= 0.5:
                is_in_header_zone = y_pos < page_height * 0.15  # ä»20%è°ƒæ•´ä¸º15%ï¼Œæ›´ä¸¥æ ¼
                is_in_footer_zone = element_bottom > page_height * 0.85  # ä»80%è°ƒæ•´ä¸º85%ï¼Œæ›´ä¸¥æ ¼
            else:
                # å…ƒç´ å¤ªé«˜ï¼Œä¸å¯èƒ½æ˜¯é¡µçœ‰é¡µè„š
                is_in_header_zone = False
                is_in_footer_zone = False
            
            if is_in_header_zone or is_in_footer_zone:
                # ä½ç½®ç¬¦åˆé¡µçœ‰é¡µè„šåŒºåŸŸï¼Œè¿›ä¸€æ­¥éªŒè¯å†…å®¹ç‰¹å¾
                text_len = len(text_stripped)
                
                # âœ… ä¼˜åŒ–ï¼šæ›´ä¸¥æ ¼çš„é¡µçœ‰é¡µè„šåˆ¤æ–­
                # é¡µçœ‰é¡µè„šé€šå¸¸æ–‡æœ¬è¾ƒçŸ­ï¼Œä¸”ä½ç½®åœ¨é¡µé¢è¾¹ç¼˜
                # å¯¹äºä½ç½®åœ¨é¡µçœ‰é¡µè„šåŒºåŸŸçš„å…ƒç´ ï¼Œéœ€è¦åŒæ—¶æ»¡è¶³ä½ç½®å’Œå†…å®¹ç‰¹å¾
                
                # 1. å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆ<50å­—ç¬¦ï¼‰ï¼Œä½ç½®åˆç¬¦åˆï¼Œå¾ˆå¯èƒ½æ˜¯é¡µçœ‰é¡µè„š
                if text_len < 50:
                    # å¯¹äºéå¸¸çŸ­çš„æ–‡æœ¬ï¼ˆ<30å­—ç¬¦ï¼‰ï¼Œä½ç½®ç¬¦åˆå°±è®¤ä¸ºæ˜¯é¡µçœ‰é¡µè„š
                    if text_len < 30:
                        return True
                    
                    # å¯¹äºç¨é•¿çš„æ–‡æœ¬ï¼ˆ30-50å­—ç¬¦ï¼‰ï¼Œéœ€è¦é¢å¤–éªŒè¯å†…å®¹ç‰¹å¾
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„é¡µçœ‰é¡µè„šå†…å®¹
                    # é¡µç æ ¼å¼
                    page_number_patterns = [
                        r'^é¡µ\s*\d+', r'^ç¬¬\s*\d+\s*é¡µ', r'^Page\s+\d+', r'^PAGE\s+\d+',
                        r'^å…±\s*\d+\s*é¡µ', r'^\d+/\d+$',  # "1/10"æ ¼å¼
                        r'^\s*\d+\s*$',  # çº¯é¡µç æ•°å­—
                    ]
                    if any(re.search(pattern, text_stripped, re.IGNORECASE) for pattern in page_number_patterns):
                        return True
                    
                    # æ—¥æœŸæ ¼å¼
                    date_pattern = r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?'
                    if re.search(date_pattern, text_stripped):
                        return True
                    
                    # ä¿å¯†å£°æ˜ã€å†…éƒ¨èµ„æ–™ï¼ˆé€šå¸¸åœ¨é¡µçœ‰é¡µè„šï¼‰
                    header_footer_content_patterns = [
                        r'å†…éƒ¨èµ„æ–™', r'è¯·å‹¿å¤–ä¼ ', r'ç¦æ­¢å¤–ä¼ ', r'ä¿å¯†', r'æœºå¯†',
                        r'ä»…ä¾›.*ä½¿ç”¨', r'ä¸å¾—.*ä¼ æ’­',
                        # å…¬å¸åç§°å•ç‹¬å‡ºç°ï¼ˆé€šå¸¸æ˜¯é¡µçœ‰é¡µè„šï¼‰
                        r'^[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸$',
                        r'^[\w\sï¼ˆï¼‰()\-]+è‚¡ä»½æœ‰é™å…¬å¸$',
                        r'^[\w\sï¼ˆï¼‰()\-]+é›†å›¢$',
                        r'^[\w\sï¼ˆï¼‰()\-]+å…¬å¸$',
                    ]
                    if any(re.search(pattern, text_stripped, re.IGNORECASE) for pattern in header_footer_content_patterns):
                        return True
                    
                    # å…¬å¸åç§° + ä¿å¯†å£°æ˜ç»„åˆ
                    company_secret_patterns = [
                        r'[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸.*å†…éƒ¨èµ„æ–™',
                        r'[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸.*è¯·å‹¿å¤–ä¼ ',
                        r'å†…éƒ¨èµ„æ–™.*[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸',
                        r'è¯·å‹¿å¤–ä¼ .*[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸',
                    ]
                    if any(re.search(pattern, text_stripped, re.IGNORECASE) for pattern in company_secret_patterns):
                        return True
                else:
                    # æ–‡æœ¬è¾ƒé•¿ï¼ˆ>=50å­—ç¬¦ï¼‰ï¼Œå³ä½¿ä½ç½®ç¬¦åˆï¼Œä¹Ÿå¾ˆå¯èƒ½æ˜¯æ­£æ–‡ï¼ˆå¦‚å°æ ‡é¢˜ï¼‰
                    return False
        
        # âš ï¸ ç­–ç•¥2ï¼šå¦‚æœæ²¡æœ‰åæ ‡ä¿¡æ¯ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
        # æ³¨æ„ï¼šè¿™ç§æ–¹å¼å¯èƒ½è¯¯åˆ¤ï¼Œä½†æ¯”å®Œå…¨ä¸åˆ¤æ–­è¦å¥½
        
        # æ£€æŸ¥å…¸å‹çš„é¡µçœ‰é¡µè„šå…³é”®è¯å’Œå†…å®¹æ¨¡å¼
        header_footer_keywords = [
            r'^é¡µ\s*\d+', r'^ç¬¬\s*\d+\s*é¡µ', r'^Page\s+\d+', r'^PAGE\s+\d+',
            r'^å…±\s*\d+\s*é¡µ', r'^\d+/\d+$',  # é¡µç æ ¼å¼å¦‚ "1/10"
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…¸å‹çš„é¡µçœ‰é¡µè„šå†…å®¹ï¼ˆä¿å¯†å£°æ˜ã€å…¬å¸åç§°ç­‰ï¼‰
        header_footer_content_patterns = [
            # ä¿å¯†å£°æ˜ã€å†…éƒ¨èµ„æ–™
            r'å†…éƒ¨èµ„æ–™', r'è¯·å‹¿å¤–ä¼ ', r'ç¦æ­¢å¤–ä¼ ', r'ä¿å¯†', r'æœºå¯†',
            r'ä»…ä¾›.*ä½¿ç”¨', r'ä¸å¾—.*ä¼ æ’­',
            # å…¬å¸åç§°å•ç‹¬å‡ºç°ï¼ˆé€šå¸¸æ˜¯é¡µçœ‰é¡µè„šï¼‰
            r'^[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸$',  # çº¯å…¬å¸åç§°ï¼Œå¦‚ "XXæœ‰é™å…¬å¸"
            r'^[\w\sï¼ˆï¼‰()\-]+è‚¡ä»½æœ‰é™å…¬å¸$',
            r'^[\w\sï¼ˆï¼‰()\-]+é›†å›¢$',
            r'^[\w\sï¼ˆï¼‰()\-]+å…¬å¸$',
            # å…¬å¸åç§° + ä¿å¯†å£°æ˜ç»„åˆ
            r'[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸.*å†…éƒ¨èµ„æ–™',
            r'[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸.*è¯·å‹¿å¤–ä¼ ',
            r'å†…éƒ¨èµ„æ–™.*[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸',
            r'è¯·å‹¿å¤–ä¼ .*[\w\sï¼ˆï¼‰()\-]+æœ‰é™å…¬å¸',
        ]
        
        # å¦‚æœåŒ¹é…é¡µçœ‰é¡µè„šå†…å®¹æ¨¡å¼ï¼Œä¸”æ–‡æœ¬è¾ƒçŸ­ï¼Œå¯èƒ½æ˜¯é¡µçœ‰é¡µè„š
        if len(text_stripped) < 60:  # é¡µçœ‰é¡µè„šé€šå¸¸è¾ƒçŸ­
            if any(re.search(pattern, text_stripped, re.IGNORECASE) for pattern in header_footer_content_patterns):
                return True
            
            # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆå°‘äº30å­—ç¬¦ï¼‰ä¸”åŒ¹é…é¡µçœ‰é¡µè„šå…³é”®è¯æ¨¡å¼
            if len(text_stripped) < 30:
                if any(re.search(keyword, text_stripped, re.IGNORECASE) for keyword in header_footer_keywords):
                    return True
                # çº¯æ•°å­—ï¼ˆå¯èƒ½æ˜¯é¡µç ï¼‰
                if re.match(r'^\d+$', text_stripped):
                    # æ’é™¤æ˜æ˜¾ä¸æ˜¯é¡µç çš„é•¿æ•°å­—
                    if len(text_stripped) <= 4:  # é¡µç é€šå¸¸ä¸è¶…è¿‡4ä½
                        return True
        
        # å¦‚æœå†…å®¹åªåŒ…å«æ—¥æœŸã€é¡µç ç­‰ç®€çŸ­ä¿¡æ¯
        if len(text_stripped) < 20:
            date_pattern = r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?'
            if re.search(date_pattern, text_stripped):
                return True
        
        return False
    
    def _is_blank_content(self, element_text: str) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯ç©ºç™½å†…å®¹
        
        ç©ºç™½å†…å®¹ç‰¹å¾ï¼š
        1. æ–‡æœ¬ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºç™½å­—ç¬¦
        2. æ–‡æœ¬é•¿åº¦è¿‡çŸ­ï¼ˆ< 3ä¸ªå­—ç¬¦ï¼‰ä¸”æ— å®é™…æ„ä¹‰
        3. ä»…åŒ…å«æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šå­—ç¬¦
        """
        if not element_text:
            return True
        
        text_stripped = element_text.strip()
        
        # å®Œå…¨ç©ºç™½
        if not text_stripped:
            return True
        
        # ä»…åŒ…å«ç©ºç™½å­—ç¬¦ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰
        if not _RE_CHARS_ONLY.search(text_stripped):
            return True
        
        # è¿‡çŸ­ä¸”æ— å®é™…æ„ä¹‰çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯OCRå™ªå£°ï¼‰
        if len(text_stripped) < 3:
            # å¦‚æœæ˜¯å•å­—ç¬¦ä¸”ä¸æ˜¯å¸¸è§æœ‰æ„ä¹‰å­—ç¬¦ï¼Œè§†ä¸ºå™ªå£°
            if len(text_stripped) == 1 and text_stripped not in ['ã€‚', 'ï¼Œ', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', '.', ',', '!', '?']:
                return True
        
        # ä»…åŒ…å«æ•°å­—å’Œæ ‡ç‚¹ï¼ˆå¯èƒ½æ˜¯æ¡å½¢ç ã€ç¼–å·ç­‰ï¼‰
        if len(text_stripped) < 10:
            if re.match(r'^[\d\s\-_./\\]+$', text_stripped):
                return True
        
        return False
    
    def _is_copyright_page(self, element_text: str, page_number: int, total_pages: int = 0, coordinates: Dict[str, Any] = None) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯ç‰ˆæƒå£°æ˜é¡µ
        
        ç‰ˆæƒå£°æ˜ç‰¹å¾ï¼š
        1. åŒ…å«ç‰ˆæƒå…³é”®è¯
        2. é€šå¸¸åœ¨æ–‡æ¡£æœ«å°¾ï¼ˆå10%çš„é¡µé¢ï¼‰- âš ï¸ ä¼˜å…ˆä½¿ç”¨é¡µç å’Œåæ ‡åˆ¤æ–­
        3. å¯èƒ½åŒ…å«ç‰ˆæƒç¬¦å·ã€å¹´ä»½ã€å‡ºç‰ˆç¤¾ä¿¡æ¯
        4. ä½ç½®é€šå¸¸åœ¨é¡µé¢åº•éƒ¨åŒºåŸŸ
        
        åˆ¤æ–­ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ï¼‰ï¼š
        1. ä¼˜å…ˆä½¿ç”¨é¡µç ä½ç½®åˆ¤æ–­ï¼ˆåœ¨æ–‡æ¡£æœ«å°¾ï¼‰
        2. ç»“åˆåæ ‡ä½ç½®åˆ¤æ–­ï¼ˆåœ¨é¡µé¢åº•éƒ¨ï¼‰
        3. å¦‚æœä½ç½®ä¿¡æ¯ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆå…³é”®è¯ï¼‰
        """
        if not element_text or len(element_text.strip()) < 20:
            return False
        
        text_lower = element_text.lower()
        text_stripped = element_text.strip()
        
        # âš ï¸ ç­–ç•¥1ï¼šä¼˜å…ˆä½¿ç”¨é¡µç ä½ç½®åˆ¤æ–­
        # ç‰ˆæƒå£°æ˜é€šå¸¸åœ¨æ–‡æ¡£æœ«å°¾ï¼ˆå10%çš„é¡µé¢ï¼‰
        is_in_last_pages = False
        if total_pages > 0:
            page_ratio = page_number / total_pages if total_pages > 0 else 0
            # å¦‚æœé¡µç åœ¨æ–‡æ¡£çš„å10%ï¼Œå¾ˆå¯èƒ½æ˜¯ç‰ˆæƒé¡µ
            is_in_last_pages = page_ratio > 0.90
        
        # âš ï¸ ç­–ç•¥2ï¼šç»“åˆåæ ‡ä½ç½®åˆ¤æ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # ç‰ˆæƒå£°æ˜é€šå¸¸åœ¨é¡µé¢åº•éƒ¨åŒºåŸŸ
        # æ³¨æ„ï¼šcopyright_page çš„åˆ¤æ–­ä¸»è¦ä¾èµ–é¡µç ï¼Œåæ ‡ä½œä¸ºè¾…åŠ©
        
        # ç‰ˆæƒç›¸å…³å…³é”®è¯
        copyright_keywords = [
            r'ç‰ˆæƒ', r'copyright', r'Â©', r'\(c\)', r'all rights reserved',
            r'ç‰ˆæƒæ‰€æœ‰', r'ç‰ˆæƒæ‰€æœ‰\s*Â©', r'published by', r'å‡ºç‰ˆç¤¾',
            r'isbn', r'issn', r'å°åˆ·', r'å‡ºç‰ˆ', r'å°æ¬¡',
            r'æœªç»è®¸å¯', r'ç¦æ­¢', r'ä¸å¾—', r'æ³•å¾‹ä¿æŠ¤'
        ]
        
        has_copyright_keyword = any(re.search(keyword, text_lower, re.IGNORECASE) for keyword in copyright_keywords)
        
        # å¦‚æœä½ç½®ç¬¦åˆï¼ˆåœ¨æ–‡æ¡£æœ«å°¾ï¼‰ä¸”åŒ…å«ç‰ˆæƒå…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯ç‰ˆæƒå£°æ˜
        if is_in_last_pages and has_copyright_keyword:
            if len(text_stripped) < 500:  # ç‰ˆæƒé¡µé€šå¸¸å†…å®¹ç®€æ´
                return True
        
        # å¦‚æœåŒ…å«ç‰ˆæƒå…³é”®è¯ä¸”æ–‡æœ¬è¾ƒçŸ­ï¼ˆç‰ˆæƒé¡µé€šå¸¸å†…å®¹ç®€æ´ï¼‰
        # å³ä½¿ä¸åœ¨æ–‡æ¡£æœ«å°¾ï¼Œä¹Ÿå¯èƒ½æ˜¯ç‰ˆæƒå£°æ˜ç‰‡æ®µ
        if has_copyright_keyword and len(text_stripped) < 500:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰ˆæƒå¹´ä»½æ¨¡å¼ï¼ˆå¦‚ Â© 2024, Copyright 2024ï¼‰
            copyright_year_pattern = r'(?:copyright|Â©|\(c\))\s*(?:Â©|\()?\s*\d{4}'
            if re.search(copyright_year_pattern, text_lower):
                return True
            # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆ<100å­—ç¬¦ï¼‰ï¼Œå³ä½¿æ²¡æœ‰å¹´ä»½ä¹Ÿå¯èƒ½æ˜¯ç‰ˆæƒç‰‡æ®µ
            if len(text_stripped) < 100:
                return True
        
        return False
    
    def _is_valid_content(self, element_text: str) -> bool:
        """
        é€šç”¨å†…å®¹ä¿æŠ¤å‡½æ•°ï¼šåˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹
        
        è¿™ä¸ªå‡½æ•°ç”¨äºåœ¨æ‰€æœ‰é™å™ªè¿‡æ»¤å™¨ä¹‹å‰ï¼Œè¯†åˆ«å¹¶ä¿æŠ¤æ­£å¸¸çš„å†…å®¹ï¼Œ
        é˜²æ­¢è¯¯åˆ¤ã€‚é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„æ–‡æ¡£ã€‚
        
        ä¿æŠ¤çš„å†…å®¹ç±»å‹ï¼š
        1. åŒ…å«ç« èŠ‚ç¼–å·çš„æ ‡é¢˜ï¼ˆå•æ•°å­—ã€å¤šçº§ã€ä¸­æ–‡ã€ç½—é©¬æ•°å­—ç­‰ï¼‰
        2. æŠ€æœ¯å†…å®¹ï¼ˆå‡½æ•°è°ƒç”¨ã€ç«¯å£å·ã€URLã€JSONã€ä»£ç ã€é…ç½®ç­‰ï¼‰
        3. ç»“æ„åŒ–å†…å®¹ï¼ˆåˆ—è¡¨é¡¹ã€è¡¨æ ¼å†…å®¹ã€å…¬å¼ã€å‘½ä»¤ç­‰ï¼‰
        4. è¾ƒé•¿çš„æœ‰æ„ä¹‰æ–‡æœ¬æ®µè½ï¼ˆä¸­æ–‡>=5å­—ç¬¦ï¼Œè‹±æ–‡>=10å­—ç¬¦ï¼‰
        
        âš ï¸ æ³¨æ„ï¼šå³ä½¿æ»¡è¶³ä»¥ä¸Šæ¡ä»¶ï¼Œå¦‚æœå†…å®¹æ˜¯å…¸å‹çš„é¡µçœ‰é¡µè„šã€æ°´å°ã€ç‰ˆæƒå£°æ˜ç­‰ï¼Œ
        ä¹Ÿä¸åº”è¯¥è¢«ä¿æŠ¤ï¼ˆåº”è¯¥åœ¨é™å™ªè¿‡æ»¤é˜¶æ®µè¢«ç§»é™¤ï¼‰ã€‚
        
        å‚æ•°:
            element_text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            
        è¿”å›:
            True å¦‚æœæ˜¯æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹ï¼ˆåº”è¯¥è¢«ä¿æŠ¤ï¼Œä¸åº”è¢«è¿‡æ»¤ï¼‰
        """
        if not element_text:
            return False
        
        text_stripped = element_text.strip()
        if not text_stripped:
            return False
        
        # âš ï¸ é‡è¦ï¼šå…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å…¸å‹çš„é¡µçœ‰é¡µè„šã€æ°´å°ã€ç‰ˆæƒå£°æ˜ç­‰å†…å®¹
        # å³ä½¿è¿™äº›å†…å®¹æ»¡è¶³å…¶ä»–"æœ‰æ•ˆå†…å®¹"æ¡ä»¶ï¼ˆå¦‚é•¿åº¦ã€ä¸­æ–‡å­—ç¬¦ç­‰ï¼‰ï¼Œä¹Ÿä¸åº”è¯¥è¢«ä¿æŠ¤
        header_footer_watermark_patterns = [
            # ä¿å¯†å£°æ˜ã€å†…éƒ¨èµ„æ–™
            r'å†…éƒ¨èµ„æ–™', r'è¯·å‹¿å¤–ä¼ ', r'ç¦æ­¢å¤–ä¼ ', r'ä¿å¯†', r'æœºå¯†',
            r'å†…éƒ¨', r'å¤–éƒ¨', r'ä»…ä¾›.*ä½¿ç”¨', r'ä¸å¾—.*ä¼ æ’­',
            # ç‰ˆæƒå£°æ˜
            r'ç‰ˆæƒæ‰€æœ‰', r'Copyright', r'Â©', r'Â®',
            r'ä¿ç•™.*æƒåˆ©', r'All rights reserved',
            # æ°´å°å…³é”®è¯
            r'æ°´å°', r'Watermark', r'æ ·æœ¬', r'Sample', r'è‰æ¡ˆ', r'Draft',
            # å…¬å¸åç§°å•ç‹¬å‡ºç°ï¼ˆé€šå¸¸æ˜¯é¡µçœ‰é¡µè„šï¼‰
            r'^[\w\sï¼ˆï¼‰()]{5,50}æœ‰é™å…¬å¸$',  # çº¯å…¬å¸åç§°
            r'^[\w\sï¼ˆï¼‰()]{5,50}è‚¡ä»½æœ‰é™å…¬å¸$',
            r'^[\w\sï¼ˆï¼‰()]{5,50}é›†å›¢$',
            # å…¸å‹çš„é¡µçœ‰é¡µè„šçŸ­è¯­ç»„åˆ
            r'å†…éƒ¨èµ„æ–™.*è¯·å‹¿å¤–ä¼ ', r'è¯·å‹¿å¤–ä¼ .*å†…éƒ¨èµ„æ–™',
            r'ä¿å¯†.*ä¸å¾—.*ä¼ æ’­', r'ä¸å¾—.*ä¼ æ’­.*ä¿å¯†',
        ]
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…é¡µçœ‰é¡µè„š/æ°´å°/ç‰ˆæƒå£°æ˜æ¨¡å¼
        if any(re.search(pattern, text_stripped, re.IGNORECASE) for pattern in header_footer_watermark_patterns):
            # å¦‚æœæ˜¯å…¸å‹çš„é¡µçœ‰é¡µè„šå†…å®¹ï¼Œå³ä½¿æ»¡è¶³å…¶ä»–æ¡ä»¶ä¹Ÿä¸ä¿æŠ¤
            return False
        
        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«ç« èŠ‚ç¼–å·ï¼ˆå„ç§æ ¼å¼ï¼‰- ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼æå‡æ€§èƒ½
        if (_RE_SECTION_SINGLE.search(text_stripped) or
            _RE_SECTION_MULTI.search(text_stripped) or
            _RE_SECTION_CHINESE.search(text_stripped) or
            _RE_SECTION_ROMAN.search(text_stripped) or
            _RE_SECTION_CHAPTER.search(text_stripped) or
            _RE_SECTION_PART.search(text_stripped) or
            _RE_APPENDIX_EN.search(text_stripped) or
            _RE_APPENDIX_CN.search(text_stripped)):
            return True
        
        # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€æœ¯å†…å®¹ï¼ˆå„ç§æ ¼å¼ï¼‰
        technical_patterns = [
            # å‡½æ•°è°ƒç”¨å’Œæ–¹æ³•è°ƒç”¨
            r'[a-zA-Z0-9_\-\.]+\s*\(',  # function(), method(), obj.method()
            r'[a-zA-Z0-9_\-]+\s*=\s*[a-zA-Z0-9_\-]+\s*\(',  # var = func()
            
            # ç«¯å£å·å’Œç½‘ç»œåœ°å€
            r':\s*\d{1,5}(?:/\w+)?',  # :27017, :8080/path
            r'[a-zA-Z0-9\-\.]+:\d+',  # host:port
            
            # URLå’ŒåŸŸå
            r'https?://[\w\.\-]+',  # http://, https://
            r'[\w\.\-]+\.[\w\.\-]+\.\w+',  # domain.com, sub.domain.org
            r'ftp://[\w\.\-]+',  # ftp://
            
            # JSONå¯¹è±¡å’Œæ•°ç»„
            r'\{[\s\S]*\}',  # { ... }
            r'\[[\s\S]*\]',  # [ ... ]
            
            # ä»£ç å—ç‰¹å¾
            r'```[\s\S]*```',  # ```code```
            r'<code>[\s\S]*</code>',  # <code>...</code>
            r'#\s*\w+',  # # comment (shell/python)
            r'//\s*\w+',  # // comment
            r'/\*[\s\S]*\*/',  # /* comment */
            
            # å‘½ä»¤è¡Œå’Œè„šæœ¬
            r'^\$\s+',  # $ command
            r'^>\s+',  # > prompt
            r'^C:\\',  # C:\path (Windows)
            r'^/\w+',  # /path (Unix)
            r'^\w+://',  # protocol://
            
            # é…ç½®æ–‡ä»¶æ ¼å¼
            r'^\s*\w+\s*[:=]\s*\w+',  # key: value, key=value
            r'<[\w/]+>',  # XML tags
            r'&[\w]+;',  # XML entities
        ]
        if any(re.search(pattern, text_stripped, re.IGNORECASE | re.MULTILINE) for pattern in technical_patterns):
            return True
        
        # 3. æ£€æŸ¥ç»“æ„åŒ–å†…å®¹
        structured_patterns = [
            # åˆ—è¡¨é¡¹ï¼ˆå„ç§æ ‡è®°ï¼‰
            r'^[\-â€¢Â·â–ªâ–«â—¦â€£âƒ]\s+',  # - item, â€¢ item
            r'^\d+[\.\)]\s+',  # 1. item, 1) item
            r'^[a-zA-Z][\.\)]\s+',  # a. item, a) item
            r'^[IVX]+[\.\)]\s+',  # I. item, I) item
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]\s+',  # ä¸€ã€item
            
            # è¡¨æ ¼å†…å®¹ç‰¹å¾
            r'\|\s*.+\s*\|',  # | cell | cell |
            r'\s+\|\s+',  # spaces | spaces
            
            # å…¬å¼å’Œè¡¨è¾¾å¼
            r'[a-zA-Z]\s*[+\-*/=]\s*[a-zA-Z0-9]',  # x = y + z
            r'[a-zA-Z0-9]+\s*[<>â‰¤â‰¥=]\s*[a-zA-Z0-9]+',  # x >= y
            r'\$\$?[\s\S]*\$\$?',  # $formula$ (LaTeX)
            
            # å¼•ç”¨æ ¼å¼
            r'^>\s+',  # > quote
            r'^"[\s\S]*"$',  # "quoted text"
            r'^ã€Œ[\s\S]*ã€$',  # ã€Œquoted textã€
        ]
        if any(re.search(pattern, text_stripped, re.MULTILINE) for pattern in structured_patterns):
            return True
        
        # 4. æ£€æŸ¥è¾ƒé•¿çš„æœ‰æ„ä¹‰æ–‡æœ¬æ®µè½
        # ä¸­æ–‡ï¼š>=5ä¸ªè¿ç»­æ±‰å­—ï¼ˆæœ‰æ„ä¹‰çš„ä¸­æ–‡æ®µè½ï¼‰- ä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™
        if _RE_CHINESE_5_CHARS.search(text_stripped):
            return True
        
        # è‹±æ–‡ï¼š>=10ä¸ªå­—æ¯ï¼ˆæœ‰æ„ä¹‰çš„è‹±æ–‡æ®µè½ï¼‰
        # æ’é™¤çº¯æ•°å­—ã€çº¯ç¬¦å·ç­‰ - ä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™
        english_words = _RE_ENGLISH_WORD.findall(text_stripped)
        if len(english_words) >= 3:  # è‡³å°‘3ä¸ªå•è¯
            return True
        
        # ä¸­è‹±æ–‡æ··åˆï¼šåŒ…å«ä¸­æ–‡å­—ç¬¦ä¸”æ€»é•¿åº¦>=10
        if _RE_CHINESE.search(text_stripped) and len(text_stripped) >= 10:
            return True
        
        # 5. æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„æ ‡ç‚¹å’Œæ ¼å¼
        # å¦‚æœåŒ…å«å¤šç§æ ‡ç‚¹ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰ï¼‰ï¼Œå¯èƒ½æ˜¯å®Œæ•´å¥å­
        sentence_endings = ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ';', 'ï¼›']
        if sum(1 for char in text_stripped if char in sentence_endings) >= 1:
            # ä¸”æ–‡æœ¬é•¿åº¦>=8ï¼Œå¯èƒ½æ˜¯å®Œæ•´å¥å­
            if len(text_stripped) >= 8:
                return True
        
        return False
    
    def _is_noise_text(self, element_text: str, is_pdf: bool = False) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯å™ªå£°ï¼ˆOCRé”™è¯¯ã€ç¢ç‰‡æ–‡æœ¬ç­‰ï¼‰
        
        å™ªå£°ç‰¹å¾ï¼š
        1. è¿‡çŸ­çš„æ–‡æœ¬ç‰‡æ®µï¼ˆ< 5ä¸ªå­—ç¬¦ï¼Œä¸”æ— æ„ä¹‰ï¼‰
        2. ä»…åŒ…å«ç‰¹æ®Šå­—ç¬¦
        3. é‡å¤å­—ç¬¦è¿‡å¤šï¼ˆå¦‚ "aaabbbccc"ï¼‰
        4. å­—ç¬¦æ¯”ä¾‹å¼‚å¸¸ï¼ˆå¦‚å…¨æ˜¯æ ‡ç‚¹ï¼‰
        5. PDF OCRå¸¸è§é”™è¯¯æ¨¡å¼ï¼ˆå¦‚æœis_pdf=Trueï¼‰
        
        æ³¨æ„ï¼šåŒ…å«ç« èŠ‚ç¼–å·çš„æ–‡æœ¬ï¼ˆå¦‚"1.1.æ ‡é¢˜"ï¼‰ä¸åº”è¢«è¯†åˆ«ä¸ºå™ªå£°
        
        å‚æ•°:
            element_text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            is_pdf: æ˜¯å¦ä¸ºPDFæ–‡ä»¶ï¼ˆPDFçš„OCRé”™è¯¯æ›´å¤šï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼‰
        """
        if not element_text:
            return True
        
        text_stripped = element_text.strip()
        
        # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ç« èŠ‚ç¼–å·æ¨¡å¼ï¼ˆå¦‚ 1.1, 1.2.1, ç¬¬ä¸€ç«  ç­‰ï¼‰
        # åŒ…å«ç« èŠ‚ç¼–å·çš„æ–‡æœ¬é€šå¸¸æ˜¯æ ‡é¢˜æˆ–ç›®å½•é¡¹ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºå™ªå£°
        section_patterns = [
            r'^\d+\.',  # å•æ•°å­—åŠ ç‚¹å¼€å¤´ï¼Œå¦‚ 1.ã€4. ç­‰
            r'\d+\.\d+(?:\.\d+)*',  # 1.1, 1.2.3, 1.2.3.4
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]',  # ä¸€ã€äºŒã€ä¸‰ã€
            r'[IVX]+\.\d+',  # I.1, II.2.3
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
        ]
        has_section_number = any(re.search(pattern, text_stripped) for pattern in section_patterns)
        
        # å¦‚æœåŒ…å«ç« èŠ‚ç¼–å·ï¼Œå³ä½¿æ˜¯çŸ­æ–‡æœ¬ä¹Ÿè®¤ä¸ºæ˜¯æœ‰æ•ˆå†…å®¹ï¼ˆæ ‡é¢˜ï¼‰
        if has_section_number:
            return False
        
        # PDF OCRé”™è¯¯ï¼šæ£€æŸ¥å¸¸è§OCRè¯¯è¯†åˆ«æ¨¡å¼
        if is_pdf:
            # 1. æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡ç›¸ä¼¼å­—ç¬¦æ··åˆï¼ˆOCRå¸¸è§é”™è¯¯ï¼‰
            # ä¾‹å¦‚ï¼šl1|Iã€O0ã€rn mç­‰æ˜“æ··æ·†å­—ç¬¦
            confusing_chars = ['l', '1', '|', 'I', 'O', '0']
            confusing_count = sum(text_stripped.count(char) for char in confusing_chars)
            if len(text_stripped) > 0 and confusing_count / len(text_stripped) > 0.5:
                # å¦‚æœæ··æ·†å­—ç¬¦å æ¯”è¿‡é«˜ï¼Œå¯èƒ½æ˜¯OCRé”™è¯¯
                return True
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡éšæœºå­—ç¬¦ç»„åˆï¼ˆOCRå™ªå£°ï¼‰
            # è¿ç»­å‡ºç°çš„éå•è¯å­—ç¬¦ç»„åˆï¼ˆä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼‰
            if len(text_stripped) > 10:  # åªå¯¹è¾ƒé•¿æ–‡æœ¬æ£€æŸ¥
                random_matches = len(_RE_RANDOM_CHARS.findall(text_stripped))
                if random_matches > 0 and random_matches / len(text_stripped) > 0.3:
                    return True
        
        # è¿‡çŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯OCRç¢ç‰‡ï¼‰
        # PDF OCRé”™è¯¯äº§ç”Ÿçš„ç¢ç‰‡å¯èƒ½æ›´çŸ­ï¼Œæ‰€ä»¥å¯¹PDFä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼
        min_length = MIN_TEXT_LENGTH_FOR_NOISE_PDF if is_pdf else MIN_TEXT_LENGTH_FOR_NOISE
        if len(text_stripped) < min_length:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„çŸ­æ–‡æœ¬ï¼ˆå¦‚"æ˜¯"ã€"å¦"ã€"OK"ç­‰ï¼‰
            meaningful_short = ['æ˜¯', 'å¦', 'ok', 'no', 'yes', 'true', 'false', 'a', 'i']
            if text_stripped.lower() not in meaningful_short:
                return True
        
        # ä»…åŒ…å«ç‰¹æ®Šå­—ç¬¦
        if not _RE_CHARS_ONLY.search(text_stripped):
            return True
        
        # å­—ç¬¦æ¯”ä¾‹å¼‚å¸¸ï¼šå¦‚æœæ ‡ç‚¹ç¬¦å·å æ¯”è¿‡é«˜ï¼ˆPDFä½¿ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼ï¼‰
        punctuation_threshold = 0.6 if is_pdf else 0.7
        punctuation_count = len(_RE_PUNCTUATION.findall(text_stripped))
        if len(text_stripped) > 0 and punctuation_count / len(text_stripped) > punctuation_threshold:
            return True
        
        # é‡å¤å­—ç¬¦è¿‡å¤šï¼ˆå¦‚ "aaa"ã€"---"ï¼‰
        if len(text_stripped) >= 3:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡80%çš„å­—ç¬¦ç›¸åŒ
            char_counts = {}
            for char in text_stripped:
                char_counts[char] = char_counts.get(char, 0) + 1
            max_char_count = max(char_counts.values()) if char_counts else 0
            if max_char_count / len(text_stripped) > 0.8:
                return True
        
        # PDFç‰¹æ®Šï¼šæ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡å•ä¸ªå­—ç¬¦è¢«ç©ºæ ¼åˆ†éš”ï¼ˆOCRé”™è¯¯æ¨¡å¼ï¼‰
        if is_pdf and len(text_stripped.split()) > 3:
            single_char_words = len([w for w in text_stripped.split() if len(w) == 1])
            total_words = len(text_stripped.split())
            if single_char_words / total_words > 0.5:
                return True
        
        return False
    
    def _is_watermark(self, element_text: str, coordinates: Dict[str, Any], page_number: int, page_height: float = 0, page_width: float = 0) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯æ°´å°æ–‡å­—
        
        æ°´å°ç‰¹å¾ï¼š
        1. æ–‡æœ¬é€šå¸¸è¾ƒçŸ­ä¸”é‡å¤ï¼ˆå¦‚"ä¿å¯†"ã€"CONFIDENTIAL"ã€"DRAFT"ç­‰ï¼‰
        2. ä½ç½®å¯èƒ½åœ¨é¡µé¢ä¸­å¤®æˆ–è§’è½ï¼ˆâš ï¸ ä¼˜å…ˆä½¿ç”¨åæ ‡åˆ¤æ–­ï¼‰
        3. å¯èƒ½å€¾æ–œã€åŠé€æ˜ï¼ˆOCRè¯†åˆ«å¯èƒ½ä¸å®Œæ•´ï¼‰
        4. å­—ä½“å¤§å°å¯èƒ½å¼‚å¸¸
        
        åˆ¤æ–­ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ï¼‰ï¼š
        1. ä¼˜å…ˆä½¿ç”¨åæ ‡ä½ç½®åˆ¤æ–­ï¼ˆæœ€å‡†ç¡®ï¼‰
        2. å¦‚æœåæ ‡ä¿¡æ¯ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆå…³é”®è¯ã€é‡å¤æ¨¡å¼ç­‰ï¼‰
        
        æ³¨æ„ï¼šæ­£å¸¸æ®µè½ä¸­å¦‚æœåŒ…å«æ°´å°å…³é”®è¯ï¼ˆå¦‚"å†…éƒ¨èµ„æ–™"ï¼‰ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºæ°´å°
        """
        if not element_text:
            return False
        
        import re
        
        text_stripped = element_text.strip()
        text_upper = text_stripped.upper()
        text_original = element_text.strip()  # ä¿ç•™åŸå§‹æ–‡æœ¬ç”¨äºæ£€æŸ¥ç« èŠ‚ç¼–å·
        
        # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ç« èŠ‚ç¼–å·æˆ–æ˜¯æœ‰æ„ä¹‰çš„æ®µè½
        # å¦‚æœæ–‡æœ¬åŒ…å«ç« èŠ‚ç¼–å·ã€æŠ€æœ¯å†…å®¹ï¼ˆå¦‚é…ç½®ã€å‘½ä»¤ç­‰ï¼‰ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºæ°´å°
        section_patterns = [
            r'^\d+\.',  # å•æ•°å­—åŠ ç‚¹å¼€å¤´ï¼Œå¦‚ 1.ã€4. ç­‰
            r'\d+\.\d+(?:\.\d+)*',  # 1.1, 1.2.3, 1.2.3.4
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]',  # ä¸€ã€äºŒã€ä¸‰ã€
            r'[IVX]+\.\d+',  # I.1, II.2.3
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
        ]
        has_section_number = any(re.search(pattern, text_original) for pattern in section_patterns)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€æœ¯å†…å®¹ï¼ˆå¦‚é…ç½®ã€å‘½ä»¤ã€URLç­‰ï¼‰
        has_technical_content = bool(
            re.search(r'[a-zA-Z0-9_\-\.]+\(', text_original) or  # å‡½æ•°è°ƒç”¨å¦‚ rs.conf()
            re.search(r':\s*\d+', text_original) or  # ç«¯å£å·å¦‚ :27017
            re.search(r'[\w\.]+\.[\w\.]+', text_original) or  # åŸŸåæˆ–åŒ…å
            re.search(r'\{.*\}', text_original) or  # JSONå¯¹è±¡
            re.search(r'[\u4e00-\u9fa5]{5,}', text_original)  # åŒ…å«è¾ƒé•¿çš„ä¸­æ–‡æ®µè½ï¼ˆ>=5ä¸ªæ±‰å­—ï¼‰
        )
        
        # å¦‚æœåŒ…å«ç« èŠ‚ç¼–å·æˆ–æŠ€æœ¯å†…å®¹ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºæ°´å°
        if has_section_number or has_technical_content:
            return False
        
        # âš ï¸ ç­–ç•¥1ï¼šä¼˜å…ˆä½¿ç”¨åæ ‡ä½ç½®åˆ¤æ–­ï¼ˆæœ€å‡†ç¡®å¯é ï¼‰
        # æ°´å°é€šå¸¸å‡ºç°åœ¨é¡µé¢ä¸­å¤®åŒºåŸŸæˆ–å››ä¸ªè§’è½ï¼Œä¸”æ–‡æœ¬è¾ƒçŸ­
        if coordinates and page_height > 0 and page_width > 0:
            y_pos = coordinates.get('y', 0)
            x_pos = coordinates.get('x', 0)
            element_height = coordinates.get('height', 0)
            element_width = coordinates.get('width', 0)
            element_center_y = y_pos + element_height / 2
            element_center_x = x_pos + element_width / 2
            
            # æ°´å°ä½ç½®ç‰¹å¾ï¼š
            # 1. é¡µé¢ä¸­å¤®åŒºåŸŸï¼ˆ30%-70%çš„å‚ç›´ä½ç½®ï¼Œä¸”æ°´å¹³ä½ç½®ä¹Ÿåœ¨30%-70%ï¼‰
            is_in_center = (
                page_height * 0.30 < element_center_y < page_height * 0.70 and
                page_width * 0.30 < element_center_x < page_width * 0.70
            )
            
            # 2. é¡µé¢å››ä¸ªè§’è½ï¼ˆé¡¶éƒ¨æˆ–åº•éƒ¨20%ï¼Œå·¦ä¾§æˆ–å³ä¾§20%ï¼‰
            is_in_corner = (
                (y_pos < page_height * 0.20 or element_center_y > page_height * 0.80) and
                (x_pos < page_width * 0.20 or element_center_x > page_width * 0.80)
            )
            
            # å¦‚æœä½ç½®ç¬¦åˆæ°´å°ç‰¹å¾ï¼Œä¸”æ–‡æœ¬è¾ƒçŸ­ï¼Œè¿›ä¸€æ­¥éªŒè¯å†…å®¹
            if (is_in_center or is_in_corner) and len(text_stripped) < 60:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ°´å°å…³é”®è¯
                watermark_keywords = [
                    'ä¿å¯†', 'CONFIDENTIAL', 'DRAFT', 'è‰ç¨¿', 'å†…éƒ¨èµ„æ–™', 'INTERNAL',
                    'æœºå¯†', 'SECRET', 'TOP SECRET', 'ç»å¯†', 'RESTRICTED', 'é™åˆ¶',
                    'ç¦æ­¢å¤åˆ¶', 'DO NOT COPY', 'æ ·æœ¬', 'SAMPLE', 'å‰¯æœ¬', 'COPY',
                ]
                for keyword in watermark_keywords:
                    if keyword.upper() in text_upper or keyword in text_original:
                        return True
                
                # å¦‚æœæ˜¯å¾ˆçŸ­æ–‡æœ¬ï¼ˆ<20å­—ç¬¦ï¼‰ï¼Œä½ç½®åˆç¬¦åˆï¼Œå¾ˆå¯èƒ½æ˜¯æ°´å°
                if len(text_stripped) < 20:
                    return True
        
        # âš ï¸ ç­–ç•¥2ï¼šå¦‚æœæ²¡æœ‰åæ ‡ä¿¡æ¯ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
        # å¸¸è§æ°´å°å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
        watermark_keywords = [
            'ä¿å¯†', 'CONFIDENTIAL', 'DRAFT', 'è‰ç¨¿', 'å†…éƒ¨èµ„æ–™', 'INTERNAL',
            'æœºå¯†', 'SECRET', 'TOP SECRET', 'ç»å¯†', 'RESTRICTED', 'é™åˆ¶',
            'ç¦æ­¢å¤åˆ¶', 'DO NOT COPY', 'æ ·æœ¬', 'SAMPLE', 'å‰¯æœ¬', 'COPY',
            'NOT FOR DISTRIBUTION', 'ä»…ä¾›å†…éƒ¨ä½¿ç”¨'
        ]
        
        # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…æ°´å°å…³é”®è¯ï¼ˆåªåŒ…å«å…³é”®è¯æœ¬èº«ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼‰
        if text_upper in [kw.upper() for kw in watermark_keywords]:
            return True
        
        # âš ï¸ ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯æ°´å°å…³é”®è¯ï¼ˆæ–‡æœ¬å†…å®¹ä¸»è¦å°±æ˜¯æ°´å°å…³é”®è¯ï¼Œè€ŒéåŒ…å«åœ¨æ­£å¸¸æ®µè½ä¸­ï¼‰
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆ<30å­—ç¬¦ï¼‰ä¸”ä¸»è¦æ˜¯æ°´å°å…³é”®è¯ï¼Œæ‰è®¤ä¸ºæ˜¯æ°´å°
        for keyword in watermark_keywords:
            keyword_upper = keyword.upper()
            if keyword_upper in text_upper or keyword in text_original:
                # å¦‚æœæ–‡æœ¬é•¿åº¦å¾ˆçŸ­ï¼ˆ<30å­—ç¬¦ï¼‰ï¼Œä¸”æ°´å°å…³é”®è¯å æ¯”å¾ˆé«˜ï¼ˆ>50%ï¼‰ï¼Œæ‰è®¤ä¸ºæ˜¯æ°´å°
                if len(text_stripped) < 30:
                    # è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å æ¯”
                    keyword_count = text_upper.count(keyword_upper) * len(keyword)
                    if keyword_count / len(text_stripped) > 0.5:
                        return True
                # å¦‚æœæ–‡æœ¬é•¿åº¦åœ¨30-50å­—ç¬¦ä¹‹é—´ï¼Œè¦æ±‚å…³é”®è¯å æ¯”æ›´é«˜ï¼ˆ>70%ï¼‰ä¸”æ–‡æœ¬ä¸»è¦æ˜¯æ°´å°çŸ­è¯­
                elif len(text_stripped) < 50:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ°´å°çŸ­è¯­ï¼ˆå¦‚"å†…éƒ¨èµ„æ–™,è¯·å‹¿å¤–ä¼ !"ï¼‰
                    watermark_phrases = [
                        r'å†…éƒ¨èµ„æ–™[ï¼Œ,ã€‚.!ï¼\s]*è¯·å‹¿å¤–ä¼ ',
                        r'CONFIDENTIAL',
                        r'DRAFT',
                        r'ç¦æ­¢å¤åˆ¶',
                        r'DO NOT COPY',
                    ]
                    if any(re.search(phrase, text_original, re.IGNORECASE) for phrase in watermark_phrases):
                        return True
        
        # æ£€æŸ¥é‡å¤çš„æ°´å°æ–‡æœ¬ï¼ˆå¦‚ "DRAFT DRAFT DRAFT"ï¼‰
        words = text_upper.split()
        if len(words) >= 2:
            # å¦‚æœæ‰€æœ‰è¯éƒ½ç›¸åŒä¸”æ˜¯æ°´å°å…³é”®è¯
            if len(set(words)) == 1:
                word = words[0]
                # æ£€æŸ¥è¿™ä¸ªè¯æ˜¯å¦æ˜¯æ°´å°å…³é”®è¯
                if any(word == kw.upper() or word in kw.upper() for kw in watermark_keywords):
                    if len(word) > 2:
                        return True
        
        return False
    
    def _is_cover_or_back_page(self, element_text: str, page_number: int, total_pages: int = 0, coordinates: Dict[str, Any] = None, page_height: float = 0, page_width: float = 0) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦å±äºå°é¢æˆ–å°åº•é¡µ
        
        å°é¢/å°åº•ç‰¹å¾ï¼š
        1. é€šå¸¸åœ¨æ–‡æ¡£çš„ç¬¬ä¸€é¡µæˆ–æœ€åä¸€é¡µï¼ˆâš ï¸ ä¼˜å…ˆä½¿ç”¨é¡µç åˆ¤æ–­ï¼‰
        2. åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€å‡ºç‰ˆç¤¾ç­‰å…ƒä¿¡æ¯
        3. æ ¼å¼é€šå¸¸ç‰¹æ®Šï¼ˆå±…ä¸­ã€å¤§å­—ä½“ç­‰ï¼‰
        4. å†…å®¹é€šå¸¸è¾ƒçŸ­
        5. ä½ç½®å¯èƒ½åœ¨é¡µé¢ä¸­å¤®åŒºåŸŸï¼ˆå°é¢ï¼‰æˆ–åº•éƒ¨åŒºåŸŸï¼ˆå°åº•ï¼‰
        
        åˆ¤æ–­ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ï¼‰ï¼š
        1. ä¼˜å…ˆä½¿ç”¨é¡µç ä½ç½®åˆ¤æ–­ï¼ˆç¬¬ä¸€é¡µ/æœ€åä¸€é¡µï¼‰
        2. ç»“åˆåæ ‡ä½ç½®åˆ¤æ–­ï¼ˆé¡µé¢ä¸­å¤®æˆ–åº•éƒ¨ï¼‰
        3. å¦‚æœä½ç½®ä¿¡æ¯ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆå…³é”®è¯ï¼‰
        """
        if not element_text:
            return False
        
        import re
        
        text_stripped = element_text.strip()
        text_lower = text_stripped.lower()
        
        # âš ï¸ ç­–ç•¥1ï¼šä¼˜å…ˆä½¿ç”¨é¡µç ä½ç½®åˆ¤æ–­ï¼ˆæœ€å‡†ç¡®ï¼‰
        is_cover_page = page_number <= 2  # å°é¢é€šå¸¸åœ¨ç¬¬ä¸€é¡µæˆ–ç¬¬äºŒé¡µ
        is_back_page = False
        if total_pages > 0:
            is_back_page = page_number >= total_pages - 1  # å°åº•é€šå¸¸åœ¨æœ€åä¸€é¡µæˆ–å€’æ•°ç¬¬äºŒé¡µ
        
        # âš ï¸ ç­–ç•¥2ï¼šç»“åˆåæ ‡ä½ç½®åˆ¤æ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # å°é¢ï¼šé€šå¸¸åœ¨é¡µé¢ä¸­å¤®åŒºåŸŸï¼ˆå‚ç›´30%-70%ï¼Œæ°´å¹³30%-70%ï¼‰
        # å°åº•ï¼šå¯èƒ½åœ¨é¡µé¢åº•éƒ¨åŒºåŸŸ
        is_in_center_area = False
        is_in_bottom_area = False
        
        if coordinates and page_height > 0 and page_width > 0:
            y_pos = coordinates.get('y', 0)
            x_pos = coordinates.get('x', 0)
            element_height = coordinates.get('height', 0)
            element_width = coordinates.get('width', 0)
            element_center_y = y_pos + element_height / 2
            element_center_x = x_pos + element_width / 2
            
            # å°é¢é€šå¸¸å±…ä¸­æ˜¾ç¤º
            is_in_center_area = (
                page_height * 0.30 < element_center_y < page_height * 0.70 and
                page_width * 0.30 < element_center_x < page_width * 0.70
            )
            
            # å°åº•é€šå¸¸åœ¨åº•éƒ¨
            is_in_bottom_area = element_center_y > page_height * 0.70
        
        # å°é¢å…³é”®è¯ï¼ˆé€šå¸¸åœ¨é¦–é¡µï¼‰
        cover_keywords = [
            r'å°é¢', r'cover', r'ä¹¦å', r'æ ‡é¢˜', r'title',
            r'ä½œè€…', r'author', r'è‘—', r'ç¼–', r'ä¸»ç¼–',
            r'å‡ºç‰ˆç¤¾', r'publisher', r'press', r'å‡ºç‰ˆæ—¥æœŸ',
            r'å‡ºç‰ˆ', r'published', r'isbn', r'issn'
        ]
        
        # å°åº•å…³é”®è¯ï¼ˆé€šå¸¸åœ¨æœ«é¡µï¼‰
        back_cover_keywords = [
            r'å°åº•', r'back cover', r'å°ä¸‰', r'å°å››',
            r'å®šä»·', r'price', r'isbn', r'æ¡ç ',
            r'è´£ä»»ç¼–è¾‘', r'è´£ä»»æ ¡å¯¹', r'å°åˆ·'
        ]
        
        # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼ˆå°é¢å°åº•é€šå¸¸å†…å®¹ç®€æ´ï¼‰
        is_short_text = len(text_stripped) < 300
        
        # æ£€æŸ¥å°é¢ç‰¹å¾
        if is_cover_page:
            has_cover_keyword = any(re.search(keyword, text_lower, re.IGNORECASE) for keyword in cover_keywords)
            # å¦‚æœä½ç½®ä¹Ÿç¬¦åˆï¼ˆå±…ä¸­ï¼‰ï¼Œæ›´å¯èƒ½æ˜¯å°é¢
            if has_cover_keyword and is_short_text:
                if is_in_center_area or not coordinates:  # å±…ä¸­æˆ–æ²¡æœ‰åæ ‡ä¿¡æ¯
                    return True
            # å¦‚æœä½ç½®å±…ä¸­ï¼Œå³ä½¿æ²¡æœ‰å…³é”®è¯ï¼Œä¹Ÿå¯èƒ½æ˜¯å°é¢æ ‡é¢˜
            elif is_in_center_area and is_short_text and len(text_stripped) < 100:
                return True
        
        # æ£€æŸ¥å°åº•ç‰¹å¾
        if is_back_page:
            has_back_keyword = any(re.search(keyword, text_lower, re.IGNORECASE) for keyword in back_cover_keywords)
            # å¦‚æœä½ç½®ä¹Ÿç¬¦åˆï¼ˆåº•éƒ¨ï¼‰ï¼Œæ›´å¯èƒ½æ˜¯å°åº•
            if has_back_keyword and is_short_text:
                return True
            # å¦‚æœä½ç½®åœ¨åº•éƒ¨ï¼Œå³ä½¿æ²¡æœ‰å…³é”®è¯ï¼Œä¹Ÿå¯èƒ½æ˜¯å°åº•ä¿¡æ¯
            elif is_in_bottom_area and is_short_text and len(text_stripped) < 100:
                return True
        else:
            # å¦‚æœæ²¡æœ‰æ€»é¡µæ•°ï¼Œä»…é€šè¿‡å†…å®¹åˆ¤æ–­
            has_back_keyword = any(re.search(keyword, text_lower, re.IGNORECASE) for keyword in back_cover_keywords)
            if has_back_keyword and is_short_text and len(text_stripped) < 200:
                return True
        
        return False
    
    def _is_footnote_or_margin_note(self, element_text: str, coordinates: Dict[str, Any], page_height: float = 0) -> bool:
        """
        åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯è„šæ³¨æˆ–é¡µè¾¹æ³¨é‡Š
        
        è„šæ³¨/é¡µè¾¹æ³¨é‡Šç‰¹å¾ï¼š
        1. ä½ç½®åœ¨é¡µé¢åº•éƒ¨æˆ–è¾¹ç¼˜
        2. é€šå¸¸ä»¥æ•°å­—ã€ç¬¦å·å¼€å¤´ï¼ˆå¦‚ [1]ã€â‘ ã€*ï¼‰
        3. å­—ä½“é€šå¸¸è¾ƒå°
        4. å†…å®¹é€šå¸¸è¾ƒçŸ­
        5. å¯èƒ½åŒ…å«å‚è€ƒæ–‡çŒ®ä¿¡æ¯
        """
        if not element_text:
            return False
        
        text_stripped = element_text.strip()
        
        # ä½ç½®åˆ¤æ–­ï¼šå¦‚æœåœ¨é¡µé¢åº•éƒ¨æˆ–è¾¹ç¼˜
        if coordinates and page_height > 0:
            y_pos = coordinates.get('y', 0)
            height = coordinates.get('height', 0)
            # è„šæ³¨é€šå¸¸åœ¨é¡µé¢åº•éƒ¨20%åŒºåŸŸ
            if y_pos + height > page_height * 0.8:
                # è¿›ä¸€æ­¥æ£€æŸ¥å†…å®¹ç‰¹å¾
                # è„šæ³¨é€šå¸¸ä»¥æ•°å­—æˆ–ç¬¦å·å¼€å¤´
                if re.match(r'^[\[\(]*[0-9â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©*â€ â€¡]+[\]\):ï¼š]*', text_stripped):
                    if len(text_stripped) < 200:  # è„šæ³¨é€šå¸¸è¾ƒçŸ­
                        return True
        
        # å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆå³ä½¿æ²¡æœ‰åæ ‡ä¿¡æ¯ï¼‰
        # 1. ä»¥è„šæ³¨æ ‡è®°å¼€å¤´ï¼ˆå¦‚ [1], (1), * , â‘ ç­‰ï¼‰
        footnote_patterns = [
            r'^[\[\(]*[0-9]+[\]\):ï¼š]',  # [1], (1), 1:
            r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # åœ†åœˆæ•°å­—
            r'^[*â€ â€¡Â§]',  # ç¬¦å·æ ‡è®°
            r'^\[æ³¨\d+\]',  # [æ³¨1]
            r'^æ³¨\s*\d+[:ï¼š]',  # æ³¨1:
        ]
        
        if any(re.match(pattern, text_stripped) for pattern in footnote_patterns):
            # å¦‚æœæ˜¯å¾ˆçŸ­çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯é¡µè¾¹æ³¨é‡Šï¼‰
            if len(text_stripped) < 100:
                return True
        
        # 2. åŒ…å«å‚è€ƒæ–‡çŒ®æ ¼å¼ï¼ˆå¦‚ "ä½œè€…. ä¹¦å. å‡ºç‰ˆç¤¾, å¹´ä»½"ï¼‰
        reference_patterns = [
            r'.+\..+\..+\d{4}',  # åŸºæœ¬å‚è€ƒæ–‡çŒ®æ ¼å¼
            r'å‚è§.*é¡µ',  # "å‚è§ç¬¬Xé¡µ"
            r'è§.*ç¬¬.*é¡µ',  # "è§ç¬¬Xé¡µ"
        ]
        
        # å¦‚æœåŒ¹é…å‚è€ƒæ–‡çŒ®æ ¼å¼ä¸”æ–‡æœ¬è¾ƒçŸ­
        if len(text_stripped) < 150:
            if any(re.search(pattern, text_stripped) for pattern in reference_patterns):
                return True
        
        return False
    
    def _detect_duplicate_content(self, current_text: str, previous_texts: List[str], similarity_threshold: float = 0.85) -> bool:
        """
        æ£€æµ‹é‡å¤å†…å®¹
        
        æ³¨æ„ï¼šæ ‡é¢˜æˆ–ç›®å½•é¡¹åœ¨ç›®å½•å’Œæ­£æ–‡ä¸­é‡å¤å‡ºç°æ˜¯æ­£å¸¸çš„ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºé‡å¤å†…å®¹
        
        å‚æ•°:
            current_text: å½“å‰æ–‡æœ¬
            previous_texts: ä¹‹å‰å¤„ç†è¿‡çš„æ–‡æœ¬åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        
        è¿”å›:
            True å¦‚æœæ£€æµ‹åˆ°é‡å¤å†…å®¹
        """
        if not current_text or not previous_texts:
            return False
        
        current_clean = _RE_WHITESPACE.sub(' ', current_text.strip())
        
        # å¤ªçŸ­çš„æ–‡æœ¬ä¸è¿›è¡Œé‡å¤æ£€æµ‹
        if len(current_clean) < MIN_TEXT_LENGTH_FOR_DUPLICATE:
            return False
        
        # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜æˆ–ç›®å½•é¡¹ï¼ˆåŒ…å«ç« èŠ‚ç¼–å·ï¼‰
        # æ ‡é¢˜åœ¨ç›®å½•å’Œæ­£æ–‡ä¸­é‡å¤å‡ºç°æ˜¯æ­£å¸¸çš„ï¼Œä¸åº”è¢«è¯†åˆ«ä¸ºé‡å¤å†…å®¹
        section_patterns = [
            r'\d+\.\d+(?:\.\d+)*',  # 1.1, 1.2.3, 1.2.3.4
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]',  # ä¸€ã€äºŒã€ä¸‰ã€
            r'[IVX]+\.\d+',  # I.1, II.2.3
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
        ]
        # âœ… ä¿®å¤ä½œç”¨åŸŸé—®é¢˜ï¼šåœ¨åˆ—è¡¨æ¨å¯¼å¼å¤–éƒ¨å¯¼å…¥ re æ¨¡å—ï¼Œæˆ–ç›´æ¥ä½¿ç”¨å¾ªç¯
        is_title_or_toc_item = False
        for pattern in section_patterns:
            if re.search(pattern, current_clean):
                is_title_or_toc_item = True
                break
        
        # å¦‚æœåŒ…å«ç« èŠ‚ç¼–å·ï¼Œå³ä½¿å®Œå…¨é‡å¤ä¹Ÿå…è®¸ï¼ˆæ ‡é¢˜å¯ä»¥é‡å¤å‡ºç°ï¼‰
        if is_title_or_toc_item:
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼šå¦‚æœæ˜¯çŸ­æ ‡é¢˜ï¼ˆ<50å­—ç¬¦ï¼‰ï¼Œå…è®¸é‡å¤
            # å¦‚æœæ˜¯é•¿æ®µè½é‡å¤ï¼Œä»ç„¶è®¤ä¸ºæ˜¯é‡å¤å†…å®¹
            if len(current_clean) < 50:
                return False  # çŸ­æ ‡é¢˜å…è®¸é‡å¤
        
        # å¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œä½¿ç”¨å­—ç¬¦çº§åˆ«æ¯”è¾ƒï¼›å¯¹äºè‹±æ–‡ï¼Œä½¿ç”¨å•è¯çº§åˆ«
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        has_chinese = bool(_RE_CHINESE.search(current_clean))
        
        for prev_text in previous_texts[-MAX_DUPLICATE_CHECK:]:  # åªæ£€æŸ¥æœ€è¿‘Nä¸ªæ–‡æœ¬ï¼Œé¿å…æ€§èƒ½é—®é¢˜
            prev_clean = _RE_WHITESPACE.sub(' ', prev_text.strip())
            
            if len(prev_clean) < MIN_TEXT_LENGTH_FOR_DUPLICATE:
                continue
            
            # âš ï¸ ä¿®æ”¹ï¼šå¯¹äºå®Œå…¨ç›¸åŒçš„çŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰ï¼Œå¦‚æœæ–‡æœ¬è¾ƒé•¿ï¼ˆ>50å­—ç¬¦ï¼‰æ‰è®¤ä¸ºæ˜¯é‡å¤
            # å¦‚æœæ˜¯çŸ­æ–‡æœ¬ä¸”åŒ…å«ç« èŠ‚ç¼–å·ï¼Œå…è®¸é‡å¤ï¼ˆæ ‡é¢˜åœ¨ç›®å½•å’Œæ­£æ–‡ä¸­éƒ½ä¼šå‡ºç°ï¼‰
            if current_clean.lower() == prev_clean.lower():
                # å¦‚æœæ˜¯çŸ­æ–‡æœ¬ï¼ˆæ ‡é¢˜ï¼‰ï¼Œå…è®¸é‡å¤
                if len(current_clean) < 50:
                    continue  # è·³è¿‡çŸ­æ–‡æœ¬çš„å®Œå…¨åŒ¹é…æ£€æŸ¥ï¼Œå…è®¸æ ‡é¢˜é‡å¤
                else:
                    # é•¿æ–‡æœ¬å®Œå…¨é‡å¤ï¼Œè®¤ä¸ºæ˜¯é‡å¤å†…å®¹
                    return True
            
            # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
            if has_chinese:
                # ä¸­æ–‡ï¼šä½¿ç”¨å­—ç¬¦çº§åˆ«æ¯”è¾ƒï¼ˆæ›´é€‚åˆä¸­æ–‡ï¼‰
                current_chars = set(current_clean.lower())
                prev_chars = set(prev_clean.lower())
                
                # ç§»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼Œåªæ¯”è¾ƒæœ‰æ„ä¹‰å­—ç¬¦
                current_chars = {c for c in current_chars if c.strip() and c not in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€'}
                prev_chars = {c for c in prev_chars if c.strip() and c not in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€'}
                
                if len(current_chars) < 5 or len(prev_chars) < 5:
                    continue
                
                intersection = len(current_chars & prev_chars)
                union = len(current_chars | prev_chars)
            else:
                # è‹±æ–‡ï¼šä½¿ç”¨å•è¯çº§åˆ«æ¯”è¾ƒ
                current_words = set(current_clean.lower().split())
                prev_words = set(prev_clean.lower().split())
                
                if len(current_words) < 3 or len(prev_words) < 3:
                    continue
                
                intersection = len(current_words & prev_words)
                union = len(current_words | prev_words)
            
            if union == 0:
                continue
            
            similarity = intersection / union
            
            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯é‡å¤å†…å®¹
            if similarity >= similarity_threshold:
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå¦‚æœæ–‡æœ¬é•¿åº¦ç›¸è¿‘ä¸”ç›¸ä¼¼åº¦å¾ˆé«˜
                length_ratio = min(len(current_clean), len(prev_clean)) / max(len(current_clean), len(prev_clean))
                if length_ratio > 0.85 and similarity >= 0.9:
                    return True
        
        return False
    
    def _cleanup_temp_files(self, temp_files: List[str], temp_dirs: List[str]) -> None:
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œä¸´æ—¶ç›®å½•
        
        å‚æ•°:
            temp_files: éœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            temp_dirs: éœ€è¦æ¸…ç†çš„ä¸´æ—¶ç›®å½•è·¯å¾„åˆ—è¡¨
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ä¿ç•™ä¸´æ—¶æ–‡ä»¶
        keep_temp = getattr(settings, 'DEBUG_KEEP_TEMP_FILES', False)
        if keep_temp:
            logger.debug(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] è°ƒè¯•æ¨¡å¼ï¼šä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼Œæ–‡ä»¶={temp_files}, ç›®å½•={temp_dirs}")
            return
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in temp_files:
            if temp_file and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    logger.debug(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                except Exception as e:
                    logger.warning(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_file}, é”™è¯¯: {e}")
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆæ³¨æ„ï¼šç›®å½•å¿…é¡»åœ¨æ–‡ä»¶ä¹‹åæ¸…ç†ï¼Œä¸”ç›®å½•ä¸ºç©ºæ‰èƒ½åˆ é™¤ï¼‰
        for temp_dir in temp_dirs:
            if temp_dir and os.path.isdir(temp_dir):
                try:
                    # å…ˆå°è¯•åˆ é™¤ç›®å½•å†…çš„æ‰€æœ‰æ–‡ä»¶
                    try:
                        for item in os.listdir(temp_dir):
                            item_path = os.path.join(temp_dir, item)
                            if os.path.isfile(item_path):
                                os.remove(item_path)
                            elif os.path.isdir(item_path):
                                shutil.rmtree(item_path)
                    except Exception as e:
                        logger.debug(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] æ¸…ç†ä¸´æ—¶ç›®å½•å†…å®¹æ—¶å‡ºé”™: {e}")
                    
                    # åˆ é™¤ç©ºç›®å½•
                    try:
                        os.rmdir(temp_dir)
                        logger.debug(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_dir}")
                    except OSError:
                        # å¦‚æœç›®å½•ä¸ä¸ºç©ºæˆ–åˆ é™¤å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨shutilå¼ºåˆ¶åˆ é™¤
                        try:
                            shutil.rmtree(temp_dir)
                            logger.debug(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] å·²å¼ºåˆ¶åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_dir}")
                        except Exception as e:
                            logger.warning(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] åˆ é™¤ä¸´æ—¶ç›®å½•å¤±è´¥: {temp_dir}, é”™è¯¯: {e}")
                except Exception as e:
                    logger.warning(f"[ä¸´æ—¶æ–‡ä»¶æ¸…ç†] å¤„ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {temp_dir}, é”™è¯¯: {e}")
    
    def _get_file_type(self, file_extension: str) -> str:
        """è·å–æ–‡ä»¶ç±»å‹"""
        extension_map = {
            '.pdf': 'pdf',
            '.docx': 'docx',
            '.pptx': 'pptx',
            '.html': 'html',
            '.htm': 'html',
            '.txt': 'txt',
            '.md': 'txt',
            '.csv': 'txt',
            '.json': 'txt',
            '.xml': 'txt'
        }
        return extension_map.get(file_extension, 'txt')
    
    def _process_parsed_elements(self, elements: List, file_path: str, file_size: int, is_pdf: Optional[bool] = None) -> Dict[str, Any]:
        """å¤„ç†è§£æåçš„å…ƒç´ """
        try:
            def _meta_get(meta_obj: Any, key: str, default: Any = None):
                if meta_obj is None:
                    return default
                # ElementMetadata åœºæ™¯
                val = getattr(meta_obj, key, None)
                if val is not None:
                    return val
                # å…¼å®¹ dict
                if isinstance(meta_obj, dict):
                    return meta_obj.get(key, default)
                # å…¼å®¹æä¾› to_dict()
                to_dict = getattr(meta_obj, 'to_dict', None)
                if callable(to_dict):
                    try:
                        d = to_dict()
                        if isinstance(d, dict):
                            return d.get(key, default)
                    except Exception:
                        pass
                return default
            # æå–æ–‡æœ¬å†…å®¹å’Œå…ƒç´ ç´¢å¼•æ˜ å°„ï¼ˆç”¨äº100%è¿˜åŸæ–‡æ¡£é¡ºåºï¼‰
            text_content = ""
            images = []
            tables = []
            # æ–‡æœ¬å…ƒç´ ç´¢å¼•æ˜ å°„ï¼šè®°å½•æ¯ä¸ªæ–‡æœ¬æ®µè½åœ¨ text_content ä¸­çš„ä½ç½®èŒƒå›´å¯¹åº”çš„ element_index
            text_element_index_map = []  # [(start_pos, end_pos, element_index), ...]
            current_text_pos = 0
            file_type = os.path.splitext(file_path)[1].lower()
            
            # å¦‚æœæœªæ˜ç¡®æŒ‡å®šis_pdfï¼Œåˆ™æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­
            # ä½†å¦‚æœæ˜ç¡®æŒ‡å®šäº†ï¼ˆå¦‚Wordè½¬PDFçš„æƒ…å†µï¼‰ï¼Œä½¿ç”¨æŒ‡å®šçš„å€¼
            if is_pdf is None:
                is_pdf = file_type == '.pdf'
            
            metadata = {
                'file_size': file_size,
                'file_type': file_type,
                'element_count': len(elements),
                'parsing_timestamp': datetime.utcnow().isoformat() + "Z",
                'is_pdf': is_pdf  # æ ‡è®°æ˜¯å¦ä¸ºPDFæ–‡ä»¶ï¼ˆåŒ…æ‹¬è½¬æ¢åçš„PDFï¼‰
            }
            
            # PDFæ–‡ä»¶çš„ç‰¹æ®Šå¤„ç†æç¤º
            if is_pdf:
                if file_type == '.pdf':
                    logger.debug("æ£€æµ‹åˆ°PDFæ–‡ä»¶ï¼Œå°†åº”ç”¨å¢å¼ºçš„OCRå™ªå£°è¿‡æ»¤ç­–ç•¥")
                else:
                    logger.debug(f"æ£€æµ‹åˆ°è½¬æ¢åçš„PDFï¼ˆåŸæ–‡ä»¶: {file_type}ï¼‰ï¼Œå°†åº”ç”¨å¢å¼ºçš„OCRå™ªå£°è¿‡æ»¤ç­–ç•¥")
            
            # è¾¹ç•Œæƒ…å†µå¤„ç†ï¼šå¦‚æœelementsä¸ºç©ºï¼Œç›´æ¥è¿”å›
            if not elements or len(elements) == 0:
                logger.warning(f"è§£æç»“æœä¸ºç©ºï¼Œelementsåˆ—è¡¨ä¸ºç©º")
                return {
                    "text_content": "",
                    "images": [],
                    "tables": [],
                    "metadata": metadata,
                    "char_count": 0,
                    "word_count": 0,
                    "element_count": 0,
                    "text_element_index_map": []
                }
            
            # è·å–é¡µé¢é«˜åº¦å’Œå®½åº¦ä¿¡æ¯ï¼ˆç”¨äºä½ç½®åˆ¤æ–­ï¼‰å’Œæ€»é¡µæ•°ä¿¡æ¯ï¼ˆç”¨äºå°é¢/å°åº•åˆ¤æ–­ï¼‰
            # ä¼˜åŒ–ï¼šåœ¨ç¬¬ä¸€ä¸ªå¾ªç¯ä¸­åŒæ—¶è®¡ç®—ï¼Œé¿å…é‡å¤éå†
            page_height = 0
            page_width = 0
            max_page_number = 1
            
            # éå†æ‰€æœ‰å…ƒç´ ä»¥è·å–å‡†ç¡®çš„é¡µé¢å°ºå¯¸å’Œæœ€å¤§é¡µç 
            # æ³¨æ„ï¼šè¿™é‡Œåªéå†ä¸€æ¬¡ï¼Œåç»­å¤„ç†å¾ªç¯ä¼šå†æ¬¡éå†ï¼Œä½†è¿™æ¬¡éå†æ˜¯å¿…éœ€çš„
            for elem in elements:
                # è·å–é¡µç ä¿¡æ¯
                elem_meta = getattr(elem, 'metadata', None)
                page_num = _meta_get(elem_meta, 'page_number', 1)
                if page_num > max_page_number:
                    max_page_number = page_num
                
                # è·å–åæ ‡ä¿¡æ¯ä»¥ä¼°ç®—é¡µé¢å°ºå¯¸
                coords = self._extract_coordinates(elem)
                if coords and coords.get('height', 0) > 0:
                    # æ ¹æ®åæ ‡ä¼°ç®—é¡µé¢é«˜åº¦ï¼ˆyåæ ‡ + å…ƒç´ é«˜åº¦ï¼‰
                    element_bottom = coords.get('y', 0) + coords.get('height', 0)
                    if element_bottom > page_height:
                        page_height = element_bottom
                    
                    # æ ¹æ®åæ ‡ä¼°ç®—é¡µé¢å®½åº¦ï¼ˆxåæ ‡ + å…ƒç´ å®½åº¦ï¼‰
                    element_right = coords.get('x', 0) + coords.get('width', 0)
                    if element_right > page_width:
                        page_width = element_right
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°ï¼Œä½¿ç”¨æ ‡å‡†A4å°ºå¯¸ä½œä¸ºé»˜è®¤å€¼
            if page_height == 0:
                page_height = DEFAULT_PAGE_HEIGHT
                logger.warning(f"[PDFè¯Šæ–­] æ— æ³•ä»å…ƒç´ åæ ‡è®¡ç®—é¡µé¢é«˜åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {page_height}")
            else:
                logger.debug(f"[PDFè¯Šæ–­] è®¡ç®—å¾—åˆ°çš„é¡µé¢é«˜åº¦: {page_height}")
            if page_width == 0:
                page_width = 595  # A4å®½åº¦ï¼ˆç‚¹ï¼‰ï¼Œæ ‡å‡†A4çº¸å¼ å°ºå¯¸ï¼š595Ã—842ç‚¹
                logger.warning(f"[PDFè¯Šæ–­] æ— æ³•ä»å…ƒç´ åæ ‡è®¡ç®—é¡µé¢å®½åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {page_width}")
            else:
                logger.debug(f"[PDFè¯Šæ–­] è®¡ç®—å¾—åˆ°çš„é¡µé¢å®½åº¦: {page_width}")
            
            # âœ… PDFè¯Šæ–­ï¼šè®°å½•åæ ‡ä¿¡æ¯ç»Ÿè®¡
            if is_pdf:
                coordinates_count = 0
                coordinates_with_valid_y = 0
                coordinates_with_valid_x = 0
                y_positions = []
                for elem in elements[:50]:  # åªæ£€æŸ¥å‰50ä¸ªå…ƒç´ ï¼ˆé¿å…å¤ªå¤šæ—¥å¿—ï¼‰
                    coords = self._extract_coordinates(elem)
                    if coords and (coords.get('x', 0) > 0 or coords.get('y', 0) > 0):
                        coordinates_count += 1
                        y_pos = coords.get('y', 0)
                        x_pos = coords.get('x', 0)
                        if y_pos > 0:
                            coordinates_with_valid_y += 1
                            y_positions.append(y_pos)
                        if x_pos > 0:
                            coordinates_with_valid_x += 1
                
                logger.info(f"[PDFè¯Šæ–­] åæ ‡ä¿¡æ¯ç»Ÿè®¡: å‰50ä¸ªå…ƒç´ ä¸­, æœ‰åæ ‡={coordinates_count}, "
                           f"æœ‰æ•ˆYåæ ‡={coordinates_with_valid_y}, æœ‰æ•ˆXåæ ‡={coordinates_with_valid_x}")
                if y_positions:
                    logger.info(f"[PDFè¯Šæ–­] Yåæ ‡èŒƒå›´: min={min(y_positions):.1f}, max={max(y_positions):.1f}, "
                               f"å¹³å‡={sum(y_positions)/len(y_positions):.1f}")
                    # å¦‚æœYåæ ‡çš„æœ€å¤§å€¼å°äºé¡µé¢é«˜åº¦çš„80%ï¼Œå¯èƒ½åæ ‡ç³»ç»Ÿæœ‰é—®é¢˜
                    if max(y_positions) < page_height * 0.8:
                        logger.warning(f"[PDFè¯Šæ–­] âš ï¸ è­¦å‘Š: Yåæ ‡æœ€å¤§å€¼({max(y_positions):.1f})æ˜æ˜¾å°äºé¡µé¢é«˜åº¦({page_height}), "
                                      f"å¯èƒ½æ˜¯åæ ‡ç³»ç»Ÿä¸å‡†ç¡®ï¼Œé¡µçœ‰é¡µè„šæ£€æµ‹å¯èƒ½å¤±æ•ˆ")
            
            # ç”¨äºç»Ÿè®¡è¿‡æ»¤ä¿¡æ¯çš„è®¡æ•°å™¨
            filter_stats = {
                'blank_content': 0,
                'header_footer': 0,
                'noise_text': 0,
                'copyright': 0,
                'watermark': 0,
                'cover_page': 0,
                'footnote': 0,
                'duplicate': 0,
                'toc_converted': 0
            }
            
            # ç”¨äºé‡å¤å†…å®¹æ£€æµ‹çš„æ–‡æœ¬å†å²è®°å½•
            previous_texts = []  # å­˜å‚¨å·²å¤„ç†çš„æœ‰æ•ˆæ–‡æœ¬ï¼ˆç”¨äºé‡å¤æ£€æµ‹ï¼‰
            
            for element_index, element in enumerate(elements):
                element_text = getattr(element, 'text', '')
                element_category = getattr(element, 'category', 'Unknown')
                
                # åŸºç¡€æ¸…æ´—ï¼šå»é™¤å›è½¦ã€åˆå¹¶å¤šç©ºæ ¼ã€ä¿®å¤è¢«æ¢è¡Œæ‰“æ–­çš„è¡Œ
                if element_text:
                    element_text = element_text.replace('\r', '')
                    # è¿ç»­ç©ºæ ¼/åˆ¶è¡¨ç¬¦å½’ä¸€ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼‰
                    element_text = _RE_TABS.sub(" ", element_text)
                    element_text = _RE_MULTI_SPACES.sub(" ", element_text)
                
                # è·å–åæ ‡å’Œé¡µç ä¿¡æ¯
                coordinates = self._extract_coordinates(element)
                page_number = _meta_get(getattr(element, 'metadata', None), 'page_number', 1)
                
                # ========== æ–‡æ¡£é™å™ªå¤„ç† ==========
                # ä½¿ç”¨try-exceptç¡®ä¿å•ä¸ªè¿‡æ»¤å™¨å¤±è´¥ä¸å½±å“æ•´ä½“å¤„ç†
                
                # 1. ç©ºç™½å†…å®¹è¿‡æ»¤ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
                if settings.ENABLE_BLANK_CONTENT_FILTER:
                    try:
                        if self._is_blank_content(element_text):
                            filter_stats['blank_content'] += 1
                            logger.debug(f"æ£€æµ‹åˆ°ç©ºç™½å†…å®¹ï¼Œè·³è¿‡å…ƒç´  {element_index}")
                            continue
                    except Exception as e:
                        logger.warning(f"ç©ºç™½å†…å®¹è¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # âš ï¸ é‡è¦ï¼šé€šç”¨å†…å®¹ä¿æŠ¤æœºåˆ¶
                # åœ¨æ‰€æœ‰é™å™ªè¿‡æ»¤å™¨ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹
                # å¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼ˆå¦‚ç« èŠ‚æ ‡é¢˜ã€æŠ€æœ¯å†…å®¹ã€æœ‰æ„ä¹‰æ®µè½ç­‰ï¼‰ï¼Œè·³è¿‡æ‰€æœ‰é™å™ªè¿‡æ»¤
                is_valid_content = False
                if element_text:
                    try:
                        is_valid_content = self._is_valid_content(element_text)
                        if is_valid_content:
                            logger.debug(f"æ£€æµ‹åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ‰€æœ‰é™å™ªè¿‡æ»¤: å…ƒç´  {element_index}: {element_text[:50]}...")
                    except Exception as e:
                        logger.warning(f"å†…å®¹ä¿æŠ¤æ£€æŸ¥å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 2. é¡µçœ‰é¡µè„šè¿‡æ»¤
                # âš ï¸ æ³¨æ„ï¼šå¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ­¤è¿‡æ»¤å™¨
                if not is_valid_content and settings.ENABLE_HEADER_FOOTER_FILTER:
                    try:
                        if element_text:
                            is_header_footer = self._is_header_or_footer(element_text, element_category, coordinates, page_number, page_height)
                            if is_header_footer:
                                filter_stats['header_footer'] += 1
                                # âœ… PDFè¯Šæ–­ï¼šè®°å½•é¡µçœ‰é¡µè„šæ£€æµ‹è¯¦æƒ…
                                if is_pdf:
                                    y_pos = coordinates.get('y', 0) if coordinates else 0
                                    element_height = coordinates.get('height', 0) if coordinates else 0
                                    element_bottom = y_pos + element_height
                                    header_ratio = (y_pos / page_height) if page_height > 0 else 0
                                    footer_ratio = (element_bottom / page_height) if page_height > 0 else 0
                                    logger.info(f"[PDFè¯Šæ–­] æ£€æµ‹åˆ°é¡µçœ‰é¡µè„š: å…ƒç´  {element_index}, "
                                               f"æ–‡æœ¬='{element_text[:30]}...', "
                                               f"é¡µç ={page_number}, "
                                               f"Yä½ç½®={y_pos:.1f} (å é¡µé¢{header_ratio:.1%}), "
                                               f"åº•éƒ¨={element_bottom:.1f} (å é¡µé¢{footer_ratio:.1%}), "
                                               f"é¡µé¢é«˜åº¦={page_height:.1f}")
                                else:
                                    logger.debug(f"æ£€æµ‹åˆ°é¡µçœ‰é¡µè„šï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                                continue
                    except Exception as e:
                        logger.warning(f"é¡µçœ‰é¡µè„šè¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 3. å™ªå£°æ–‡æœ¬è¿‡æ»¤ï¼ˆOCRé”™è¯¯ã€ç¢ç‰‡æ–‡æœ¬ç­‰ï¼‰
                # PDFæ–‡ä»¶ä½¿ç”¨æ›´ä¸¥æ ¼çš„OCRå™ªå£°è¿‡æ»¤
                # âš ï¸ æ³¨æ„ï¼šå¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ­¤è¿‡æ»¤å™¨
                if not is_valid_content and settings.ENABLE_NOISE_TEXT_FILTER:
                    try:
                        if self._is_noise_text(element_text, is_pdf=is_pdf):
                            filter_stats['noise_text'] += 1
                            logger.debug(f"æ£€æµ‹åˆ°å™ªå£°æ–‡æœ¬{'ï¼ˆPDF OCRé”™è¯¯ï¼‰' if is_pdf else ''}ï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                            continue
                    except Exception as e:
                        logger.warning(f"å™ªå£°æ–‡æœ¬è¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 4. ç‰ˆæƒå£°æ˜é¡µè¿‡æ»¤
                # âš ï¸ æ³¨æ„ï¼šå¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ­¤è¿‡æ»¤å™¨
                if not is_valid_content and settings.ENABLE_COPYRIGHT_FILTER:
                    try:
                        if self._is_copyright_page(element_text, page_number, max_page_number, coordinates):
                            filter_stats['copyright'] += 1
                            logger.debug(f"æ£€æµ‹åˆ°ç‰ˆæƒå£°æ˜ï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                            continue
                    except Exception as e:
                        logger.warning(f"ç‰ˆæƒå£°æ˜è¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 5. æ°´å°è¿‡æ»¤
                # âš ï¸ æ³¨æ„ï¼šå¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ­¤è¿‡æ»¤å™¨
                if not is_valid_content and settings.ENABLE_WATERMARK_FILTER:
                    try:
                        if self._is_watermark(element_text, coordinates, page_number, page_height, page_width):
                            filter_stats['watermark'] += 1
                            logger.debug(f"æ£€æµ‹åˆ°æ°´å°ï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                            continue
                    except Exception as e:
                        logger.warning(f"æ°´å°è¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 6. å°é¢/å°åº•é¡µè¿‡æ»¤
                # âš ï¸ æ³¨æ„ï¼šå¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ­¤è¿‡æ»¤å™¨
                if not is_valid_content and settings.ENABLE_COVER_PAGE_FILTER:
                    try:
                        if self._is_cover_or_back_page(element_text, page_number, max_page_number, coordinates, page_height, page_width):
                            filter_stats['cover_page'] += 1
                            logger.debug(f"æ£€æµ‹åˆ°å°é¢/å°åº•é¡µï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                            continue
                    except Exception as e:
                        logger.warning(f"å°é¢/å°åº•é¡µè¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 7. è„šæ³¨/é¡µè¾¹æ³¨é‡Šè¿‡æ»¤ï¼ˆå¯é€‰ï¼ŒæŸäº›è„šæ³¨å¯èƒ½æ˜¯æœ‰ç”¨çš„ï¼‰
                # âš ï¸ æ³¨æ„ï¼šå¦‚æœæ˜¯æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡æ­¤è¿‡æ»¤å™¨
                if not is_valid_content and settings.ENABLE_FOOTNOTE_FILTER:
                    try:
                        if self._is_footnote_or_margin_note(element_text, coordinates, page_height):
                            filter_stats['footnote'] += 1
                            logger.debug(f"æ£€æµ‹åˆ°è„šæ³¨/é¡µè¾¹æ³¨é‡Šï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                            continue
                    except Exception as e:
                        logger.warning(f"è„šæ³¨è¿‡æ»¤å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # 8. é‡å¤å†…å®¹æ£€æµ‹ï¼ˆåœ¨å¤„ç†æ–‡æœ¬å…ƒç´ æ—¶æ£€æµ‹ï¼‰
                # æ³¨æ„ï¼šé‡å¤æ£€æµ‹éœ€è¦åœ¨æ–‡æœ¬æå–ä¹‹å‰è¿›è¡Œï¼Œä½†åªå¯¹æ–‡æœ¬å…ƒç´ ç”Ÿæ•ˆ
                
                # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦æ˜¯ç›®å½•ï¼ˆç›®å½•åº”è¯¥ä½œä¸ºæ–‡æœ¬å¤„ç†ï¼Œè€Œä¸æ˜¯è¡¨æ ¼ï¼‰
                is_toc = False
                if settings.ENABLE_TOC_DETECTION:
                    try:
                        if element_category == 'Table' and element_text:
                            if self._is_table_of_contents(element_text, element_category, coordinates, page_number):
                                filter_stats['toc_converted'] += 1
                                logger.info(f"æ£€æµ‹åˆ°ç›®å½•ï¼ˆè¢«è¯¯è¯†åˆ«ä¸ºè¡¨æ ¼ï¼‰ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬å¤„ç†: å…ƒç´  {element_index}")
                                # å°†ç›®å½•ä½œä¸ºæ–‡æœ¬å¤„ç†ï¼Œè€Œä¸æ˜¯è¡¨æ ¼
                                element_category = 'NarrativeText'  # ä¸´æ—¶ä¿®æ”¹ç±»åˆ«ï¼Œä½¿å…¶è¿›å…¥æ–‡æœ¬å¤„ç†åˆ†æ”¯
                                is_toc = True  # æ ‡è®°ä¸ºç›®å½•ï¼Œé¿å…è¿›å…¥è¡¨æ ¼å¤„ç†åˆ†æ”¯
                    except Exception as e:
                        logger.warning(f"ç›®å½•è¯†åˆ«å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                
                # å¤„ç†æ–‡æœ¬å…ƒç´ ï¼ˆåŒ…æ‹¬ Title, NarrativeText, ListItem ç­‰ï¼Œä»¥åŠè¢«è¯†åˆ«ä¸ºç›®å½•çš„è¡¨æ ¼ï¼‰
                if element_text and element_category not in ['Image', 'Table']:
                    # 8. é‡å¤å†…å®¹æ£€æµ‹ï¼ˆä»…å¯¹æ–‡æœ¬å…ƒç´ è¿›è¡Œï¼‰
                    # âš ï¸ æ³¨æ„ï¼šæ ‡é¢˜å’Œç›®å½•é¡¹åœ¨ç›®å½•å’Œæ­£æ–‡ä¸­é‡å¤å‡ºç°æ˜¯æ­£å¸¸çš„ï¼Œä¸åº”è¢«è¿‡æ»¤
                    if settings.ENABLE_DUPLICATE_DETECTION:
                        try:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜æˆ–ç›®å½•é¡¹ï¼ˆåŒ…å«ç« èŠ‚ç¼–å·çš„çŸ­æ–‡æœ¬ï¼‰
                            section_patterns = [
                                r'^\d+\.',  # å•æ•°å­—åŠ ç‚¹å¼€å¤´ï¼Œå¦‚ 1.ã€4. ç­‰
                                r'\d+\.\d+(?:\.\d+)*',  # 1.1, 1.2.3, 1.2.3.3.1
                                r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]',  # ä¸€ã€äºŒã€ä¸‰ã€
                                r'[IVX]+\.\d+',  # I.1, II.2
                                r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
                            ]
                            text_clean = element_text.strip()
                            
                            # âš ï¸ ä¼˜åŒ–ï¼šæ”¾å®½æ ‡é¢˜æ£€æµ‹æ¡ä»¶ï¼Œå³ä½¿æ–‡æœ¬è¾ƒé•¿ï¼ˆ<150å­—ç¬¦ï¼‰ä¹Ÿå…è®¸
                            # å› ä¸ºæ ‡é¢˜å¯èƒ½åŒ…å«ä¸€äº›æè¿°æ€§æ–‡å­—ï¼Œå¦‚"1.2.3.3.1.åœä¸»èŠ‚ç‚¹ mongo-0 æ˜¯å¦åˆ‡æ¢..."
                            # âœ… ä¿®å¤ä½œç”¨åŸŸé—®é¢˜ï¼šä½¿ç”¨å¾ªç¯ä»£æ›¿ç”Ÿæˆå™¨è¡¨è¾¾å¼
                            is_title_or_toc = False
                            for pattern in section_patterns:
                                if re.search(pattern, text_clean):
                                    is_title_or_toc = True
                                    break
                            
                            # å¦‚æœåŒ…å«ç« èŠ‚ç¼–å·ï¼Œæ— è®ºæ˜¯çŸ­æ–‡æœ¬è¿˜æ˜¯ç¨é•¿çš„æ–‡æœ¬ï¼ˆ<150å­—ç¬¦ï¼‰ï¼Œéƒ½å…è®¸é‡å¤
                            if is_title_or_toc and len(text_clean) < 150:
                                logger.debug(f"æ£€æµ‹åˆ°æ ‡é¢˜/ç›®å½•é¡¹ï¼ˆåŒ…å«ç« èŠ‚ç¼–å·ï¼‰ï¼Œå…è®¸é‡å¤å‡ºç°: å…ƒç´  {element_index}: {element_text[:50]}...")
                            else:
                                # éæ ‡é¢˜å†…å®¹æ‰è¿›è¡Œé‡å¤æ£€æµ‹
                                if self._detect_duplicate_content(element_text, previous_texts):
                                    filter_stats['duplicate'] += 1
                                    logger.debug(f"æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼Œè·³è¿‡å…ƒç´  {element_index}: {element_text[:50]}...")
                                    continue  # è·³è¿‡é‡å¤å†…å®¹
                        except Exception as e:
                            logger.warning(f"é‡å¤å†…å®¹æ£€æµ‹å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†å…ƒç´  {element_index}")
                    
                    # æ·»åŠ åˆ°æ–‡æœ¬å†…å®¹
                    text_start_pos = current_text_pos
                    text_end_pos = current_text_pos + len(element_text)
                    text_content += element_text + "\n"
                    # è®°å½•æ–‡æœ¬æ®µè½çš„ element_index æ˜ å°„
                    text_element_index_map.append({
                        'start_pos': text_start_pos,
                        'end_pos': text_end_pos - 1,  # å‡å»æ¢è¡Œç¬¦
                        'element_index': element_index,
                        'element_type': element.category,  # ä½¿ç”¨åŸå§‹ç±»åˆ«
                        'page_number': page_number,
                        'coordinates': coordinates
                    })
                    current_text_pos = text_end_pos + 1  # +1 for \n
                    
                    # å°†å¤„ç†åçš„æ–‡æœ¬æ·»åŠ åˆ°å†å²è®°å½•ï¼ˆç”¨äºé‡å¤æ£€æµ‹ï¼‰
                    if settings.ENABLE_DUPLICATE_DETECTION:
                        previous_texts.append(element_text)
                        # é™åˆ¶å†å²è®°å½•å¤§å°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
                        if len(previous_texts) > MAX_PREVIOUS_TEXTS:
                            previous_texts.pop(0)  # ç§»é™¤æœ€æ—§çš„è®°å½•
                
                # æå–å›¾ç‰‡ä¿¡æ¯ï¼ˆè®°å½• element_indexï¼‰
                elif element.category == 'Image':
                    elem_id = getattr(element, 'element_id', getattr(element, 'id', None))
                    # å°è¯•ä» Unstructured å…ƒæ•°æ®ä¸­å–å‡ºå›¾ç‰‡äºŒè¿›åˆ¶ï¼ˆbase64ï¼‰
                    data_bytes = None
                    image_ext = '.png'
                    try:
                        meta_obj = getattr(element, 'metadata', None)
                        # å¸¸è§å­—æ®µï¼šimage_base64 / image_bytes / binary / data ç­‰
                        b64 = None
                        for key in ('image_base64', 'image_data', 'data', 'binary'):
                            val = getattr(meta_obj, key, None)
                            if val is None and isinstance(meta_obj, dict):
                                val = meta_obj.get(key)
                            if val:
                                b64 = val
                                break
                        if not b64:
                            # æŸäº›å®ç°å¯èƒ½æŠŠbase64æ”¾åœ¨ element è‡ªèº«
                            for key in ('image_base64', 'image_data', 'data', 'binary'):
                                val = getattr(element, key, None)
                                if val:
                                    b64 = val
                                    break
                        if b64:
                            from app.utils.conversion_utils import base64_to_bytes
                            data_bytes = base64_to_bytes(b64)
                        else:
                            # å°è¯•ä» element.image (PIL) å¯¼å‡º
                            pil_img = getattr(element, 'image', None)
                            if pil_img is not None:
                                try:
                                    from io import BytesIO
                                    buf = BytesIO()
                                    pil_img.save(buf, format='PNG')
                                    data_bytes = buf.getvalue()
                                    image_ext = '.png'
                                except Exception:
                                    data_bytes = None
                        # è¿›ä¸€æ­¥å…œåº•ï¼šæœ‰äº›è§£æå™¨ä»…ç»™å‡ºç£ç›˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                        if not data_bytes and meta_obj is not None:
                            for path_key in ('image_path', 'file_path', 'filename', 'png_path', 'path'):
                                p = getattr(meta_obj, path_key, None)
                                if p is None and isinstance(meta_obj, dict):
                                    p = meta_obj.get(path_key)
                                if p and isinstance(p, str) and os.path.exists(p):
                                    try:
                                        with open(p, 'rb') as f:
                                            data_bytes = f.read()
                                        # æ ¹æ®æ‰©å±•åè®¾ç½® ext
                                        _, ext = os.path.splitext(p)
                                        if ext:
                                            image_ext = ext.lower()
                                        break
                                    except Exception:
                                        pass
                    except Exception:
                        data_bytes = None

                    image_info = {
                        'element_id': elem_id,
                        'element_index': element_index,  # å…³é”®ï¼šè®°å½•å…ƒç´ åœ¨åŸå§‹elementsä¸­çš„ç´¢å¼•
                        'image_type': 'image',
                        'page_number': _meta_get(getattr(element, 'metadata', None), 'page_number', 1),
                        'coordinates': self._extract_coordinates(element),
                        'description': element_text,
                        'ocr_text': element_text
                    }
                    # æä¾›ç»™åç»­æµæ°´çº¿çš„äºŒè¿›åˆ¶ä¸æ‰©å±•åï¼ˆè‹¥å¯ç”¨ï¼‰
                    if data_bytes:
                        image_info['data'] = data_bytes
                        image_info['ext'] = image_ext
                    images.append(image_info)
                
                # æå–è¡¨æ ¼ä¿¡æ¯ï¼ˆè®°å½• element_indexï¼‰
                # âœ… ä¿®å¤ï¼šè¡¨æ ¼åº”è¯¥ç‹¬ç«‹æˆå—ï¼Œä¸å‚ä¸æ–‡æœ¬åˆ†å—ï¼Œé¿å…è¡¨æ ¼ç»“æ„è¢«ç ´å
                # è¡¨æ ¼é€šè¿‡ element_index åœ¨åç»­åˆå¹¶æ—¶æ’å…¥åˆ°æ­£ç¡®ä½ç½®ï¼Œä¿æŒåŸæ–‡æ¡£é¡ºåº
                elif element.category == 'Table' and not is_toc:
                    elem_id = getattr(element, 'element_id', getattr(element, 'id', None))
                    
                    # âœ… ä¼˜åŒ–ï¼šä» table_data ä¸­æå–æ›´å®Œæ•´çš„è¡¨æ ¼æ–‡æœ¬è¡¨ç¤º
                    table_data_result = self._extract_table_data(element)
                    
                    # âœ… ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–æ•°æ®ç”Ÿæˆè¡¨æ ¼æ–‡æœ¬ï¼Œé¿å…OCRé”™è¯¯
                    # ç­–ç•¥ä¼˜å…ˆçº§ï¼šcells > htmlæå– > element_textï¼ˆOCRæ–‡æœ¬ï¼Œå¯èƒ½æœ‰é”™è¯¯ï¼‰
                    table_text_optimized = None
                    
                    # 1. ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–çš„ cells æ•°æ®ç”Ÿæˆæ–‡æœ¬ï¼ˆæœ€å‡†ç¡®ï¼‰
                    if table_data_result.get('cells'):
                        try:
                            cells = table_data_result['cells']
                            text_lines = []
                            for row in cells:
                                if isinstance(row, (list, tuple)):
                                    # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œä¿æŒåˆ—å¯¹é½
                                    text_lines.append('\t'.join(str(cell) if cell is not None else '' for cell in row))
                                else:
                                    text_lines.append(str(row))
                            if text_lines:
                                table_text_optimized = '\n'.join(text_lines)
                                logger.debug(f"ä»ç»“æ„åŒ– cells æ•°æ®ç”Ÿæˆè¡¨æ ¼æ–‡æœ¬: {len(text_lines)} è¡Œ")
                        except Exception as e:
                            logger.debug(f"ä»ç»“æ„åŒ– cells æ•°æ®ç”Ÿæˆè¡¨æ ¼æ–‡æœ¬å¤±è´¥: {e}")
                    
                    # 2. å¦‚æœæ²¡æœ‰ cellsï¼Œå°è¯•ä» HTML ä¸­æ­£ç¡®æå–è¡¨æ ¼ç»“æ„æ–‡æœ¬ï¼ˆæ¯”OCRæ–‡æœ¬æ›´å‡†ç¡®ï¼‰
                    if not table_text_optimized and table_data_result.get('html'):
                        try:
                            import re
                            html_text = table_data_result['html']
                            
                            # âœ… ä¿®å¤ï¼šæ­£ç¡®è§£æ HTML è¡¨æ ¼ç»“æ„ï¼Œä¿æŒè¡Œåˆ—å…³ç³»
                            # æ–¹æ³•1ï¼šä½¿ç”¨ BeautifulSoupï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            try:
                                from bs4 import BeautifulSoup
                                soup = BeautifulSoup(html_text, 'html.parser')
                                table = soup.find('table')
                                if table:
                                    text_lines = []
                                    for tr in table.find_all('tr'):
                                        row_cells = []
                                        for td in tr.find_all(['td', 'th']):
                                            cell_text = td.get_text(strip=True)
                                            row_cells.append(cell_text)
                                        if row_cells:
                                            # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”åŒä¸€è¡Œçš„å•å…ƒæ ¼
                                            text_lines.append('\t'.join(row_cells))
                                    
                                    if text_lines:
                                        # ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”ä¸åŒè¡Œ
                                        table_text_optimized = '\n'.join(text_lines)
                                        logger.debug(f"ä» HTML (BeautifulSoup) æå–è¡¨æ ¼æ–‡æœ¬: {len(text_lines)} è¡Œ")
                            except ImportError:
                                # æ–¹æ³•2ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
                                tr_pattern = r'<tr[^>]*>(.*?)</tr>'
                                td_pattern = r'<t[dh][^>]*>(.*?)</t[dh]>'
                                
                                trs = re.findall(tr_pattern, html_text, re.DOTALL | re.IGNORECASE)
                                if trs:
                                    text_lines = []
                                    for tr_content in trs:
                                        tds = re.findall(td_pattern, tr_content, re.DOTALL | re.IGNORECASE)
                                        if tds:
                                            # æ¸…ç†æ¯ä¸ªå•å…ƒæ ¼å†…å®¹
                                            row_cells = []
                                            for td in tds:
                                                # ç§»é™¤å†…åµŒçš„ HTML æ ‡ç­¾
                                                cell_text = re.sub(r'<[^>]+>', '', td)
                                                # å¤„ç† HTML å®ä½“
                                                cell_text = cell_text.replace('&nbsp;', ' ').replace('&amp;', '&')
                                                cell_text = cell_text.replace('&lt;', '<').replace('&gt;', '>')
                                                # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
                                                cell_text = ' '.join(cell_text.split())
                                                row_cells.append(cell_text.strip())
                                            
                                            if row_cells:
                                                # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”åŒä¸€è¡Œçš„å•å…ƒæ ¼
                                                text_lines.append('\t'.join(row_cells))
                                    
                                    if text_lines:
                                        # ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”ä¸åŒè¡Œ
                                        table_text_optimized = '\n'.join(text_lines)
                                        logger.debug(f"ä» HTML (æ­£åˆ™) æå–è¡¨æ ¼æ–‡æœ¬: {len(text_lines)} è¡Œ")
                        except Exception as e:
                            logger.warning(f"ä» HTML æå–è¡¨æ ¼æ–‡æœ¬å¤±è´¥: {e}")
                    
                    # 3. æœ€åæ‰ä½¿ç”¨ element_textï¼ˆå¯èƒ½åŒ…å«OCRé”™è¯¯ï¼‰
                    if not table_text_optimized:
                        table_text_optimized = element_text
                        if is_pdf:
                            logger.warning(f"è¡¨æ ¼æ–‡æœ¬åªèƒ½ä»OCRæ–‡æœ¬ç”Ÿæˆï¼Œå¯èƒ½åŒ…å«OCRé”™è¯¯: {element_text[:50]}...")
                        else:
                            logger.debug(f"ä½¿ç”¨åŸå§‹ element_text ä½œä¸ºè¡¨æ ¼æ–‡æœ¬")
                    
                    table_info = {
                        'element_id': elem_id,
                        'element_index': element_index,  # å…³é”®ï¼šè®°å½•å…ƒç´ åœ¨åŸå§‹elementsä¸­çš„ç´¢å¼•
                        'page_number': page_number,
                        'coordinates': coordinates,
                        'table_data': table_data_result,
                        'table_text': table_text_optimized  # âœ… ä½¿ç”¨ä¼˜åŒ–åçš„è¡¨æ ¼æ–‡æœ¬
                    }
                    tables.append(table_info)
                    # âœ… é‡è¦ï¼šè¡¨æ ¼ä¸æ·»åŠ åˆ° text_contentï¼Œé¿å…è¢«åˆ†å—ç®—æ³•åˆ†å‰²
                    # è¡¨æ ¼ä¼šä½œä¸ºç‹¬ç«‹å—ï¼Œé€šè¿‡ element_index åœ¨åç»­æŒ‰é¡ºåºæ’å…¥
                    logger.info(f"âœ… æå–è¡¨æ ¼ï¼ˆelement_index={element_index}ï¼‰: {table_data_result.get('rows', 0)} è¡Œ x {table_data_result.get('columns', 0)} åˆ—, æ–‡æœ¬é•¿åº¦={len(table_text_optimized)}")
            
            # æå–æ–‡æ¡£å±æ€§
            doc_metadata = self._extract_document_metadata(elements)
            metadata.update(doc_metadata)
            
            # æ·»åŠ è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
            total_filtered = sum(filter_stats.values())
            metadata['filter_stats'] = {
                **filter_stats,
                'total_filtered': total_filtered,
                'filter_rate': total_filtered / len(elements) if len(elements) > 0 else 0.0
            }
            
            result = {
                "text_content": text_content.strip(),
                "images": images,
                "tables": tables,
                "metadata": metadata,
                "char_count": len(text_content),
                "word_count": len(text_content.split()),
                "element_count": len(elements),
                # æ–°å¢ï¼šæ–‡æœ¬å…ƒç´ ç´¢å¼•æ˜ å°„ï¼ˆç”¨äº100%è¿˜åŸæ–‡æ¡£é¡ºåºï¼‰
                "text_element_index_map": text_element_index_map
            }
            
            # è®°å½•è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—
            if total_filtered > 0:
                pdf_note = "ï¼ˆPDFæ–‡ä»¶ï¼Œå·²åº”ç”¨å¢å¼ºOCRå™ªå£°è¿‡æ»¤ï¼‰" if is_pdf else ""
                logger.info(f"æ–‡æ¡£é™å™ªç»Ÿè®¡{pdf_note}: æ€»å…ƒç´ ={len(elements)}, è¿‡æ»¤={total_filtered}, "
                           f"è¿‡æ»¤ç‡={metadata['filter_stats']['filter_rate']:.2%}, "
                           f"è¯¦æƒ…={filter_stats}")
            
            # âœ… PDFè¯Šæ–­ï¼šæ£€æŸ¥é™å™ªå‡†ç¡®æ€§
            if is_pdf:
                # è®¡ç®—é¡µçœ‰é¡µè„šè¿‡æ»¤ç‡
                header_footer_rate = filter_stats.get('header_footer', 0) / len(elements) if len(elements) > 0 else 0
                noise_rate = filter_stats.get('noise_text', 0) / len(elements) if len(elements) > 0 else 0
                
                logger.info(f"[PDFè¯Šæ–­] é™å™ªå‡†ç¡®æ€§åˆ†æ:")
                logger.info(f"[PDFè¯Šæ–­]   é¡µçœ‰é¡µè„šè¿‡æ»¤: {filter_stats.get('header_footer', 0)} ä¸ªå…ƒç´  ({header_footer_rate:.1%})")
                logger.info(f"[PDFè¯Šæ–­]   å™ªå£°æ–‡æœ¬è¿‡æ»¤: {filter_stats.get('noise_text', 0)} ä¸ªå…ƒç´  ({noise_rate:.1%})")
                logger.info(f"[PDFè¯Šæ–­]   æ€»è¿‡æ»¤ç‡: {metadata['filter_stats']['filter_rate']:.1%}")
                
                # è­¦å‘Šï¼šå¦‚æœé¡µçœ‰é¡µè„šè¿‡æ»¤ç‡è¿‡ä½ï¼ˆ<2%ï¼‰ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜
                if header_footer_rate < 0.02 and len(elements) > 50:
                    logger.warning(f"[PDFè¯Šæ–­] âš ï¸ è­¦å‘Š: é¡µçœ‰é¡µè„šè¿‡æ»¤ç‡è¿‡ä½ ({header_footer_rate:.1%})ï¼Œ"
                                 f"å¯èƒ½é¡µçœ‰é¡µè„šæœªè¢«æ­£ç¡®è¯†åˆ«ã€‚è¯·æ£€æŸ¥åæ ‡ä¿¡æ¯æ˜¯å¦å‡†ç¡®ã€‚")
                
                # è­¦å‘Šï¼šå¦‚æœæ€»è¿‡æ»¤ç‡è¿‡é«˜ï¼ˆ>50%ï¼‰ï¼Œå¯èƒ½è¿‡åº¦è¿‡æ»¤
                if metadata['filter_stats']['filter_rate'] > 0.50:
                    logger.warning(f"[PDFè¯Šæ–­] âš ï¸ è­¦å‘Š: æ€»è¿‡æ»¤ç‡è¿‡é«˜ ({metadata['filter_stats']['filter_rate']:.1%})ï¼Œ"
                                 f"å¯èƒ½è¯¯è¿‡æ»¤äº†æ­£å¸¸å†…å®¹ã€‚è¯·æ£€æŸ¥é™å™ªé€»è¾‘ã€‚")
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†è§£æå…ƒç´ é”™è¯¯: {e}", exc_info=True)
            return {
                "text_content": "",
                "images": [],
                "tables": [],
                "metadata": {"error": str(e)},
                "char_count": 0,
                "word_count": 0,
                "element_count": 0
            }
    
    def _extract_coordinates(self, element) -> Dict[str, Any]:
        """æå–å…ƒç´ åæ ‡ä¿¡æ¯"""
        try:
            metadata = getattr(element, 'metadata', None)
            coordinates = None
            if metadata is not None:
                coordinates = getattr(metadata, 'coordinates', None)
                if coordinates is None and isinstance(metadata, dict):
                    coordinates = metadata.get('coordinates')
            
            if coordinates:
                # å…¼å®¹å¯¹è±¡/å­—å…¸ä¸¤ç§ç»“æ„
                if isinstance(coordinates, dict):
                    return {
                        'x': coordinates.get('x', 0),
                        'y': coordinates.get('y', 0),
                        'width': coordinates.get('width', 0),
                        'height': coordinates.get('height', 0)
                    }
                # å¯¹è±¡åœºæ™¯ï¼Œå°½é‡è¯»å–å¯ç”¨å­—æ®µ
                return {
                    'x': getattr(coordinates, 'x', 0),
                    'y': getattr(coordinates, 'y', 0),
                    'width': getattr(coordinates, 'width', 0),
                    'height': getattr(coordinates, 'height', 0)
                }
            else:
                return {
                    'x': 0,
                    'y': 0,
                    'width': 0,
                    'height': 0
                }
        except Exception as e:
            logger.error(f"æå–åæ ‡ä¿¡æ¯é”™è¯¯: {e}")
            return {'x': 0, 'y': 0, 'width': 0, 'height': 0}
    
    def _extract_table_data(self, element) -> Dict[str, Any]:
        """æå–è¡¨æ ¼æ•°æ® - ä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒæ›´å®Œæ•´çš„è¡¨æ ¼ç»“æ„æå–"""
        try:
            table_data: Dict[str, Any] = {
                'rows': 0,
                'columns': 0,
                'cells': [],
                'structure': 'unknown'
            }

            # âœ… ä¼˜åŒ–ï¼šä½¿ç”¨ unstructured.staging.base çš„è¡¨æ ¼è½¬æ¢åŠŸèƒ½
            # è¿™æ ·å¯ä»¥è·å–æ›´å®Œæ•´çš„è¡¨æ ¼ç»“æ„æ•°æ®
            try:
                from unstructured.staging.base import elements_to_json
                # å°†å•ä¸ªè¡¨æ ¼å…ƒç´ è½¬æ¢ä¸º JSONï¼Œè·å–å®Œæ•´ç»“æ„
                element_json = elements_to_json([element])
                if element_json and len(element_json) > 0:
                    elem_data = element_json[0]
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„åŒ–çš„è¡¨æ ¼æ•°æ®
                    if isinstance(elem_data, dict):
                        # ä¼˜å…ˆä½¿ç”¨ text_as_htmlï¼ˆæœ€å®Œæ•´çš„ç»“æ„ï¼‰
                        html = elem_data.get('metadata', {}).get('text_as_html') or \
                               elem_data.get('metadata', {}).get('table_html') or \
                               elem_data.get('text_as_html')
                        if html:
                            table_data['html'] = html
                            logger.debug(f"ä» element_json æå–åˆ° HTML è¡¨æ ¼æ•°æ®ï¼Œé•¿åº¦: {len(html)}")
            except Exception as e:
                logger.debug(f"å°è¯•ä» elements_to_json æå–è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")

            # 1) å°è¯•æå– HTML ç‰‡æ®µï¼ˆä¾¿äºå‰ç«¯åŸæ ·é¢„è§ˆï¼‰
            html = getattr(element, 'text_as_html', None)
            if not html:
                meta = getattr(element, 'metadata', None)
                candidate_keys = ('text_as_html', 'table_html', 'html', 'text_html')
                if meta is not None:
                    for k in candidate_keys:
                        val = getattr(meta, k, None)
                        if val is None and isinstance(meta, dict):
                            val = meta.get(k)
                        if val:
                            html = val
                            break
                        # âœ… æ–°å¢ï¼šå°è¯•è°ƒç”¨ to_dict() æ–¹æ³•è·å–å…ƒæ•°æ®
                        try:
                            if hasattr(meta, 'to_dict'):
                                meta_dict = meta.to_dict()
                                val = meta_dict.get(k)
                                if val:
                                    html = val
                                    break
                        except Exception:
                            pass
            if html and 'html' not in table_data:
                table_data['html'] = html
                logger.debug(f"æå–åˆ° HTML è¡¨æ ¼æ•°æ®ï¼Œé•¿åº¦: {len(html)}")

            # 2) âœ… ä¼˜åŒ–ï¼šæå–ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ï¼ˆå¦‚æœå¯ç”¨äº† infer_table_structureï¼‰
            # unstructured åº“åœ¨å¯ç”¨è¡¨æ ¼ç»“æ„è¯†åˆ«æ—¶ï¼Œä¼šåœ¨ metadata ä¸­å­˜å‚¨æ›´å®Œæ•´çš„è¡¨æ ¼æ•°æ®
            meta = getattr(element, 'metadata', None)
            cells_from_meta = None
            
            if meta:
                # å°è¯•ä» metadata ä¸­è·å–ç»“æ„åŒ–å•å…ƒæ ¼æ•°æ®
                meta_dict = None
                if isinstance(meta, dict):
                    meta_dict = meta
                elif hasattr(meta, 'to_dict'):
                    try:
                        meta_dict = meta.to_dict()
                    except Exception:
                        pass
                
                if meta_dict:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ cells æˆ– table_structure æ•°æ®
                    cells_from_meta = meta_dict.get('cells') or \
                                     meta_dict.get('table_cells') or \
                                     meta_dict.get('structure', {}).get('cells')
                    
                    if cells_from_meta:
                        logger.debug(f"ä» metadata æå–åˆ°ç»“æ„åŒ–å•å…ƒæ ¼æ•°æ®: {len(cells_from_meta)} è¡Œ")
                        table_data['cells'] = cells_from_meta
                        table_data['rows'] = len(cells_from_meta)
                        table_data['columns'] = max((len(r) if isinstance(r, (list, tuple)) else 1 for r in cells_from_meta), default=0)
                        table_data['structure'] = 'structured'

            # 3) âœ… å¦‚æœ HTML å­˜åœ¨ä½† cells ä¸å­˜åœ¨ï¼Œä» HTML ä¸­è§£æ cellsï¼ˆæœ€å‡†ç¡®ï¼‰
            if not cells_from_meta and table_data.get('html'):
                try:
                    import html.parser
                    from html.parser import HTMLParser
                    from io import StringIO
                    
                    html_content = table_data['html']
                    # ä½¿ç”¨ BeautifulSoup æˆ–æ­£åˆ™è¡¨è¾¾å¼è§£æ HTML è¡¨æ ¼
                    # å¦‚æœæ²¡æœ‰ BeautifulSoupï¼Œä½¿ç”¨ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£æ
                    try:
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(html_content, 'html.parser')
                        table = soup.find('table')
                        if table:
                            cells_from_html = []
                            for tr in table.find_all('tr'):
                                row = []
                                for td in tr.find_all(['td', 'th']):
                                    cell_text = td.get_text(strip=True)
                                    row.append(cell_text)
                                if row:  # åªæ·»åŠ éç©ºè¡Œ
                                    cells_from_html.append(row)
                            
                            if cells_from_html and len(cells_from_html) > 0:
                                # éªŒè¯è§£æç»“æœï¼šè‡³å°‘è¦æœ‰2åˆ—æˆ–2è¡Œï¼Œæˆ–è€…HTMLç¡®å®åŒ…å«è¡¨æ ¼ç»“æ„
                                first_row_cols = len(cells_from_html[0]) if cells_from_html else 0
                                if first_row_cols >= 2 or len(cells_from_html) >= 2:
                                    table_data['cells'] = cells_from_html
                                    table_data['rows'] = len(cells_from_html)
                                    table_data['columns'] = max((len(r) for r in cells_from_html), default=0)
                                    table_data['structure'] = 'html_parsed'
                                    cells_from_meta = cells_from_html  # æ ‡è®°å·²æ‰¾åˆ°ï¼Œé¿å…ç»§ç»­æ‰§è¡Œæ–‡æœ¬è§£æ
                                    logger.info(f"âœ… ä» HTML è§£æå‡ºè¡¨æ ¼ cells: {table_data['rows']} è¡Œ x {table_data['columns']} åˆ—")
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰ BeautifulSoupï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç®€å•è§£æ
                        import re
                        # æå–æ‰€æœ‰ tr æ ‡ç­¾ä¸­çš„å†…å®¹
                        tr_pattern = r'<tr[^>]*>(.*?)</tr>'
                        td_pattern = r'<t[dh][^>]*>(.*?)</t[dh]>'
                        
                        trs = re.findall(tr_pattern, html_content, re.DOTALL | re.IGNORECASE)
                        if trs:
                            cells_from_html = []
                            for tr_content in trs:
                                tds = re.findall(td_pattern, tr_content, re.DOTALL | re.IGNORECASE)
                                if tds:
                                    # æ¸…ç† HTML æ ‡ç­¾å’Œå®ä½“
                                    row = []
                                    for td in tds:
                                        # ç§»é™¤å†…åµŒçš„ HTML æ ‡ç­¾å’Œå®ä½“
                                        cell_text = re.sub(r'<[^>]+>', '', td)
                                        cell_text = cell_text.replace('&nbsp;', ' ').replace('&amp;', '&')
                                        cell_text = cell_text.replace('&lt;', '<').replace('&gt;', '>')
                                        cell_text = ' '.join(cell_text.split())  # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
                                        row.append(cell_text)
                                    if row:
                                        cells_from_html.append(row)
                            
                            if cells_from_html and len(cells_from_html) > 0:
                                first_row_cols = len(cells_from_html[0]) if cells_from_html else 0
                                if first_row_cols >= 2 or len(cells_from_html) >= 2:
                                    table_data['cells'] = cells_from_html
                                    table_data['rows'] = len(cells_from_html)
                                    table_data['columns'] = max((len(r) for r in cells_from_html), default=0)
                                    table_data['structure'] = 'html_parsed_regex'
                                    cells_from_meta = cells_from_html
                                    logger.info(f"âœ… ä» HTML (æ­£åˆ™) è§£æå‡ºè¡¨æ ¼ cells: {table_data['rows']} è¡Œ x {table_data['columns']} åˆ—")
                except Exception as e:
                    logger.warning(f"ä» HTML è§£æ cells å¤±è´¥: {e}")
            
            # 4) å…œåº•ï¼šå°†è¡¨æ ¼æ–‡æœ¬æŒ‰ \n / \t æ‹†åˆ†æˆäºŒç»´æ•°ç»„
            # âš ï¸ åªåœ¨æ²¡æœ‰ä» metadata å’Œ HTML è·å–åˆ°ç»“æ„åŒ–æ•°æ®æ—¶æ‰ä½¿ç”¨æ­¤æ–¹æ³•
            if not cells_from_meta:
                text = getattr(element, 'text', '') or ''
                if text:
                    # âœ… ä¼˜åŒ–ï¼šå°è¯•å¤šç§åˆ†éš”ç¬¦ï¼Œæé«˜è§£æå‡†ç¡®æ€§
                    # è¡¨æ ¼æ–‡æœ¬å¯èƒ½ä½¿ç”¨åˆ¶è¡¨ç¬¦ã€å¤šä¸ªç©ºæ ¼ã€æˆ–ç®¡é“ç¬¦åˆ†éš”
                    lines = [ln.strip() for ln in text.split('\n') if ln.strip()]
                    cells: List[List[str]] = []
                    
                    for ln in lines:
                        # å°è¯•åˆ¶è¡¨ç¬¦åˆ†éš”
                        if '\t' in ln:
                            cols = [c.strip() for c in ln.split('\t') if c.strip()]
                        # å°è¯•ç®¡é“ç¬¦åˆ†éš”ï¼ˆMarkdown è¡¨æ ¼æ ¼å¼ï¼‰
                        elif '|' in ln:
                            cols = [c.strip() for c in ln.split('|') if c.strip()]
                            # ç§»é™¤å¯èƒ½çš„è¡¨å¤´/è¡¨å°¾åˆ†éš”ç¬¦ï¼ˆå¦‚ |---|---|ï¼‰
                            cols = [c for c in cols if not all(char in ['-', ':', ' '] for char in c)]
                        # å°è¯•å¤šä¸ªç©ºæ ¼åˆ†éš”
                        else:
                            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¤šä¸ªè¿ç»­ç©ºæ ¼
                            cols = _RE_MULTI_SPACES.split(ln)
                            cols = [c.strip() for c in cols if c.strip()]
                        
                        if cols:
                            cells.append(cols)
                    
                    # âš ï¸ é‡è¦ï¼šéªŒè¯è§£æç»“æœï¼Œé¿å…å•ä¸ªé•¿å­—ç¬¦ä¸²è¢«å½“ä½œæœ‰æ•ˆå•å…ƒæ ¼
                    if cells:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è¡¨æ ¼ç»“æ„ï¼ˆè‡³å°‘2åˆ—æˆ–2è¡Œï¼‰
                        first_row_cols = len(cells[0]) if cells else 0
                        if first_row_cols >= 2 or len(cells) >= 2:
                            table_data['cells'] = cells
                            table_data['rows'] = len(cells)
                            table_data['columns'] = max((len(r) for r in cells), default=0)
                            table_data['structure'] = 'tsv'
                            logger.debug(f"ä»æ–‡æœ¬æå–åˆ°è¡¨æ ¼æ•°æ®: {table_data['rows']} è¡Œ x {table_data['columns']} åˆ—")
                        else:
                            # å¦‚æœè§£æå‡ºæ¥çš„ç»“æ„æ— æ•ˆï¼ˆåªæœ‰1è¡Œ1åˆ—ï¼‰ï¼Œä¸” HTML å­˜åœ¨ï¼Œä¸è¦è¦†ç›–
                            if not table_data.get('html'):
                                logger.warning(f"âš ï¸ ä»æ–‡æœ¬è§£æå‡ºçš„è¡¨æ ¼ç»“æ„æ— æ•ˆï¼ˆ{len(cells)} è¡Œ x {first_row_cols} åˆ—ï¼‰ï¼Œä¸”æ²¡æœ‰ HTMLï¼Œä¿ç•™è§£æç»“æœ")
                                table_data['cells'] = cells
                                table_data['rows'] = len(cells)
                                table_data['columns'] = first_row_cols
                                table_data['structure'] = 'tsv_invalid'
                            else:
                                logger.warning(f"âš ï¸ ä»æ–‡æœ¬è§£æå‡ºçš„è¡¨æ ¼ç»“æ„æ— æ•ˆï¼ˆ{len(cells)} è¡Œ x {first_row_cols} åˆ—ï¼‰ï¼Œä½† HTML å­˜åœ¨ï¼Œè·³è¿‡æ–‡æœ¬è§£æç»“æœ")

            # âœ… è®°å½•è¡¨æ ¼æ•°æ®çš„å®Œæ•´æ€§
            if table_data['cells']:
                total_cells = sum(len(row) for row in table_data['cells'])
                logger.info(f"âœ… è¡¨æ ¼æ•°æ®æå–å®Œæˆ: {table_data['rows']} è¡Œ x {table_data['columns']} åˆ—, å…± {total_cells} ä¸ªå•å…ƒæ ¼")
            elif table_data.get('html'):
                logger.info(f"âœ… è¡¨æ ¼ HTML æå–å®Œæˆ: {len(table_data['html'])} å­—ç¬¦")
            else:
                logger.warning(f"âš ï¸ è¡¨æ ¼æ•°æ®æå–ä¸å®Œæ•´: ä»…è·å–åˆ°æ–‡æœ¬ï¼Œå¯èƒ½ä¸¢å¤±ç»“æ„ä¿¡æ¯")

            return table_data
            
        except Exception as e:
            logger.error(f"æå–è¡¨æ ¼æ•°æ®é”™è¯¯: {e}", exc_info=True)
            return {'rows': 0, 'columns': 0, 'cells': [], 'structure': 'unknown'}
    
    def _extract_document_metadata(self, elements: List) -> Dict[str, Any]:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        try:
            metadata = {
                'title': '',
                'author': '',
                'created_date': '',
                'modified_date': '',
                'language': 'unknown',
                'keywords': [],
                'entities': []
            }
            
            # ä»å…ƒç´ ä¸­æå–å…ƒæ•°æ®
            for element in elements:
                element_metadata = getattr(element, 'metadata', None)
                
                # æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªTitleå…ƒç´ ï¼‰
                if element.category == 'Title' and not metadata['title']:
                    metadata['title'] = getattr(element, 'text', '')
                
                # æå–ä½œè€…/æ—¶é—´ä¿¡æ¯ï¼ˆå…¼å®¹ ElementMetadata/dict/Noneï¼‰
                try:
                    def _safe_get(obj, key, default=None):
                        if obj is None:
                            return default
                        val = getattr(obj, key, None)
                        if val is not None:
                            return val
                        if isinstance(obj, dict):
                            return obj.get(key, default)
                        to_dict = getattr(obj, 'to_dict', None)
                        if callable(to_dict):
                            try:
                                d = to_dict()
                                if isinstance(d, dict):
                                    return d.get(key, default)
                            except Exception:
                                return default
                        return default
                    author = _safe_get(element_metadata, 'author', None)
                    if author:
                        metadata['author'] = author
                    created_date = _safe_get(element_metadata, 'created_date', None)
                    if created_date:
                        metadata['created_date'] = created_date
                    modified_date = _safe_get(element_metadata, 'modified_date', None)
                    if modified_date:
                        metadata['modified_date'] = modified_date
                except Exception:
                    pass
            
            return metadata
            
        except Exception as e:
            logger.error(f"æå–æ–‡æ¡£å…ƒæ•°æ®é”™è¯¯: {e}")
            return {
                'title': '',
                'author': '',
                'created_date': '',
                'modified_date': '',
                'language': 'unknown',
                'keywords': [],
                'entities': []
            }
    
    def _split_long_paragraph(self, paragraph: str, chunk_size: int) -> List[str]:
        """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
        try:
            chunks = []
            sentences = paragraph.split('ã€‚')
            current_chunk = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                if len(current_chunk) + len(sentence) > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        current_chunk = sentence
                    else:
                        # å¥å­æœ¬èº«å¤ªé•¿ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
                        char_chunks = [sentence[i:i+chunk_size] for i in range(0, len(sentence), chunk_size)]
                        chunks.extend(char_chunks[:-1])
                        current_chunk = char_chunks[-1]
                else:
                    if current_chunk:
                        current_chunk += "ã€‚" + sentence
                    else:
                        current_chunk = sentence
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"åˆ†å‰²é•¿æ®µè½é”™è¯¯: {e}")
            return [paragraph]