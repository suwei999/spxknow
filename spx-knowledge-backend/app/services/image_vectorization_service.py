"""
Image Vectorization Service
æ ¹æ®æ–‡æ¡£å¤„ç†æµç¨‹è®¾è®¡å®ç°CLIP/ResNet/ViTå›¾ç‰‡å‘é‡åŒ–åŠŸèƒ½
"""

import os
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from PIL import Image
import torch
import open_clip
import cv2
from app.core.logging import logger
from app.core.exceptions import CustomException, ErrorCode
from app.utils.download_progress import (
    log_download_start, 
    log_download_success, 
    log_download_error,
    setup_hf_download_progress
)

# å½»åº•ç§»é™¤å¯¹ torchvision çš„å¯¼å…¥ï¼Œé¿å…å› ç¯å¢ƒä¸å…¼å®¹å¯¼è‡´åº”ç”¨å¯åŠ¨å¤±è´¥
# å¦‚æœåç»­éœ€è¦ ResNet/ViTï¼Œå¯åœ¨å…·å¤‡å…¼å®¹ç¯å¢ƒæ—¶å†æŒ‰éœ€å¼•å…¥

class ImageVectorizationService:
    """å›¾ç‰‡å‘é‡åŒ–æœåŠ¡ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models = {}
        self.transforms = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–è§†è§‰æ¨¡å‹ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è§†è§‰æ¨¡å‹")
            from app.config.settings import settings as _settings
            
            # âš ï¸ é‡è¦ï¼šåœ¨æ£€æŸ¥æ¨¡å‹ä¹‹å‰å°±è®¾ç½® HF_HOMEï¼Œç¡®ä¿ open_clip èƒ½æ‰¾åˆ°ç¼“å­˜
            # ä¼˜å…ˆä½¿ç”¨ settings.py ä¸­é…ç½®çš„ HF_HOMEï¼Œå¦åˆ™ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä½ç½®
            # å…³é”®ä¿®å¤ï¼šå¦‚æœé…ç½®äº† CLIP_CACHE_DIRï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•ä½œä¸º HF_HOME
            if _settings.HF_HOME:
                # å¦‚æœæ˜ç¡®é…ç½®äº† HF_HOMEï¼Œä½¿ç”¨é…ç½®çš„å€¼
                hf_home = _settings.HF_HOME
                logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®çš„ HF_HOME: {hf_home}")
            else:
                # å¦‚æœæ²¡æœ‰é…ç½® HF_HOMEï¼Œä½†é…ç½®äº† CLIP_CACHE_DIRï¼Œä¼˜å…ˆä½¿ç”¨ CLIP_CACHE_DIR çš„çˆ¶ç›®å½•
                # è¿™æ · open_clip å’Œ huggingface_hub éƒ½ä¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•
                if _settings.CLIP_CACHE_DIR:
                    # å°† HF_HOME è®¾ç½®ä¸º CLIP_CACHE_DIR çš„çˆ¶ç›®å½•ï¼ˆé€šå¸¸æ˜¯ models/clipï¼‰
                    # æˆ–è€…ç›´æ¥è®¾ç½®ä¸º CLIP_CACHE_DIRï¼ˆå¦‚æœå¸Œæœ› HF ç¼“å­˜ç›´æ¥æ”¾åœ¨è¿™é‡Œï¼‰
                    hf_home = os.path.dirname(_settings.CLIP_CACHE_DIR)  # ä¾‹å¦‚ï¼šF:\spxknowlage\spx-knowledge-backend\models\clip
                    logger.info(f"ğŸ“ æœªé…ç½® HF_HOMEï¼Œä½¿ç”¨ CLIP_CACHE_DIR çš„çˆ¶ç›®å½•ä½œä¸º HF_HOME: {hf_home}")
                else:
                    # æœ€åæ‰ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä½ç½®
                    hf_home = os.path.expanduser("~/.cache/huggingface")
                    logger.info(f"ğŸ“ ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ HF_HOME: {hf_home}")
            
            # âš ï¸ å…³é”®ï¼šå¼ºåˆ¶è®¾ç½® HF_HOME ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ open_clip å’Œ huggingface_hub ä½¿ç”¨é…ç½®çš„ç›®å½•
            os.environ["HF_HOME"] = hf_home  # ä½¿ç”¨ = è€Œä¸æ˜¯ setdefaultï¼Œç¡®ä¿è¦†ç›–ä»»ä½•é»˜è®¤å€¼
            logger.info(f"âœ… å·²è®¾ç½® HF_HOME ç¯å¢ƒå˜é‡: {hf_home}")
            
            # ä»…åˆå§‹åŒ–CLIPï¼Œé¿å…ä¸‹è½½å…¶ä»–æ¨¡å‹
            logger.info("åˆå§‹åŒ–CLIPæ¨¡å‹")
            # å‡†å¤‡æœ¬åœ°ç›®å½•ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨åˆ›å»ºï¼‰
            try:
                os.makedirs(_settings.CLIP_MODELS_DIR, exist_ok=True)
                os.makedirs(_settings.CLIP_CACHE_DIR, exist_ok=True)
                logger.info(f"âœ… CLIPæ¨¡å‹ç›®å½•å·²å‡†å¤‡: {_settings.CLIP_MODELS_DIR}")
                logger.info(f"âœ… CLIPç¼“å­˜ç›®å½•å·²å‡†å¤‡: {_settings.CLIP_CACHE_DIR}")
            except Exception as e:
                logger.warning(f"åˆ›å»ºCLIPç›®å½•å¤±è´¥: {e}")
            
            # âš ï¸ å…³é”®ï¼šè®¾ç½® OPENCLIP_CACHE ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ open_clip ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•
            os.environ["OPENCLIP_CACHE"] = _settings.CLIP_CACHE_DIR  # ä½¿ç”¨ = è€Œä¸æ˜¯ setdefaultï¼Œç¡®ä¿è¦†ç›–
            logger.info(f"âœ… å·²è®¾ç½® OPENCLIP_CACHE ç¯å¢ƒå˜é‡: {_settings.CLIP_CACHE_DIR}")

            # æ£€æŸ¥æœ¬åœ°æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_name = getattr(_settings, 'CLIP_MODEL_NAME', 'ViT-B-32')
            model_full_name = f"{model_name} (CLIP)"
            
            # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒ…æ‹¬æŒ‡å®šè·¯å¾„å’Œç¼“å­˜ç›®å½•ï¼‰
            # open_clip åº“ä¸‹è½½çš„æ¨¡å‹å¯èƒ½ç¼“å­˜åœ¨ OPENCLIP_CACHE ç›®å½•ä¸­
            model_found = False
            model_location = None
            hf_cache_model_path = None  # Hugging Face Hub ç¼“å­˜ä¸­çš„æ¨¡å‹è·¯å¾„
            
            def _check_clip_model_in_directory(directory, desc=""):
                """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾ CLIP æ¨¡å‹æ–‡ä»¶"""
                if not os.path.exists(directory):
                    return None
                try:
                    # CLIP æ¨¡å‹å¯èƒ½æ˜¯ .pt, .pth, .safetensors ç­‰æ ¼å¼
                    model_extensions = ['.pt', '.pth', '.safetensors', '.bin']
                    for root, dirs, files in os.walk(directory):
                        for file in files:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯CLIPæ¨¡å‹æ–‡ä»¶ï¼ˆé€šå¸¸åŒ…å«æ¨¡å‹åç§°æˆ–pretrainedæ ‡è¯†ï¼‰
                            file_lower = file.lower()
                            if any(file_lower.endswith(ext) for ext in model_extensions):
                                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«CLIPç›¸å…³æ ‡è¯†
                                if ('clip' in file_lower or 
                                    'vit-b-32' in file_lower or 
                                    'laion' in file_lower or
                                    'openclip' in file_lower):
                                    model_path = os.path.join(root, file)
                                    try:
                                        file_size = os.path.getsize(model_path)
                                        # CLIPæ¨¡å‹é€šå¸¸>10MB
                                        if file_size > 10 * 1024 * 1024:  # è‡³å°‘10MB
                                            return model_path
                                    except OSError:
                                        continue
                except Exception:
                    pass
                return None
            
            # 1. ä¼˜å…ˆæ£€æŸ¥æŒ‡å®šè·¯å¾„
            if os.path.exists(_settings.CLIP_PRETRAINED_PATH):
                model_found = True
                model_location = _settings.CLIP_PRETRAINED_PATH
                pretrained_arg = _settings.CLIP_PRETRAINED_PATH
                logger.info(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°CLIPæ¨¡å‹æƒé‡: {_settings.CLIP_PRETRAINED_PATH}")
                logger.info(f"ğŸ”§ æ­£åœ¨ä»æœ¬åœ°åŠ è½½ CLIP æ¨¡å‹: {model_name}")
            else:
                # 2. æ£€æŸ¥ open_clip ç¼“å­˜ç›®å½•
                cached_model = _check_clip_model_in_directory(_settings.CLIP_CACHE_DIR, "CLIPç¼“å­˜ç›®å½•")
                if cached_model:
                    model_found = True
                    model_location = cached_model
                    # open_clip ä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œä¸éœ€è¦æŒ‡å®šè·¯å¾„ï¼›ä½†éœ€å¼ºåˆ¶ç¦»çº¿é¿å…æ¢æµ‹ç½‘ç»œ
                    pretrained_arg = "laion2b_s34b_b79k"
                    logger.info(f"âœ… åœ¨ CLIP ç¼“å­˜ç›®å½•ä¸­å‘ç°æ¨¡å‹: {cached_model}")
                    logger.info(f"ğŸ’¡ open_clip åº“å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
                    logger.info(f"ğŸ”§ æ­£åœ¨ä»ç¼“å­˜åŠ è½½ CLIP æ¨¡å‹: {model_name}")
                else:
                    # 3. æ£€æŸ¥ Hugging Face ç¼“å­˜ä½ç½®ï¼ˆopen_clip å¯èƒ½ä½¿ç”¨ HF Hubï¼‰
                    # âš ï¸ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥å·²è®¾ç½®çš„ HF_HOMEï¼ˆå³é…ç½®çš„ç¼“å­˜ç›®å½•ï¼‰ï¼Œè€Œä¸æ˜¯ç³»ç»Ÿé»˜è®¤ä½ç½®
                    try:
                        # ä½¿ç”¨å·²è®¾ç½®çš„ HF_HOMEï¼ˆå·²ç»åœ¨å‰é¢è®¾ç½®ä¸ºé…ç½®çš„ç›®å½•ï¼‰
                        hf_cache_dir = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
                        cached_model = _check_clip_model_in_directory(hf_cache_dir, "HFç¼“å­˜ç›®å½•ï¼ˆé…ç½®çš„ï¼‰")
                        if cached_model:
                            model_found = True
                            model_location = cached_model
                            hf_cache_model_path = cached_model
                            pretrained_arg = "laion2b_s34b_b79k"
                            logger.info(f"âœ… åœ¨é…ç½®çš„ HF ç¼“å­˜ç›®å½•ä¸­å‘ç° CLIP æ¨¡å‹: {cached_model}")
                            logger.info(f"ğŸ’¡ open_clip åº“å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
                            logger.info(f"ğŸ”§ æ­£åœ¨ä»ç¼“å­˜åŠ è½½ CLIP æ¨¡å‹: {model_name}")
                        else:
                            # å¦‚æœé…ç½®çš„ç›®å½•ä¸­æ²¡æœ‰ï¼Œå†æ£€æŸ¥ç³»ç»Ÿé»˜è®¤ä½ç½®ï¼ˆä½œä¸ºæœ€åçš„å¤‡é€‰ï¼‰
                            default_hf_cache = os.path.expanduser("~/.cache/huggingface")
                            if default_hf_cache != hf_cache_dir:
                                cached_model = _check_clip_model_in_directory(default_hf_cache, "HFç¼“å­˜ç›®å½•ï¼ˆç³»ç»Ÿé»˜è®¤ï¼‰")
                                if cached_model:
                                    logger.warning(f"âš ï¸ åœ¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ç›®å½•ä¸­å‘ç°æ¨¡å‹ï¼Œä½†é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­æœªæ‰¾åˆ°: {cached_model}")
                                    logger.warning(f"âš ï¸ å»ºè®®å°†æ¨¡å‹å¤åˆ¶åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•: {_settings.CLIP_CACHE_DIR}")
                                    logger.warning(f"âš ï¸ æ³¨æ„ï¼šç³»ç»Ÿå°†ä¼˜å…ˆä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•ï¼Œä¸ä¼šä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ä¸­çš„æ¨¡å‹")
                                    logger.warning(f"âš ï¸ å¦‚æœå¸Œæœ›ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ï¼Œè¯·åœ¨ settings.py ä¸­é…ç½® HF_HOME æŒ‡å‘: {default_hf_cache}")
                                    # æ³¨æ„ï¼šè¿™é‡Œä¸è®¾ç½® model_found = Trueï¼Œå› ä¸ºå¸Œæœ›ä¼˜å…ˆä½¿ç”¨é…ç½®çš„ç›®å½•
                                    # å¦‚æœç”¨æˆ·å¸Œæœ›ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ï¼Œå¯ä»¥æ‰‹åŠ¨é…ç½® HF_HOME æŒ‡å‘é»˜è®¤ä½ç½®
                    except Exception as e:
                        logger.debug(f"æ£€æŸ¥ HF ç¼“å­˜ç›®å½•æ—¶å‡ºé”™: {e}")
                
                # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ‰è®°å½•ä¸‹è½½å¼€å§‹
                if not model_found:
                    logger.info(f"âš ï¸ æœ¬åœ°CLIPæ¨¡å‹æƒé‡ä¸å­˜åœ¨: {_settings.CLIP_PRETRAINED_PATH}")
                    logger.info(f"âš ï¸ CLIPç¼“å­˜ç›®å½•ä¸­æœªå‘ç°æ¨¡å‹: {_settings.CLIP_CACHE_DIR}")
                    logger.info("ğŸŒ å°†å…è®¸è”ç½‘ä¸‹è½½æ¨¡å‹ï¼ˆæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼‰")
                    
                    # âœ… é‡è¦ï¼šç¡®ä¿ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå…è®¸ä¸‹è½½
                    # æ¸…é™¤ä»»ä½•å¯èƒ½é˜»æ­¢ä¸‹è½½çš„ç¦»çº¿æ¨¡å¼è®¾ç½®
                    if "HF_HUB_OFFLINE" in os.environ:
                        old_offline = os.environ.pop("HF_HUB_OFFLINE", None)
                        logger.debug(f"å·²æ¸…é™¤ HF_HUB_OFFLINE={old_offline}ï¼Œå…è®¸ä¸‹è½½")
                    if "TRANSFORMERS_OFFLINE" in os.environ:
                        old_transformers = os.environ.pop("TRANSFORMERS_OFFLINE", None)
                        logger.debug(f"å·²æ¸…é™¤ TRANSFORMERS_OFFLINE={old_transformers}ï¼Œå…è®¸ä¸‹è½½")
                    if "HF_DATASETS_OFFLINE" in os.environ:
                        old_datasets = os.environ.pop("HF_DATASETS_OFFLINE", None)
                        logger.debug(f"å·²æ¸…é™¤ HF_DATASETS_OFFLINE={old_datasets}ï¼Œå…è®¸ä¸‹è½½")
                    
                    # è®¾ç½® Hugging Face ä¸‹è½½è¿›åº¦æ˜¾ç¤º
                    try:
                        # å¯ç”¨ Hugging Face Hub çš„è¿›åº¦æ˜¾ç¤º
                        os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'
                        # å°è¯•è®¾ç½® tqdm è¿›åº¦æ¡
                        from huggingface_hub.utils import disable_progress_bars
                        disable_progress_bars(False)
                        logger.debug("âœ… å·²å¯ç”¨ Hugging Face ä¸‹è½½è¿›åº¦æ˜¾ç¤º")
                    except Exception as e:
                        logger.debug(f"å¯ç”¨ä¸‹è½½è¿›åº¦æ˜¾ç¤ºå¤±è´¥: {e}")
                    
                    # è®°å½•ä¸‹è½½å¼€å§‹ä¿¡æ¯
                    log_download_start(
                        model_name=model_full_name,
                        source="Hugging Face",
                        estimated_size="300-500 MB"
                    )
                    
                    pretrained_arg = "laion2b_s34b_b79k"
                    logger.info(f"ğŸ”§ æ­£åœ¨ä¸‹è½½å¹¶åŠ è½½ CLIP æ¨¡å‹: {model_name}, pretrained={pretrained_arg}")
                    logger.info(f"ğŸ’¾ ä¸‹è½½åçš„æ¨¡å‹å°†ä¿å­˜åˆ°ç¼“å­˜ç›®å½•: {_settings.CLIP_CACHE_DIR}")
            
            # å¦‚æœå·²æ‰¾åˆ°æœ¬åœ°/ç¼“å­˜æ¨¡å‹ï¼Œå¼ºåˆ¶å¼€å¯ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ä»»ä½•ç½‘ç»œæ¢æµ‹
            if model_found:
                # è®¾ç½®å¤šä¸ªç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿æ‰€æœ‰åº“éƒ½éµå®ˆç¦»çº¿æ¨¡å¼
                os.environ["HF_HUB_OFFLINE"] = "1"
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_DATASETS_OFFLINE"] = "1"
                # å¼ºåˆ¶ Hugging Face Hub ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œç¦æ­¢ç½‘ç»œè¿æ¥
                os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
                logger.info("ğŸŒ å·²å¯ç”¨ HF ç¦»çº¿æ¨¡å¼ï¼ˆå‘ç°æœ¬åœ°/ç¼“å­˜æ¨¡å‹ï¼Œç¦æ­¢è”ç½‘æ¢æµ‹ï¼‰")
                logger.info(f"ğŸ“ æ¨¡å‹ä½ç½®: {model_location}")
                
                # âš ï¸ å¦‚æœä½¿ç”¨çš„æ˜¯ Hugging Face Hub ç¼“å­˜çš„æ¨¡å‹ï¼Œç¡®ä¿ HF_HOME æŒ‡å‘æ­£ç¡®ä½ç½®
                # æ³¨æ„ï¼šå¦‚æœæ¨¡å‹åœ¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ä¸­ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•ï¼Œè¿™é‡Œä¸åº”è¯¥ä¿®æ”¹ HF_HOME
                if hf_cache_model_path:
                    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦åœ¨é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­
                    configured_hf_home = os.environ.get("HF_HOME", "")
                    if hf_cache_model_path.startswith(_settings.CLIP_CACHE_DIR) or hf_cache_model_path.startswith(configured_hf_home):
                        # æ¨¡å‹å·²ç»åœ¨é…ç½®çš„ç›®å½•ä¸­ï¼Œä¸éœ€è¦ä¿®æ”¹ HF_HOME
                        logger.debug(f"âœ… æ¨¡å‹å·²åœ¨é…ç½®çš„ç¼“å­˜ç›®å½•ä¸­ï¼Œæ— éœ€ä¿®æ”¹ HF_HOME")
                    else:
                        # æ¨¡å‹åœ¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ä¸­ï¼Œä½†æˆ‘ä»¬å·²ç»è®¾ç½®äº† HF_HOME æŒ‡å‘é…ç½®çš„ç›®å½•
                        # è¿™é‡Œä¸åº”è¯¥ä¿®æ”¹ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¼ºåˆ¶ä½¿ç”¨é…ç½®çš„ç›®å½•
                        logger.warning(f"âš ï¸ å‘ç°æ¨¡å‹åœ¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜ä¸­: {hf_cache_model_path}")
                        logger.warning(f"âš ï¸ ä½†å·²é…ç½®ä½¿ç”¨: {configured_hf_home}")
                        logger.warning(f"âš ï¸ å»ºè®®å°†æ¨¡å‹å¤åˆ¶åˆ°é…ç½®çš„ç¼“å­˜ç›®å½•: {_settings.CLIP_CACHE_DIR}")
                        logger.warning(f"âš ï¸ å½“å‰å°†ç»§ç»­ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•ï¼Œæ¨¡å‹ä¸‹è½½å°†ä¿å­˜åˆ°é…ç½®çš„ç›®å½•")

            # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ˜¯ä¸‹è½½è¿‡ç¨‹ï¼Œæ•è·ä¸‹è½½ç›¸å…³é”™è¯¯
            try:
                # å¦‚æœæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œåœ¨è°ƒç”¨ open_clip ä¹‹å‰å†æ¬¡ç¡®è®¤ç¦»çº¿æ¨¡å¼è®¾ç½®
                if model_found:
                    # ç¡®ä¿æ‰€æœ‰ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡éƒ½å·²è®¾ç½®ï¼ˆåœ¨è°ƒç”¨å‰å†æ¬¡ç¡®è®¤ï¼‰
                    os.environ["HF_HUB_OFFLINE"] = "1"
                    os.environ["TRANSFORMERS_OFFLINE"] = "1"
                    os.environ["HF_DATASETS_OFFLINE"] = "1"
                    # å¼ºåˆ¶ Hugging Face Hub ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œç¦æ­¢ä»»ä½•ç½‘ç»œè¿æ¥
                    os.environ["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = "1"
                    
                    # âœ… å…³é”®ä¿®å¤ï¼šä½¿ç”¨å¤šç§æ–¹å¼å¼ºåˆ¶ç¦ç”¨ç½‘ç»œè¿æ¥
                    try:
                        # æ–¹æ³•1: ä½¿ç”¨ huggingface_hub çš„ offline_modeï¼ˆå¦‚æœæ”¯æŒï¼‰
                        try:
                            from huggingface_hub import offline_mode
                            offline_mode(True)
                            logger.info("âœ… å·²å¯ç”¨ huggingface_hub.offline_mode(True)")
                        except (ImportError, AttributeError):
                            pass
                        
                        # æ–¹æ³•2: ä½¿ç”¨ç¯å¢ƒå˜é‡å¼ºåˆ¶ç¦»çº¿ï¼ˆå·²è®¾ç½®ï¼‰
                        # æ–¹æ³•3: ä¸´æ—¶ç¦ç”¨ Hugging Face ç›¸å…³çš„ç½‘ç»œè¯·æ±‚ï¼ˆä»…é™ huggingface.coï¼‰
                        try:
                            import requests
                            from urllib.parse import urlparse
                            # ä¿å­˜åŸå§‹çš„ get å’Œ head æ–¹æ³•
                            original_get = requests.get
                            original_head = requests.head
                            
                            def disabled_get(url, *args, **kwargs):
                                """ç¦ç”¨ Hugging Face ç›¸å…³çš„ç½‘ç»œè¯·æ±‚"""
                                if isinstance(url, str):
                                    parsed = urlparse(url)
                                    if 'huggingface.co' in parsed.netloc or 'hf.co' in parsed.netloc:
                                        raise ConnectionError(f"ç½‘ç»œè¿æ¥å·²ç¦ç”¨ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰ï¼Œæ‹’ç»è®¿é—®: {url}")
                                # å¯¹äºé Hugging Face çš„è¯·æ±‚ï¼Œå…è®¸é€šè¿‡
                                return original_get(url, *args, **kwargs)
                            
                            def disabled_head(url, *args, **kwargs):
                                """ç¦ç”¨ Hugging Face ç›¸å…³çš„ HEAD è¯·æ±‚"""
                                if isinstance(url, str):
                                    parsed = urlparse(url)
                                    if 'huggingface.co' in parsed.netloc or 'hf.co' in parsed.netloc:
                                        raise ConnectionError(f"ç½‘ç»œè¿æ¥å·²ç¦ç”¨ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰ï¼Œæ‹’ç»è®¿é—®: {url}")
                                # å¯¹äºé Hugging Face çš„è¯·æ±‚ï¼Œå…è®¸é€šè¿‡
                                return original_head(url, *args, **kwargs)
                            
                            # ä¸´æ—¶æ›¿æ¢ requests æ–¹æ³•ï¼Œä»…ç¦ç”¨ Hugging Face ç›¸å…³è¯·æ±‚
                            requests.get = disabled_get
                            requests.head = disabled_head
                            logger.info("âœ… å·²ä¸´æ—¶ç¦ç”¨ Hugging Face ç½‘ç»œè¿æ¥ï¼ˆä»…ç”¨äºæ¨¡å‹åŠ è½½ï¼‰")
                            
                            # åŠ è½½æ¨¡å‹
                            try:
                                clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                                    model_name, pretrained=pretrained_arg, device=self.device
                                )
                                logger.info("âœ… æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½ CLIP æ¨¡å‹ï¼ˆæœªè”ç½‘ï¼‰")
                            finally:
                                # æ¢å¤åŸå§‹çš„ requests æ–¹æ³•
                                requests.get = original_get
                                requests.head = original_head
                                logger.debug("âœ… å·²æ¢å¤ requests ç½‘ç»œè¿æ¥")
                        except Exception as req_err:
                            logger.warning(f"ç¦ç”¨ Hugging Face ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†ç¦»çº¿æ¨¡å¼: {req_err}")
                            # å›é€€åˆ°æ ‡å‡†åŠ è½½æ–¹å¼ï¼ˆä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
                            clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                                model_name, pretrained=pretrained_arg, device=self.device
                            )
                    except Exception as offline_err:
                        logger.warning(f"å¼ºåˆ¶ç¦»çº¿æ¨¡å¼è®¾ç½®å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹å¼: {offline_err}")
                        # æœ€åçš„å›é€€ï¼šä½¿ç”¨æ ‡å‡†åŠ è½½æ–¹å¼
                        clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                            model_name, pretrained=pretrained_arg, device=self.device
                        )
                else:
                    # âœ… æ¨¡å‹ä¸å­˜åœ¨ï¼Œå…è®¸ä¸‹è½½ï¼ˆä¸è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼‰
                    logger.info("ğŸŒ å…è®¸è”ç½‘ä¸‹è½½ï¼ˆæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼‰")
                    logger.info(f"ğŸ“¥ æ­£åœ¨ä» Hugging Face ä¸‹è½½æ¨¡å‹: {pretrained_arg}")
                    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                        model_name, pretrained=pretrained_arg, device=self.device
                    )
                    logger.info("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
                
                # å¦‚æœæ˜¯ä»ç½‘ç»œä¸‹è½½çš„ï¼ˆæ¨¡å‹ä¸åœ¨ç¼“å­˜ä¸­ï¼‰ï¼Œè®°å½•æˆåŠŸ
                if not model_found:
                    log_download_success(
                        model_name=model_full_name,
                        save_path=_settings.CLIP_CACHE_DIR
                    )
                
                logger.info("âœ… CLIPæ¨¡å‹åŠ è½½å®Œæˆï¼Œæ­£åœ¨ç§»åŠ¨åˆ°è®¾å¤‡...")
            except Exception as download_error:
                # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸‹è½½ç›¸å…³çš„é”™è¯¯
                error_str = str(download_error).lower()
                is_download_error = any(keyword in error_str for keyword in [
                    'download', 'huggingface', 'hub', 'network', 'connection', 
                    'timeout', 'unpack', 'http', 'https', 'ssl', 'certificate'
                ])
                
                if is_download_error:
                    # ä½¿ç”¨ç»Ÿä¸€çš„é”™è¯¯æ—¥å¿—æ ¼å¼
                    log_download_error(
                        model_name=model_full_name,
                        error=download_error,
                        download_url="https://huggingface.co/laion/CLIP-ViT-B-32-xlaion2b-s34b-b79k",
                        local_path=_settings.CLIP_PRETRAINED_PATH,
                        readme_path="models/clip/README.md"
                    )
                    
                    raise CustomException(
                        code=ErrorCode.VECTOR_GENERATION_FAILED,
                        message=f"CLIPæ¨¡å‹ä¸‹è½½å¤±è´¥: {str(download_error)}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° {_settings.CLIP_PRETRAINED_PATH}ã€‚è¯¦è§ models/clip/README.md"
                    )
                else:
                    # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼ˆå¦‚æ¨¡å‹åŠ è½½ã€æ ¼å¼é”™è¯¯ç­‰ï¼‰
                    logger.error(f"âŒ CLIPæ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆéä¸‹è½½é”™è¯¯ï¼‰: {download_error}")
                    raise
            
            clip_model.eval()
            clip_model.to(self.device)
            self.models['clip'] = clip_model
            self.transforms['clip'] = clip_preprocess
            logger.info("âœ… CLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå·²åŠ è½½åˆ°è®¾å¤‡")
            
            # å…³é—­ ResNet/ViT ä»¥é¿å…è”ç½‘ä¸‹è½½
            
            logger.info("âœ… æ‰€æœ‰è§†è§‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except CustomException:
            # é‡æ–°æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸
            raise
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"è§†è§‰æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_clip_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨CLIPç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹CLIPå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'clip')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨CLIPæå–ç‰¹å¾
            with torch.no_grad():
                image_features = self.models['clip'].encode_image(image)
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                embedding = image_features.cpu().numpy().flatten().tolist()
            
            logger.info(f"CLIPå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"CLIPå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"CLIPå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_clip_text_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆ512ç»´ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬ï¼‰
            
        Returns:
            512ç»´å‘é‡åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹CLIPæ–‡æœ¬å‘é‡åŒ–: {text[:50]}...")
            
            # æ£€æŸ¥CLIPæ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if 'clip' not in self.models:
                raise CustomException(
                    code=ErrorCode.VECTOR_GENERATION_FAILED,
                    message="CLIPæ¨¡å‹æœªåˆå§‹åŒ–"
                )
            
            clip_model = self.models['clip']
            
            # å¯¹æ–‡æœ¬è¿›è¡Œtokenize
            # open_clip.tokenize è¿”å›çš„æ˜¯ torch.Tensorï¼Œéœ€è¦ç§»åŠ¨åˆ°device
            text_tokens = open_clip.tokenize([text]).to(self.device)
            
            # ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨
            with torch.no_grad():
                text_features = clip_model.encode_text(text_tokens)
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡ï¼ˆä¸å›¾åƒå‘é‡å¤„ç†æ–¹å¼ä¸€è‡´ï¼‰
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                embedding = text_features.cpu().numpy().flatten().tolist()
            
            if not embedding or len(embedding) != 512:
                logger.error(f"CLIPæ–‡æœ¬å‘é‡ç”Ÿæˆå¤±è´¥æˆ–ç»´åº¦ä¸æ­£ç¡®: {len(embedding) if embedding else 0}")
                raise CustomException(
                    code=ErrorCode.VECTOR_GENERATION_FAILED,
                    message=f"CLIPæ–‡æœ¬å‘é‡ç”Ÿæˆå¤±è´¥æˆ–ç»´åº¦ä¸æ­£ç¡®: æœŸæœ›512ç»´ï¼Œå®é™…{len(embedding) if embedding else 0}ç»´"
                )
            
            logger.info(f"CLIPæ–‡æœ¬å‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except CustomException:
            raise
        except Exception as e:
            logger.error(f"CLIPæ–‡æœ¬å‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"CLIPæ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_resnet_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨ResNetç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹ResNetå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'resnet')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨ResNetæå–ç‰¹å¾
            with torch.no_grad():
                features = self.models['resnet'](image)
                # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–åçš„ç‰¹å¾ä½œä¸ºåµŒå…¥å‘é‡
                embedding = features.cpu().numpy().flatten().tolist()
            
            logger.info(f"ResNetå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ResNetå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"ResNetå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_vit_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨ViTç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹ViTå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'vit')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨ViTæå–ç‰¹å¾
            with torch.no_grad():
                features = self.models['vit'](image)
                # ä½¿ç”¨åˆ†ç±»å¤´å‰çš„ç‰¹å¾ä½œä¸ºåµŒå…¥å‘é‡
                embedding = features.cpu().numpy().flatten().tolist()
            
            logger.info(f"ViTå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ViTå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"ViTå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_multi_model_embedding(self, image_path: str, models: List[str] = None) -> Dict[str, List[float]]:
        """ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹å¤šæ¨¡å‹å‘é‡åŒ–: {image_path}")
            
            if models is None:
                models = ['clip', 'resnet', 'vit']
            
            embeddings = {}
            
            for model_name in models:
                try:
                    if model_name == 'clip':
                        embeddings['clip'] = self.generate_clip_embedding(image_path)
                    elif model_name == 'resnet':
                        embeddings['resnet'] = self.generate_resnet_embedding(image_path)
                    elif model_name == 'vit':
                        embeddings['vit'] = self.generate_vit_embedding(image_path)
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {model_name} å‘é‡åŒ–å¤±è´¥: {e}")
                    embeddings[model_name] = []
            
            logger.info(f"å¤šæ¨¡å‹å‘é‡åŒ–å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡å‹å‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"å¤šæ¨¡å‹å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_hybrid_embedding(self, image_path: str) -> List[float]:
        """ç”Ÿæˆæ··åˆåµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æ··åˆå‘é‡åŒ–: {image_path}")
            
            # è·å–å¤šä¸ªæ¨¡å‹çš„åµŒå…¥å‘é‡
            embeddings = self.generate_multi_model_embedding(image_path)
            
            # èåˆç­–ç•¥ï¼šåŠ æƒå¹³å‡
            weights = {
                'clip': 0.5,    # CLIPæƒé‡æœ€é«˜ï¼Œå› ä¸ºæ”¯æŒå›¾æ–‡è”åˆ
                'resnet': 0.3,  # ResNetæƒé‡ä¸­ç­‰
                'vit': 0.2      # ViTæƒé‡è¾ƒä½
            }
            
            # è®¡ç®—åŠ æƒå¹³å‡
            hybrid_embedding = []
            for i in range(512):  # ä½¿ç”¨512ç»´ä½œä¸ºæ ‡å‡†ç»´åº¦
                weighted_sum = 0.0
                total_weight = 0.0
                
                for model_name, embedding in embeddings.items():
                    if embedding and len(embedding) > i:
                        weight = weights.get(model_name, 0.0)
                        weighted_sum += embedding[i] * weight
                        total_weight += weight
                
                if total_weight > 0:
                    hybrid_embedding.append(weighted_sum / total_weight)
                else:
                    hybrid_embedding.append(0.0)
            
            logger.info(f"æ··åˆå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(hybrid_embedding)}")
            return hybrid_embedding
            
        except Exception as e:
            logger.error(f"æ··åˆå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"æ··åˆå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def _preprocess_image(self, image_path: str, model_type: str) -> Optional[torch.Tensor]:
        """å›¾ç‰‡é¢„å¤„ç† - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.debug(f"å¼€å§‹å›¾ç‰‡é¢„å¤„ç†: {image_path}, æ¨¡å‹: {model_type}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„å¤„ç†æ–¹æ³•
            if model_type == 'clip':
                # CLIPé¢„å¤„ç†
                transform = self.transforms['clip']
                processed_image = transform(image).unsqueeze(0).to(self.device)
            elif model_type in ['resnet', 'vit']:
                # ResNet/ViTé¢„å¤„ç†
                transform = self.transforms[model_type]
                processed_image = transform(image).unsqueeze(0).to(self.device)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return None
            
            logger.debug(f"å›¾ç‰‡é¢„å¤„ç†å®Œæˆ: {image_path}")
            return processed_image
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡é¢„å¤„ç†é”™è¯¯: {e}", exc_info=True)
            return None
    
    def extract_image_features(self, image_path: str) -> Dict[str, Any]:
        """æå–å›¾ç‰‡ç‰¹å¾ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æå–å›¾ç‰‡ç‰¹å¾: {image_path}")
            
            # ä½¿ç”¨OpenCVæå–ä¼ ç»Ÿç‰¹å¾
            image = cv2.imread(image_path)
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"æ— æ³•è¯»å–å›¾ç‰‡: {image_path}"
                )
            
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # æå–SIFTç‰¹å¾
            sift = cv2.SIFT_create()
            keypoints, descriptors = sift.detectAndCompute(gray, None)
            
            # æå–ORBç‰¹å¾
            orb = cv2.ORB_create()
            orb_keypoints, orb_descriptors = orb.detectAndCompute(gray, None)
            
            # è·å–å›¾ç‰‡åŸºæœ¬ä¿¡æ¯
            height, width = image.shape[:2]
            
            features = {
                'sift_keypoints': len(keypoints) if keypoints is not None else 0,
                'sift_descriptors': descriptors.tolist() if descriptors is not None else [],
                'orb_keypoints': len(orb_keypoints) if orb_keypoints is not None else 0,
                'orb_descriptors': orb_descriptors.tolist() if orb_descriptors is not None else [],
                'image_size': {'width': width, 'height': height},
                'aspect_ratio': width / height if height > 0 else 1.0
            }
            
            logger.info(f"å›¾ç‰‡ç‰¹å¾æå–å®Œæˆ: {image_path}")
            return features
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç‰¹å¾æå–é”™è¯¯: {e}", exc_info=True)
            return {}
    
    def calculate_image_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """è®¡ç®—å›¾ç‰‡ç›¸ä¼¼åº¦ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.debug("å¼€å§‹è®¡ç®—å›¾ç‰‡ç›¸ä¼¼åº¦")
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            if len(vec1) != len(vec2):
                logger.warning(f"å‘é‡ç»´åº¦ä¸ä¸€è‡´: {len(vec1)} vs {len(vec2)}")
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                logger.warning("å‘é‡ä¸ºé›¶å‘é‡")
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            logger.debug(f"å›¾ç‰‡ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ: {similarity}")
            return similarity
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}", exc_info=True)
            return 0.0
    
    def batch_process_images(self, image_paths: List[str], model_type: str = 'clip') -> List[List[float]]:
        """æ‰¹é‡å¤„ç†å›¾ç‰‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†å›¾ç‰‡ï¼Œæ•°é‡: {len(image_paths)}, æ¨¡å‹: {model_type}")
            
            embeddings = []
            
            for i, image_path in enumerate(image_paths):
                try:
                    logger.debug(f"å¤„ç†å›¾ç‰‡ {i+1}/{len(image_paths)}: {image_path}")
                    
                    if model_type == 'clip':
                        embedding = self.generate_clip_embedding(image_path)
                    elif model_type == 'resnet':
                        embedding = self.generate_resnet_embedding(image_path)
                    elif model_type == 'vit':
                        embedding = self.generate_vit_embedding(image_path)
                    elif model_type == 'hybrid':
                        embedding = self.generate_hybrid_embedding(image_path)
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                        embedding = []
                    
                    embeddings.append(embedding)
                    
                except Exception as e:
                    logger.error(f"å›¾ç‰‡ {image_path} å¤„ç†å¤±è´¥: {e}")
                    embeddings.append([])
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(embeddings)} ä¸ªå›¾ç‰‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†é”™è¯¯: {e}", exc_info=True)
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info("è·å–è§†è§‰æ¨¡å‹ä¿¡æ¯")
            
            model_info = {
                'available_models': list(self.models.keys()),
                'device': str(self.device),
                'clip_info': {
                    'model_name': 'ViT-B/32',
                    'embedding_dim': 512,
                    'supports_text_image': True
                },
                'resnet_info': {
                    'model_name': 'ResNet-50',
                    'embedding_dim': 1000,
                    'supports_text_image': False
                },
                'vit_info': {
                    'model_name': 'ViT-B/16',
                    'embedding_dim': 1000,
                    'supports_text_image': False
                },
                'hybrid_info': {
                    'model_name': 'Hybrid (CLIP+ResNet+ViT)',
                    'embedding_dim': 512,
                    'supports_text_image': True
                }
            }
            
            logger.info("è§†è§‰æ¨¡å‹ä¿¡æ¯è·å–å®Œæˆ")
            return model_info
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯é”™è¯¯: {e}", exc_info=True)
            return {'error': str(e)}
    
    def cleanup_models(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        try:
            logger.info("å¼€å§‹æ¸…ç†æ¨¡å‹èµ„æº")
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¸…ç†æ¨¡å‹
            self.models.clear()
            self.transforms.clear()
            
            logger.info("æ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹èµ„æºæ¸…ç†é”™è¯¯: {e}", exc_info=True)
