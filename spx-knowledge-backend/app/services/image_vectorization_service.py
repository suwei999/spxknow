"""
Image Vectorization Service
æ ¹æ®æ–‡æ¡£å¤„ç†æµç¨‹è®¾è®¡å®ç°CLIP/ResNet/ViTå›¾ç‰‡å‘é‡åŒ–åŠŸèƒ½
"""

import os
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50, vit_b_16
import open_clip
import cv2
from app.core.logging import logger
from app.core.exceptions import CustomException, ErrorCode
from app.utils.download_progress import (
    log_download_start, 
    log_download_success, 
    log_download_error,
    setup_hf_download_progress
)

class ImageVectorizationService:
    """å›¾ç‰‡å‘é‡åŒ–æœåŠ¡ - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models = {}
        self.transforms = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–è§†è§‰æ¨¡å‹ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è§†è§‰æ¨¡å‹")
            from app.config.settings import settings as _settings
            # ä»…åˆå§‹åŒ–CLIPï¼Œé¿å…ä¸‹è½½å…¶ä»–æ¨¡å‹
            logger.info("åˆå§‹åŒ–CLIPæ¨¡å‹")
            # å‡†å¤‡æœ¬åœ°ç›®å½•ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨åˆ›å»ºï¼‰
            try:
                os.makedirs(_settings.CLIP_MODELS_DIR, exist_ok=True)
                os.makedirs(_settings.CLIP_CACHE_DIR, exist_ok=True)
                logger.info(f"âœ… CLIPæ¨¡å‹ç›®å½•å·²å‡†å¤‡: {_settings.CLIP_MODELS_DIR}")
                logger.info(f"âœ… CLIPç¼“å­˜ç›®å½•å·²å‡†å¤‡: {_settings.CLIP_CACHE_DIR}")
            except Exception as e:
                logger.warning(f"åˆ›å»ºCLIPç›®å½•å¤±è´¥: {e}")
            os.environ.setdefault('OPENCLIP_CACHE', _settings.CLIP_CACHE_DIR)

            # æ£€æŸ¥æœ¬åœ°æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_name = getattr(_settings, 'CLIP_MODEL_NAME', 'ViT-B-32')
            model_full_name = f"{model_name} (CLIP)"
            
            # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒ…æ‹¬æŒ‡å®šè·¯å¾„å’Œç¼“å­˜ç›®å½•ï¼‰
            # open_clip åº“ä¸‹è½½çš„æ¨¡å‹å¯èƒ½ç¼“å­˜åœ¨ OPENCLIP_CACHE ç›®å½•ä¸­
            model_found = False
            model_location = None
            
            def _check_clip_model_in_directory(directory, desc=""):
                """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾ CLIP æ¨¡å‹æ–‡ä»¶"""
                if not os.path.exists(directory):
                    return None
                try:
                    # CLIP æ¨¡å‹å¯èƒ½æ˜¯ .pt, .pth, .safetensors ç­‰æ ¼å¼
                    model_extensions = ['.pt', '.pth', '.safetensors', '.bin']
                    for root, dirs, files in os.walk(directory):
                        for file in files:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯CLIPæ¨¡å‹æ–‡ä»¶ï¼ˆé€šå¸¸åŒ…å«æ¨¡å‹åç§°æˆ–pretrainedæ ‡è¯†ï¼‰
                            file_lower = file.lower()
                            if any(file_lower.endswith(ext) for ext in model_extensions):
                                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«CLIPç›¸å…³æ ‡è¯†
                                if ('clip' in file_lower or 
                                    'vit-b-32' in file_lower or 
                                    'laion' in file_lower or
                                    'openclip' in file_lower):
                                    model_path = os.path.join(root, file)
                                    try:
                                        file_size = os.path.getsize(model_path)
                                        # CLIPæ¨¡å‹é€šå¸¸>10MB
                                        if file_size > 10 * 1024 * 1024:  # è‡³å°‘10MB
                                            return model_path
                                    except OSError:
                                        continue
                except Exception:
                    pass
                return None
            
            # 1. ä¼˜å…ˆæ£€æŸ¥æŒ‡å®šè·¯å¾„
            if os.path.exists(_settings.CLIP_PRETRAINED_PATH):
                model_found = True
                model_location = _settings.CLIP_PRETRAINED_PATH
                pretrained_arg = _settings.CLIP_PRETRAINED_PATH
                logger.info(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°CLIPæ¨¡å‹æƒé‡: {_settings.CLIP_PRETRAINED_PATH}")
                logger.info(f"ğŸ”§ æ­£åœ¨ä»æœ¬åœ°åŠ è½½ CLIP æ¨¡å‹: {model_name}")
            else:
                # 2. æ£€æŸ¥ open_clip ç¼“å­˜ç›®å½•
                cached_model = _check_clip_model_in_directory(_settings.CLIP_CACHE_DIR, "CLIPç¼“å­˜ç›®å½•")
                if cached_model:
                    model_found = True
                    model_location = cached_model
                    # open_clip ä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œä¸éœ€è¦æŒ‡å®šè·¯å¾„
                    pretrained_arg = "laion2b_s34b_b79k"
                    logger.info(f"âœ… åœ¨ CLIP ç¼“å­˜ç›®å½•ä¸­å‘ç°æ¨¡å‹: {cached_model}")
                    logger.info(f"ğŸ’¡ open_clip åº“å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
                    logger.info(f"ğŸ”§ æ­£åœ¨ä»ç¼“å­˜åŠ è½½ CLIP æ¨¡å‹: {model_name}")
                else:
                    # 3. æ£€æŸ¥ Hugging Face é»˜è®¤ç¼“å­˜ä½ç½®ï¼ˆopen_clip å¯èƒ½ä½¿ç”¨ HF Hubï¼‰
                    try:
                        hf_default_cache = os.path.expanduser("~/.cache/huggingface")
                        cached_model = _check_clip_model_in_directory(hf_default_cache, "HFé»˜è®¤ç¼“å­˜")
                        if cached_model:
                            model_found = True
                            model_location = cached_model
                            pretrained_arg = "laion2b_s34b_b79k"
                            logger.info(f"âœ… åœ¨ HF é»˜è®¤ç¼“å­˜ç›®å½•ä¸­å‘ç° CLIP æ¨¡å‹: {cached_model}")
                            logger.info(f"ğŸ’¡ open_clip åº“å°†è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
                            logger.info(f"ğŸ”§ æ­£åœ¨ä»ç¼“å­˜åŠ è½½ CLIP æ¨¡å‹: {model_name}")
                    except Exception as e:
                        logger.debug(f"æ£€æŸ¥ HF é»˜è®¤ç¼“å­˜ç›®å½•æ—¶å‡ºé”™: {e}")
                
                # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ‰è®°å½•ä¸‹è½½å¼€å§‹
                if not model_found:
                    logger.info(f"âš ï¸ æœ¬åœ°CLIPæ¨¡å‹æƒé‡ä¸å­˜åœ¨: {_settings.CLIP_PRETRAINED_PATH}")
                    logger.info(f"âš ï¸ CLIPç¼“å­˜ç›®å½•ä¸­æœªå‘ç°æ¨¡å‹: {_settings.CLIP_CACHE_DIR}")
                    
                    # è®¾ç½® Hugging Face ä¸‹è½½è¿›åº¦æ˜¾ç¤º
                    try:
                        # å¯ç”¨ Hugging Face Hub çš„è¿›åº¦æ˜¾ç¤º
                        os.environ.setdefault('HF_HUB_DISABLE_PROGRESS_BARS', '0')
                        # å°è¯•è®¾ç½® tqdm è¿›åº¦æ¡
                        from huggingface_hub.utils import disable_progress_bars
                        disable_progress_bars(False)
                    except Exception:
                        pass
                    
                    # è®°å½•ä¸‹è½½å¼€å§‹ä¿¡æ¯
                    log_download_start(
                        model_name=model_full_name,
                        source="Hugging Face",
                        estimated_size="300-500 MB"
                    )
                    
                    pretrained_arg = "laion2b_s34b_b79k"
                    logger.info(f"ğŸ”§ æ­£åœ¨ä¸‹è½½å¹¶åŠ è½½ CLIP æ¨¡å‹: {model_name}, pretrained={pretrained_arg}")
                    logger.info(f"ğŸ’¾ ä¸‹è½½åçš„æ¨¡å‹å°†ä¿å­˜åˆ°ç¼“å­˜ç›®å½•: {_settings.CLIP_CACHE_DIR}")
            
            # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ˜¯ä¸‹è½½è¿‡ç¨‹ï¼Œæ•è·ä¸‹è½½ç›¸å…³é”™è¯¯
            try:
                clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                    model_name, pretrained=pretrained_arg, device=self.device
                )
                
                # å¦‚æœæ˜¯ä»ç½‘ç»œä¸‹è½½çš„ï¼ˆæ¨¡å‹ä¸åœ¨ç¼“å­˜ä¸­ï¼‰ï¼Œè®°å½•æˆåŠŸ
                if not model_found:
                    log_download_success(
                        model_name=model_full_name,
                        save_path=_settings.CLIP_CACHE_DIR
                    )
                
                logger.info("âœ… CLIPæ¨¡å‹åŠ è½½å®Œæˆï¼Œæ­£åœ¨ç§»åŠ¨åˆ°è®¾å¤‡...")
            except Exception as download_error:
                # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸‹è½½ç›¸å…³çš„é”™è¯¯
                error_str = str(download_error).lower()
                is_download_error = any(keyword in error_str for keyword in [
                    'download', 'huggingface', 'hub', 'network', 'connection', 
                    'timeout', 'unpack', 'http', 'https', 'ssl', 'certificate'
                ])
                
                if is_download_error:
                    # ä½¿ç”¨ç»Ÿä¸€çš„é”™è¯¯æ—¥å¿—æ ¼å¼
                    log_download_error(
                        model_name=model_full_name,
                        error=download_error,
                        download_url="https://huggingface.co/laion/CLIP-ViT-B-32-xlaion2b-s34b-b79k",
                        local_path=_settings.CLIP_PRETRAINED_PATH,
                        readme_path="models/clip/README.md"
                    )
                    
                    raise CustomException(
                        code=ErrorCode.VECTOR_GENERATION_FAILED,
                        message=f"CLIPæ¨¡å‹ä¸‹è½½å¤±è´¥: {str(download_error)}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° {_settings.CLIP_PRETRAINED_PATH}ã€‚è¯¦è§ models/clip/README.md"
                    )
                else:
                    # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼ˆå¦‚æ¨¡å‹åŠ è½½ã€æ ¼å¼é”™è¯¯ç­‰ï¼‰
                    logger.error(f"âŒ CLIPæ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆéä¸‹è½½é”™è¯¯ï¼‰: {download_error}")
                    raise
            
            clip_model.eval()
            clip_model.to(self.device)
            self.models['clip'] = clip_model
            self.transforms['clip'] = clip_preprocess
            logger.info("âœ… CLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå·²åŠ è½½åˆ°è®¾å¤‡")
            
            # å…³é—­ ResNet/ViT ä»¥é¿å…è”ç½‘ä¸‹è½½
            
            logger.info("âœ… æ‰€æœ‰è§†è§‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except CustomException:
            # é‡æ–°æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸
            raise
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"è§†è§‰æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_clip_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨CLIPç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹CLIPå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'clip')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨CLIPæå–ç‰¹å¾
            with torch.no_grad():
                image_features = self.models['clip'].encode_image(image)
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                embedding = image_features.cpu().numpy().flatten().tolist()
            
            logger.info(f"CLIPå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"CLIPå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"CLIPå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_resnet_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨ResNetç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹ResNetå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'resnet')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨ResNetæå–ç‰¹å¾
            with torch.no_grad():
                features = self.models['resnet'](image)
                # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–åçš„ç‰¹å¾ä½œä¸ºåµŒå…¥å‘é‡
                embedding = features.cpu().numpy().flatten().tolist()
            
            logger.info(f"ResNetå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ResNetå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"ResNetå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_vit_embedding(self, image_path: str) -> List[float]:
        """ä½¿ç”¨ViTç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹ViTå‘é‡åŒ–: {image_path}")
            
            # å›¾ç‰‡é¢„å¤„ç†
            image = self._preprocess_image(image_path, 'vit')
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {image_path}"
                )
            
            # ä½¿ç”¨ViTæå–ç‰¹å¾
            with torch.no_grad():
                features = self.models['vit'](image)
                # ä½¿ç”¨åˆ†ç±»å¤´å‰çš„ç‰¹å¾ä½œä¸ºåµŒå…¥å‘é‡
                embedding = features.cpu().numpy().flatten().tolist()
            
            logger.info(f"ViTå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ViTå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"ViTå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_multi_model_embedding(self, image_path: str, models: List[str] = None) -> Dict[str, List[float]]:
        """ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç”Ÿæˆå›¾ç‰‡åµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹å¤šæ¨¡å‹å‘é‡åŒ–: {image_path}")
            
            if models is None:
                models = ['clip', 'resnet', 'vit']
            
            embeddings = {}
            
            for model_name in models:
                try:
                    if model_name == 'clip':
                        embeddings['clip'] = self.generate_clip_embedding(image_path)
                    elif model_name == 'resnet':
                        embeddings['resnet'] = self.generate_resnet_embedding(image_path)
                    elif model_name == 'vit':
                        embeddings['vit'] = self.generate_vit_embedding(image_path)
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {model_name} å‘é‡åŒ–å¤±è´¥: {e}")
                    embeddings[model_name] = []
            
            logger.info(f"å¤šæ¨¡å‹å‘é‡åŒ–å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡å‹å‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"å¤šæ¨¡å‹å‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def generate_hybrid_embedding(self, image_path: str) -> List[float]:
        """ç”Ÿæˆæ··åˆåµŒå…¥å‘é‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æ··åˆå‘é‡åŒ–: {image_path}")
            
            # è·å–å¤šä¸ªæ¨¡å‹çš„åµŒå…¥å‘é‡
            embeddings = self.generate_multi_model_embedding(image_path)
            
            # èåˆç­–ç•¥ï¼šåŠ æƒå¹³å‡
            weights = {
                'clip': 0.5,    # CLIPæƒé‡æœ€é«˜ï¼Œå› ä¸ºæ”¯æŒå›¾æ–‡è”åˆ
                'resnet': 0.3,  # ResNetæƒé‡ä¸­ç­‰
                'vit': 0.2      # ViTæƒé‡è¾ƒä½
            }
            
            # è®¡ç®—åŠ æƒå¹³å‡
            hybrid_embedding = []
            for i in range(512):  # ä½¿ç”¨512ç»´ä½œä¸ºæ ‡å‡†ç»´åº¦
                weighted_sum = 0.0
                total_weight = 0.0
                
                for model_name, embedding in embeddings.items():
                    if embedding and len(embedding) > i:
                        weight = weights.get(model_name, 0.0)
                        weighted_sum += embedding[i] * weight
                        total_weight += weight
                
                if total_weight > 0:
                    hybrid_embedding.append(weighted_sum / total_weight)
                else:
                    hybrid_embedding.append(0.0)
            
            logger.info(f"æ··åˆå‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(hybrid_embedding)}")
            return hybrid_embedding
            
        except Exception as e:
            logger.error(f"æ··åˆå‘é‡åŒ–é”™è¯¯: {e}", exc_info=True)
            raise CustomException(
                code=ErrorCode.VECTOR_GENERATION_FAILED,
                message=f"æ··åˆå‘é‡åŒ–å¤±è´¥: {str(e)}"
            )
    
    def _preprocess_image(self, image_path: str, model_type: str) -> Optional[torch.Tensor]:
        """å›¾ç‰‡é¢„å¤„ç† - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.debug(f"å¼€å§‹å›¾ç‰‡é¢„å¤„ç†: {image_path}, æ¨¡å‹: {model_type}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„å¤„ç†æ–¹æ³•
            if model_type == 'clip':
                # CLIPé¢„å¤„ç†
                transform = self.transforms['clip']
                processed_image = transform(image).unsqueeze(0).to(self.device)
            elif model_type in ['resnet', 'vit']:
                # ResNet/ViTé¢„å¤„ç†
                transform = self.transforms[model_type]
                processed_image = transform(image).unsqueeze(0).to(self.device)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return None
            
            logger.debug(f"å›¾ç‰‡é¢„å¤„ç†å®Œæˆ: {image_path}")
            return processed_image
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡é¢„å¤„ç†é”™è¯¯: {e}", exc_info=True)
            return None
    
    def extract_image_features(self, image_path: str) -> Dict[str, Any]:
        """æå–å›¾ç‰‡ç‰¹å¾ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æå–å›¾ç‰‡ç‰¹å¾: {image_path}")
            
            # ä½¿ç”¨OpenCVæå–ä¼ ç»Ÿç‰¹å¾
            image = cv2.imread(image_path)
            if image is None:
                raise CustomException(
                    code=ErrorCode.IMAGE_PROCESSING_FAILED,
                    message=f"æ— æ³•è¯»å–å›¾ç‰‡: {image_path}"
                )
            
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # æå–SIFTç‰¹å¾
            sift = cv2.SIFT_create()
            keypoints, descriptors = sift.detectAndCompute(gray, None)
            
            # æå–ORBç‰¹å¾
            orb = cv2.ORB_create()
            orb_keypoints, orb_descriptors = orb.detectAndCompute(gray, None)
            
            # è·å–å›¾ç‰‡åŸºæœ¬ä¿¡æ¯
            height, width = image.shape[:2]
            
            features = {
                'sift_keypoints': len(keypoints) if keypoints is not None else 0,
                'sift_descriptors': descriptors.tolist() if descriptors is not None else [],
                'orb_keypoints': len(orb_keypoints) if orb_keypoints is not None else 0,
                'orb_descriptors': orb_descriptors.tolist() if orb_descriptors is not None else [],
                'image_size': {'width': width, 'height': height},
                'aspect_ratio': width / height if height > 0 else 1.0
            }
            
            logger.info(f"å›¾ç‰‡ç‰¹å¾æå–å®Œæˆ: {image_path}")
            return features
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç‰¹å¾æå–é”™è¯¯: {e}", exc_info=True)
            return {}
    
    def calculate_image_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """è®¡ç®—å›¾ç‰‡ç›¸ä¼¼åº¦ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.debug("å¼€å§‹è®¡ç®—å›¾ç‰‡ç›¸ä¼¼åº¦")
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            if len(vec1) != len(vec2):
                logger.warning(f"å‘é‡ç»´åº¦ä¸ä¸€è‡´: {len(vec1)} vs {len(vec2)}")
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                logger.warning("å‘é‡ä¸ºé›¶å‘é‡")
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            logger.debug(f"å›¾ç‰‡ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ: {similarity}")
            return similarity
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}", exc_info=True)
            return 0.0
    
    def batch_process_images(self, image_paths: List[str], model_type: str = 'clip') -> List[List[float]]:
        """æ‰¹é‡å¤„ç†å›¾ç‰‡ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†å›¾ç‰‡ï¼Œæ•°é‡: {len(image_paths)}, æ¨¡å‹: {model_type}")
            
            embeddings = []
            
            for i, image_path in enumerate(image_paths):
                try:
                    logger.debug(f"å¤„ç†å›¾ç‰‡ {i+1}/{len(image_paths)}: {image_path}")
                    
                    if model_type == 'clip':
                        embedding = self.generate_clip_embedding(image_path)
                    elif model_type == 'resnet':
                        embedding = self.generate_resnet_embedding(image_path)
                    elif model_type == 'vit':
                        embedding = self.generate_vit_embedding(image_path)
                    elif model_type == 'hybrid':
                        embedding = self.generate_hybrid_embedding(image_path)
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                        embedding = []
                    
                    embeddings.append(embedding)
                    
                except Exception as e:
                    logger.error(f"å›¾ç‰‡ {image_path} å¤„ç†å¤±è´¥: {e}")
                    embeddings.append([])
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(embeddings)} ä¸ªå›¾ç‰‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†é”™è¯¯: {e}", exc_info=True)
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ - æ ¹æ®è®¾è®¡æ–‡æ¡£å®ç°"""
        try:
            logger.info("è·å–è§†è§‰æ¨¡å‹ä¿¡æ¯")
            
            model_info = {
                'available_models': list(self.models.keys()),
                'device': str(self.device),
                'clip_info': {
                    'model_name': 'ViT-B/32',
                    'embedding_dim': 512,
                    'supports_text_image': True
                },
                'resnet_info': {
                    'model_name': 'ResNet-50',
                    'embedding_dim': 1000,
                    'supports_text_image': False
                },
                'vit_info': {
                    'model_name': 'ViT-B/16',
                    'embedding_dim': 1000,
                    'supports_text_image': False
                },
                'hybrid_info': {
                    'model_name': 'Hybrid (CLIP+ResNet+ViT)',
                    'embedding_dim': 512,
                    'supports_text_image': True
                }
            }
            
            logger.info("è§†è§‰æ¨¡å‹ä¿¡æ¯è·å–å®Œæˆ")
            return model_info
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯é”™è¯¯: {e}", exc_info=True)
            return {'error': str(e)}
    
    def cleanup_models(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        try:
            logger.info("å¼€å§‹æ¸…ç†æ¨¡å‹èµ„æº")
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¸…ç†æ¨¡å‹
            self.models.clear()
            self.transforms.clear()
            
            logger.info("æ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹èµ„æºæ¸…ç†é”™è¯¯: {e}", exc_info=True)
